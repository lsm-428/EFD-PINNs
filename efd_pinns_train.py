#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
EFD-PINNs ç»Ÿä¸€è®­ç»ƒè„šæœ¬
æ•´åˆçŸ­è®­ / å¢å¼ºç®¡çº¿ / é•¿æœŸè®­ç»ƒ / 3D æ˜ å°„ / åŠ¨æ€æƒé‡ / æŠ¥å‘Š / å¯è§†åŒ– / æ£€æŸ¥ç‚¹ / ONNX å¯¼å‡º
ç”¨æ³•ï¼š
  çŸ­è®­ï¼š python efd_pinns_train.py --mode train --config config/exp_short_config.json --output-dir results_short
  å¢å¼ºï¼š python efd_pinns_train.py --mode train --config config/exp_short_config.json --output-dir results_enhanced --quick_run
  é•¿æœŸï¼š python efd_pinns_train.py --mode train --config config/long_run_config.json --output-dir results_long --epochs 100000 --dynamic_weight --weight_strategy adaptive
"""

import argparse
import copy
import contextlib
import datetime
import glob
import json
import logging
import math
import os
import random
import shutil
import sys
import time
from typing import Dict, List, Optional, Tuple

# æ·»åŠ æ··åˆç²¾åº¦è®­ç»ƒæ”¯æŒ
try:
    from torch.cuda.amp import autocast, GradScaler
except ImportError:
    # é™çº§å®ç°
    class GradScaler:
        def scale(self, loss):
            return loss
        
        def unscale_(self, optimizer):
            pass
        
        def step(self, optimizer):
            optimizer.step()
        
        def update(self):
            pass
    
    def autocast(enabled=True):
        return contextlib.nullcontext()

import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset

# å†…éƒ¨æ¨¡å—ï¼ˆä¿æŒä¸æ—§è„šæœ¬ä¸€è‡´ï¼‰
try:
    from ewp_pinn_input_layer import EWPINNInputLayer
    from ewp_pinn_output_layer import EWPINNOutputLayer
    from ewp_data_interface import validate_units
    from ewp_pinn_performance_monitor import ModelPerformanceMonitor
    from ewp_pinn_adaptive_hyperoptimizer import AdaptiveHyperparameterOptimizer
    from scripts.generate_constraint_report import compute_constraint_stats
    from scripts.visualize_constraint_report import plot_residual_stats, plot_weight_series
except ImportError as e:
    print("[WARN] éƒ¨åˆ†å†…éƒ¨æ¨¡å—å¯¼å…¥å¤±è´¥ï¼Œå°†è·³è¿‡å¯¹åº”åŠŸèƒ½:", e)

# ç‰©ç†ä¸æ¨¡å‹ç»„ä»¶
try:
    from ewp_pinn_physics import PINNConstraintLayer as ExternalPINNConstraintLayer, PhysicsEnhancedLoss
    from ewp_pinn_regularization import AdvancedRegularizer, GradientNoiseRegularizer, apply_regularization_to_model
    from ewp_pinn_optimized_architecture import EfficientEWPINN, create_optimized_model, get_model_optimization_suggestions
except ImportError:
    ExternalPINNConstraintLayer = None
    PhysicsEnhancedLoss = None
    AdvancedRegularizer = GradientNoiseRegularizer = apply_regularization_to_model = None
    EfficientEWPINN = create_optimized_model = get_model_optimization_suggestions = None

# OptimizedEWPINN ç±»å®ç° - å¢å¼ºå‹ç¥ç»ç½‘ç»œæ¶æ„
class OptimizedEWPINN(nn.Module):
    def __init__(self, input_dim, hidden_dims, output_dim, activation='relu', config=None):
        super().__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.config = config or {}
        self.use_batch_norm = self.config.get('use_batch_norm', True)
        self.use_residual = self.config.get('use_residual', True)
        self.use_attention = self.config.get('use_attention', False)
        
        layers = []
        prev_dim = input_dim
        
        # æ„å»ºéšè—å±‚
        for i, h_dim in enumerate(hidden_dims):
            # ä¸»å±‚
            layers.append(nn.Linear(prev_dim, h_dim))
            
            # æ‰¹é‡å½’ä¸€åŒ–
            if self.use_batch_norm:
                layers.append(nn.BatchNorm1d(h_dim))
            
            # æ¿€æ´»å‡½æ•°
            if activation == 'relu':
                layers.append(nn.ReLU())
            elif activation == 'gelu':
                layers.append(nn.GELU())
            elif activation == 'tanh':
                layers.append(nn.Tanh())
            elif activation == 'leaky_relu':
                layers.append(nn.LeakyReLU(0.1))
            else:
                layers.append(nn.ReLU())
            
            # æ³¨æ„åŠ›æœºåˆ¶ï¼ˆå¯é€‰ï¼‰
            if self.use_attention and i == len(hidden_dims) // 2:
                layers.append(SimpleAttention(h_dim))
            
            prev_dim = h_dim
        
        # è¾“å‡ºå±‚
        layers.append(nn.Linear(prev_dim, output_dim))
        
        self.main_layers = nn.Sequential(*layers)
        
        # æ®‹å·®è¿æ¥ï¼ˆå¦‚æœè¾“å…¥å’Œè¾“å‡ºç»´åº¦ç›¸åŒï¼‰
        if self.use_residual and input_dim == output_dim:
            self.residual_layer = nn.Identity()
        else:
            self.residual_layer = None
        
        # æƒé‡åˆå§‹åŒ–
        self.apply(self._initialize_weights)
    
    def _initialize_weights(self, m):
        if isinstance(m, nn.Linear):
            # ä½¿ç”¨ He åˆå§‹åŒ–
            nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')
            if m.bias is not None:
                nn.init.zeros_(m.bias)
    
    def forward(self, x):
        out = self.main_layers(x)
        
        # åº”ç”¨æ®‹å·®è¿æ¥
        if self.residual_layer is not None:
            out = out + self.residual_layer(x)
        
        return out

# ç®€å•æ³¨æ„åŠ›æœºåˆ¶
class SimpleAttention(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.query = nn.Linear(dim, dim)
        self.key = nn.Linear(dim, dim)
        self.value = nn.Linear(dim, dim)
        self.softmax = nn.Softmax(dim=-1)
    
    def forward(self, x):
        q = self.query(x)
        k = self.key(x)
        v = self.value(x)
        
        # ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›
        scale = math.sqrt(x.size(-1))
        attention = self.softmax(q @ k.transpose(-2, -1) / scale)
        
        return (attention @ v) + x

# LossStabilizer ç±»å®ç° - é«˜çº§æŸå¤±ç¨³å®šå™¨
class LossStabilizer:
    def __init__(self, config=None):
        self.config = config or {}
        self.loss_type = self.config.get('loss_type', 'mse')
        self.epsilon = self.config.get('epsilon', 1e-8)
        self.adaptive_weighting = self.config.get('adaptive_weighting', False)
        self.huber_delta = self.config.get('huber_delta', 1.0)
        self.relative_weight = self.config.get('relative_weight', 0.5)
        self.history_size = self.config.get('history_size', 100)
        self.loss_history = []
        self.early_stopping_patience = self.config.get('early_stopping_patience', 20)
        self.early_stopping_min_delta = self.config.get('early_stopping_min_delta', 1e-5)
        self.best_loss = float('inf')
        self.patience_counter = 0
    
    def safe_mse_loss(self, pred, target):
        """å®‰å…¨çš„MSEæŸå¤±ï¼Œé¿å…æ•°å€¼ä¸ç¨³å®š"""
        return torch.mean(torch.clamp((pred - target) ** 2, max=1e8))
    
    def relative_loss(self, pred, target):
        """ç›¸å¯¹æŸå¤±ï¼Œå¯¹å¤§å€¼å’Œå°å€¼éƒ½æ•æ„Ÿ"""
        diff = pred - target
        relative_diff = diff / (torch.abs(target) + self.epsilon)
        return torch.mean(torch.clamp(relative_diff ** 2, max=1e8))
    
    def huber_loss(self, pred, target):
        """HuberæŸå¤±ï¼Œå¹³è¡¡MSEå’ŒMAEçš„é²æ£’æ€§"""
        diff = pred - target
        abs_diff = torch.abs(diff)
        quadratic = torch.minimum(abs_diff, torch.tensor(self.huber_delta, device=diff.device))
        linear = abs_diff - quadratic
        return torch.mean(0.5 * quadratic ** 2 + self.huber_delta * linear)
    
    def combined_loss(self, pred, target):
        """ç»„åˆæŸå¤±å‡½æ•°"""
        mse_loss = self.safe_mse_loss(pred, target)
        rel_loss = self.relative_loss(pred, target)
        return (1 - self.relative_weight) * mse_loss + self.relative_weight * rel_loss
    
    def compute_loss(self, pred, target, physics_loss=None, physics_weight=0.0):
        """è®¡ç®—æœ€ç»ˆæŸå¤±"""
        # é€‰æ‹©åŸºç¡€æŸå¤±å‡½æ•°
        if self.loss_type == 'mse':
            base_loss = self.safe_mse_loss(pred, target)
        elif self.loss_type == 'relative':
            base_loss = self.relative_loss(pred, target)
        elif self.loss_type == 'huber':
            base_loss = self.huber_loss(pred, target)
        elif self.loss_type == 'combined':
            base_loss = self.combined_loss(pred, target)
        else:
            base_loss = self.safe_mse_loss(pred, target)
        
        # æ·»åŠ ç‰©ç†æŸå¤±
        if physics_loss is not None:
            total_loss = base_loss + physics_weight * physics_loss
        else:
            total_loss = base_loss
        
        # æ›´æ–°å†å²
        self.update_history(total_loss.item())
        
        return total_loss
    
    def update_history(self, loss_value):
        """æ›´æ–°æŸå¤±å†å²"""
        self.loss_history.append(loss_value)
        if len(self.loss_history) > self.history_size:
            self.loss_history.pop(0)
    
    def check_early_stopping(self):
        """æ£€æŸ¥æ—©åœæ¡ä»¶"""
        current_loss = self.loss_history[-1] if self.loss_history else float('inf')
        
        if current_loss < self.best_loss - self.early_stopping_min_delta:
            self.best_loss = current_loss
            self.patience_counter = 0
            return False
        else:
            self.patience_counter += 1
            if self.patience_counter >= self.early_stopping_patience:
                return True
            return False
    
    def get_adaptive_physics_weight(self):
        """è·å–è‡ªé€‚åº”ç‰©ç†æƒé‡"""
        if not self.adaptive_weighting or len(self.loss_history) < 10:
            return 1.0
        
        # åŸºäºæŸå¤±å˜åŒ–ç‡è°ƒæ•´æƒé‡
        recent_avg = np.mean(self.loss_history[-10:])
        earlier_avg = np.mean(self.loss_history[:10])
        
        if earlier_avg == 0:
            return 1.0
        
        improvement_ratio = (earlier_avg - recent_avg) / earlier_avg
        
        # å¦‚æœæ”¹è¿›ç¼“æ…¢ï¼Œå¢åŠ ç‰©ç†æƒé‡
        if improvement_ratio < 0.01:
            return min(10.0, 1.0 + improvement_ratio * 100)
        else:
            return 1.0

# é•¿æœŸè®­ç»ƒç»„ä»¶
try:
    from ewp_pinn_model import EWPINN, EWPINNDataset, extract_predictions
    from ewp_pinn_optimizer import EWPINNOptimizerManager, WarmupCosineLR
    from ewp_pinn_dynamic_weight import DynamicPhysicsWeightScheduler, PhysicsWeightIntegration
except ImportError:
    EWPINN = EWPINNDataset = extract_predictions = None
    EWPINNOptimizerManager = WarmupCosineLR = None
    DynamicPhysicsWeightScheduler = PhysicsWeightIntegration = None

# é¢„å¤„ç†
try:
    from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
    from sklearn.exceptions import DataConversionWarning
except ImportError:
    StandardScaler = MinMaxScaler = RobustScaler = None

# æ—¥å¿—
logging.basicConfig(
    format="[%(asctime)s] %(levelname)s | %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    level=logging.INFO,
)
logger = logging.getLogger("EFD_PINNs_Train")

# æ·»åŠ æ··åˆç²¾åº¦è®­ç»ƒæ”¯æŒ
try:
    from torch.cuda.amp import autocast, GradScaler
except ImportError:
    logger.warning("âš ï¸  PyTorch AMP not available, mixed precision training disabled")
    
    # é™çº§å®ç°
    class GradScaler:
        def scale(self, loss):
            return loss
        
        def unscale_(self, optimizer):
            pass
        
        def step(self, optimizer):
            optimizer.step()
        
        def update(self):
            pass
    
    def autocast(enabled=True):
        import contextlib
        return contextlib.nullcontext()

# å…¨å±€å¸¸é‡
DEFAULT_CONFIG = "model_config.json"
DEFAULT_OUTPUT_DIR = "outputs"
DEFAULT_NUM_SAMPLES = 1000
DEFAULT_BATCH_SIZE = 128
DEFAULT_EPOCHS = 200
DEFAULT_LR = 1e-3
DEFAULT_MIN_LR = 1e-6
DEFAULT_WARMUP_EPOCHS = 5
DEFAULT_PHYSICS_WEIGHT = 0.1
DEFAULT_WEIGHT_STRATEGY = "adaptive"
DEFAULT_CHECKPOINT_INTERVAL = 10
DEFAULT_VALIDATION_INTERVAL = 5

# è®¾å¤‡
def get_device(preference: Optional[str] = None) -> torch.device:
    if preference:
        return torch.device(preference)
    return torch.device("cuda" if torch.cuda.is_available() else "cpu")

# éšæœºç§å­
def set_global_seed(seed: int = 42, deterministic: bool = False):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    if deterministic:
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    logger.info(f"ğŸŒ± å…¨å±€éšæœºç§å­è®¾ç½®ä¸º {seed} (deterministic={deterministic})")

# æ—¶é—´æˆ³ç›®å½•
def make_timestamp_dir(base: str) -> str:
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    path = f"{base}_{timestamp}"
    os.makedirs(path, exist_ok=True)
    return path

# ç»Ÿä¸€è¾“å‡ºç›®å½•ç»“æ„
def setup_output_dirs(output_dir: str):
    checkpoints = os.path.join(output_dir, "checkpoints")
    reports = os.path.join(output_dir, "reports")
    visuals = os.path.join(output_dir, "visualizations")
    logs = os.path.join(output_dir, "logs")
    for d in [checkpoints, reports, visuals, logs]:
        os.makedirs(d, exist_ok=True)
    return {"checkpoints": checkpoints, "reports": reports, "visualizations": visuals, "logs": logs}

# ä¿å­˜æ¨¡å‹
def save_model(
    model: nn.Module,
    normalizer,
    save_path: str,
    config: Optional[dict] = None,
    metadata: Optional[dict] = None,
    export_onnx: bool = False,
    onnx_path: Optional[str] = None,
):
    torch.save({
        "model_state_dict": model.state_dict(),
        "normalizer": normalizer.state_dict() if normalizer else None,
        "config": config or {},
        "metadata": metadata or {},
    }, save_path)
    logger.info(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜è‡³ {save_path}")
    if export_onnx and onnx_path:
        try:
            dummy_dim = model.input_dim if hasattr(model, "input_dim") else (config.get("model", {}).get("input_dim", 3) if isinstance(config, dict) else 3)
            dummy = torch.randn(1, dummy_dim)
            model_device = next(model.parameters()).device if any(True for _ in model.parameters()) else torch.device('cpu')
            dummy = dummy.to(model_device)
            torch.onnx.export(model, dummy, onnx_path, input_names=["input"], output_names=["output"], opset_version=11)
            logger.info(f"ğŸ§Š ONNX å¯¼å‡ºå®Œæˆ: {onnx_path}")
        except Exception as e:
            logger.warning(f"âš ï¸ ONNX å¯¼å‡ºå¤±è´¥: {e}ï¼Œè·³è¿‡")

# ä¿å­˜æ£€æŸ¥ç‚¹
def save_checkpoint(
    model: nn.Module,
    optimizer: torch.optim.Optimizer,
    scheduler,
    epoch: int,
    loss_history: Dict[str, List[float]],
    path: str,
    is_best: bool = False,
):
    ckpt = {
        "epoch": epoch,
        "model_state_dict": model.state_dict(),
        "optimizer_state_dict": optimizer.state_dict(),
        "scheduler_state_dict": scheduler.state_dict() if scheduler else None,
        "loss_history": loss_history,
    }
    torch.save(ckpt, path)
    logger.info(f"ğŸ—‚ï¸  æ£€æŸ¥ç‚¹å·²ä¿å­˜: {path}")
    if is_best:
        best_path = path.replace(".pth", "_best.pth")
        shutil.copy(path, best_path)
        logger.info(f"â­ æœ€ä½³æ£€æŸ¥ç‚¹å·²å¤åˆ¶: {best_path}")

# å¢å¼ºç‰ˆæ•°æ®æ ‡å‡†åŒ–å™¨
class DataNormalizer:
    def __init__(self, method: str = "standard", config: dict = None):
        self.method = method
        self.config = config or {}
        self.scaler = None
        self.mean = None
        self.std = None
        self.min_val = None
        self.max_val = None
        self.q1 = None
        self.q3 = None
        self.outlier_threshold = self.config.get('outlier_threshold', 3.0)
        
        if method == "standard" and StandardScaler:
            self.scaler = StandardScaler()
        elif method == "minmax" and MinMaxScaler:
            self.scaler = MinMaxScaler()
        elif method == "robust" and RobustScaler:
            self.scaler = RobustScaler()
        elif method == "custom":
            # è‡ªå®šä¹‰æ ‡å‡†åŒ–ï¼Œéœ€è¦ä»configè·å–å‚æ•°
            pass
        else:
            logger.warning(f"âš ï¸  ä¸æ”¯æŒçš„æ ‡å‡†åŒ–æ–¹æ³•: {method}ï¼Œå°†ä½¿ç”¨standard")
            if StandardScaler:
                self.scaler = StandardScaler()

    def handle_outliers(self, X: np.ndarray):
        """å¤„ç†å¼‚å¸¸å€¼"""
        if self.config.get('handle_outliers', False):
            # ä½¿ç”¨IQRæ–¹æ³•æˆ–Z-scoreæ–¹æ³•
            if self.config.get('outlier_method') == 'zscore':
                if self.method == 'standard' and self.mean is not None and self.std is not None:
                    z_scores = np.abs((X - self.mean) / (self.std + 1e-8))
                    X = np.clip(X, self.mean - self.outlier_threshold * self.std, 
                               self.mean + self.outlier_threshold * self.std)
            elif self.config.get('outlier_method') == 'iqr':
                if self.q1 is not None and self.q3 is not None:
                    iqr = self.q3 - self.q1
                    lower_bound = self.q1 - self.outlier_threshold * iqr
                    upper_bound = self.q3 + self.outlier_threshold * iqr
                    X = np.clip(X, lower_bound, upper_bound)
        return X

    def fit(self, X: np.ndarray):
        # å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯
        self.mean = np.mean(X, axis=0)
        self.std = np.std(X, axis=0)
        self.min_val = np.min(X, axis=0)
        self.max_val = np.max(X, axis=0)
        self.q1 = np.percentile(X, 25, axis=0)
        self.q3 = np.percentile(X, 75, axis=0)
        
        # å¤„ç†å¼‚å¸¸å€¼ï¼ˆå¦‚æœéœ€è¦ï¼‰
        X_processed = self.handle_outliers(X.copy())
        
        if self.scaler:
            self.scaler.fit(X_processed)
        elif self.method == "custom":
            # è‡ªå®šä¹‰æ ‡å‡†åŒ–é€»è¾‘
            pass

    def transform(self, X: np.ndarray) -> np.ndarray:
        if self.scaler:
            return self.scaler.transform(X)
        elif self.method == "custom" and self.mean is not None and self.std is not None:
            # è‡ªå®šä¹‰æ ‡å‡†åŒ–
            return (X - self.mean) / (self.std + 1e-8)
        return X

    def inverse_transform(self, X: np.ndarray) -> np.ndarray:
        if self.scaler:
            return self.scaler.inverse_transform(X)
        elif self.method == "custom" and self.mean is not None and self.std is not None:
            # è‡ªå®šä¹‰é€†å˜æ¢
            return X * (self.std + 1e-8) + self.mean
        return X

    def state_dict(self):
        return {
            "method": self.method,
            "config": self.config,
            "scaler": self.scaler.__dict__ if self.scaler else None,
            "mean": self.mean.tolist() if self.mean is not None else None,
            "std": self.std.tolist() if self.std is not None else None,
            "min_val": self.min_val.tolist() if self.min_val is not None else None,
            "max_val": self.max_val.tolist() if self.max_val is not None else None,
            "q1": self.q1.tolist() if self.q1 is not None else None,
            "q3": self.q3.tolist() if self.q3 is not None else None
        }

    def load_state_dict(self, state):
        self.method = state["method"]
        self.config = state.get("config", {})
        self.mean = np.array(state["mean"]) if state["mean"] is not None else None
        self.std = np.array(state["std"]) if state["std"] is not None else None
        self.min_val = np.array(state["min_val"]) if state["min_val"] is not None else None
        self.max_val = np.array(state["max_val"]) if state["max_val"] is not None else None
        self.q1 = np.array(state["q1"]) if state["q1"] is not None else None
        self.q3 = np.array(state["q3"]) if state["q3"] is not None else None
        
        if state["scaler"]:
            if self.method == "standard" and StandardScaler:
                self.scaler = StandardScaler()
                self.scaler.__dict__.update(state["scaler"])
            elif self.method == "minmax" and MinMaxScaler:
                self.scaler = MinMaxScaler()
                self.scaler.__dict__.update(state["scaler"])
            elif self.method == "robust" and RobustScaler:
                self.scaler = RobustScaler()
                self.scaler.__dict__.update(state["scaler"])

# ç”Ÿæˆæ•°æ®ï¼ˆå…¼å®¹ 3D æ˜ å°„ä¸ GPU å®‰å…¨åˆ†æ‰¹ï¼‰
def generate_training_data(
    config: dict,
    num_samples: int,
    device: torch.device,
    output_dir: str,
    use_3d_mapping: bool = False,
    gpu_safe: bool = False,
    quick_run: bool = False,
):
    if quick_run and num_samples > 500:
        logger.info("ğŸš€ quick_run æ¨¡å¼ï¼Œå¼ºåˆ¶ num_samples=500")
        num_samples = 500

    # ç®€å•ç¤ºä¾‹ï¼šéšæœºç”Ÿæˆè¾“å…¥ + å•ä½éªŒè¯
    model_config = config.get("æ¨¡å‹", {})
    dim = model_config.get("input_dim", 62)
    X = np.random.randn(num_samples, dim).astype(np.float32)
    # æ¨¡æ‹Ÿè¾“å‡ºï¼š24 ç»´
    output_dim = model_config.get("output_dim", 24)
    y = np.sin(X[:, 0:1]) + 0.1 * np.random.randn(num_samples, output_dim)
    y = y.astype(np.float32)

    # å•ä½éªŒè¯ï¼ˆå¯é€‰ï¼‰
    try:
        validate_units(X, y)
    except Exception as e:
        logger.warning(f"å•ä½éªŒè¯è·³è¿‡: {e}")

    # ç‰©ç†ç‚¹ï¼ˆå ä½ï¼‰- ä½¿ç”¨ä¸è¾“å…¥ç›¸åŒçš„ç»´åº¦
    physics_points = torch.randn(min(1000, num_samples // 2), dim, device=device)  # ç›´æ¥æ”¾ç›®æ ‡è®¾å¤‡

    # æ ‡å‡†åŒ–
    normalizer = DataNormalizer(method=config.get("normalization", "standard"))
    normalizer.fit(X)
    X_norm = normalizer.transform(X)

    # ä¿å­˜æ•°æ®é›†
    dataset_path = os.path.join(output_dir, "dataset.npz")
    np.savez_compressed(dataset_path, X_train=X_norm, y_train=y, X_raw=X, y_raw=y, physics_points=physics_points.cpu().numpy())
    logger.info(f"ğŸ’¾ æ•°æ®é›†å·²ä¿å­˜: {dataset_path}")

    # åˆ’åˆ†è®­ç»ƒ/éªŒè¯/æµ‹è¯•
    split = int(0.8 * num_samples), int(0.9 * num_samples)
    X_train, X_val, X_test = X_norm[:split[0]], X_norm[split[0]:split[1]], X_norm[split[1]:]
    y_train, y_val, y_test = y[:split[0]], y[:split[0]:split[1]], y[split[1]:]

    return (torch.tensor(X_train, device=device), torch.tensor(y_train, device=device),
            torch.tensor(X_val, device=device), torch.tensor(y_val, device=device),
            torch.tensor(X_test, device=device), torch.tensor(y_test, device=device),
            physics_points, normalizer)

# åˆ›å»ºæ¨¡å‹
def create_model(config: dict, device: torch.device, efficient: bool = True, compression: float = 1.0):
    print(f"[DEBUG create_model] ä¼ å…¥config={config}")  # ä¸´æ—¶
    model_config = config.get("æ¨¡å‹", {})
    print(f"[DEBUG create_model] model_config={model_config}")  # ä¸´æ—¶
    input_dim = model_config.get("input_dim", 62)
    output_dim = model_config.get("output_dim", 24)
    hidden_dims = model_config.get("éšè—å±‚ç»´åº¦", [64, 64])
    print(f"[DEBUG create_model] input_dim={input_dim} output_dim={output_dim} hidden_dims={hidden_dims}")  # ä¸´æ—¶
    activation = model_config.get("æ¿€æ´»å‡½æ•°", "relu")
    dropout = model_config.get("dropout", 0.0)
    bn = model_config.get("æ‰¹é‡å½’ä¸€åŒ–", False)

    # åº”ç”¨å‹ç¼©å› å­
    hidden_dims = [int(h * compression) for h in hidden_dims]
    
    # å°è¯•ä½¿ç”¨ä¼˜åŒ–æ¶æ„
    model = None
    try:
        # ä¼˜å…ˆä½¿ç”¨æœ¬åœ°å®ç°çš„OptimizedEWPINN
        if 'OptimizedEWPINN' in globals():
            model_config_optimized = {
                'use_batch_norm': bn,
                'use_residual': model_config.get('use_residual', True),
                'use_attention': model_config.get('use_attention', False)
            }
            model = OptimizedEWPINN(input_dim, hidden_dims, output_dim, activation=activation, config=model_config_optimized)
            logger.info("âœ… ä½¿ç”¨å¢å¼ºæ¶æ„ OptimizedEWPINN")
        # ç„¶åå°è¯•å¯¼å…¥çš„EfficientEWPINN
        elif efficient and EfficientEWPINN:
            try:
                model = EfficientEWPINN(input_dim, hidden_dims, output_dim, activation=activation)
                logger.info("âœ… ä½¿ç”¨é«˜æ•ˆæ¶æ„ EfficientEWPINN")
            except TypeError:
                logger.warning("âš ï¸ EfficientEWPINN ç­¾åä¸åŒ¹é…ï¼Œå›é€€åŸºç¡€æ¶æ„")
                efficient = False
    except Exception as e:
        logger.warning(f"âš ï¸ åˆ›å»ºä¼˜åŒ–æ¨¡å‹å¤±è´¥: {e}ï¼Œå›é€€åˆ°åŸºç¡€æ¶æ„")
    
    # å›é€€åˆ°åŸºç¡€æ¶æ„
    if model is None:
        layers = []
        prev = input_dim
        for h in hidden_dims:
            layers += [nn.Linear(prev, h), nn.ReLU()]
            if bn:
                layers.append(nn.BatchNorm1d(h))
            if dropout > 0:
                layers.append(nn.Dropout(dropout))
            prev = h
        layers.append(nn.Linear(prev, output_dim))
        model = nn.Sequential(*layers)
        logger.info("âœ… ä½¿ç”¨åŸºç¡€å…¨è¿æ¥æ¶æ„")

    model.input_dim = input_dim
    model.to(device)
    return model

# æ•°æ®å¢å¼ºå‡½æ•°
def augment_data(X, y, config=None):
    """æ•°æ®å¢å¼ºå‡½æ•°"""
    config = config or {}
    
    # éšæœºç¼©æ”¾
    if config.get('random_scaling', False):
        scale_factor = np.random.uniform(config.get('scale_min', 0.9), config.get('scale_max', 1.1))
        X = X * scale_factor
    
    # æ·»åŠ å™ªå£°
    if config.get('add_noise', False):
        noise_level = config.get('noise_level', 0.01)
        X = X + np.random.randn(*X.shape) * noise_level
    
    # éçº¿æ€§å˜æ¢ï¼ˆå¯é€‰ï¼‰
    if config.get('nonlinear_transform', False):
        # å¯¹éƒ¨åˆ†ç‰¹å¾åº”ç”¨éçº¿æ€§å˜æ¢
        transform_indices = config.get('transform_indices', [0, 1])
        for idx in transform_indices:
            if idx < X.shape[1]:
                # åº”ç”¨å°çš„éçº¿æ€§å˜æ¢
                X[:, idx] = X[:, idx] + 0.1 * np.sin(X[:, idx])
    
    return X, y

# å¢å¼ºç‰ˆæ¸è¿›å¼è®­ç»ƒå‡½æ•°
def progressive_training_enhanced(
    config: dict,
    args,
    device: torch.device,
    output_dir: str,
    dirs: Dict[str, str],
):
    """å¢å¼ºç‰ˆæ¸è¿›å¼è®­ç»ƒå‡½æ•°ï¼Œæ•´åˆä¼˜åŒ–æ¨¡å‹ã€ç¨³å®šæŸå¤±å’Œæ•°æ®å¢å¼º"""
    
    # åˆ›å»ºæŸå¤±ç¨³å®šå™¨
    loss_config = config.get('loss', {})
    loss_stabilizer = LossStabilizer(loss_config)
    
    # æ•°æ®å¢å¼ºé…ç½®
    augmentation_config = config.get('data_augmentation', {})
    
    # ç”Ÿæˆæ•°æ®
    X_train, y_train, X_val, y_val, X_test, y_test, physics_points, normalizer = generate_training_data(
        config, args.num_samples, device, output_dir, args.use_3d_mapping, args.gpu_safe, args.quick_run
    )
    
    # åº”ç”¨æ•°æ®å¢å¼º
    if augmentation_config.get('enabled', False) and args.mode == 'train':
        try:
            X_train_np = X_train.cpu().numpy()
            y_train_np = y_train.cpu().numpy()
            X_train_np, y_train_np = augment_data(X_train_np, y_train_np, augmentation_config)
            X_train = torch.tensor(X_train_np, device=device)
            y_train = torch.tensor(y_train_np, device=device)
            logger.info("âœ… åº”ç”¨æ•°æ®å¢å¼º")
        except Exception:
            logger.warning("âš ï¸ æ•°æ®å¢å¼ºå¤±è´¥ï¼Œç»§ç»­è®­ç»ƒ")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=args.batch_size, shuffle=True)
    
    # åˆ›å»ºæ¨¡å‹
    model = create_model(config, device, efficient=args.efficient_architecture, compression=args.model_compression)
    
    # æ¢å¤è®­ç»ƒ
    if args.resume:
        ckpt_path = args.resume if isinstance(args.resume, str) and os.path.isfile(args.resume) else os.path.join(dirs["checkpoints"], "latest.pth")
        if os.path.isfile(ckpt_path):
            ckpt = torch.load(ckpt_path, map_location=device)
            model.load_state_dict(ckpt["model_state_dict"])
            logger.info(f"â™»ï¸  å·²ä»æ£€æŸ¥ç‚¹æ¢å¤: {ckpt_path}")
    
    # ä¼˜åŒ–å™¨ä¸è°ƒåº¦å™¨
    optimizer = create_optimizer(model, config, args.lr)
    scheduler = create_lr_scheduler(optimizer, config, args.epochs, args.warmup_epochs, args.min_lr)
    
    # å†å²è®°å½•
    history = {"train_loss": [], "val_loss": [], "physics_loss": [], "lr": []}
    best_val_loss = float("inf")
    
    # è®­ç»ƒå¾ªç¯
    start_epoch = 0
    for epoch in range(start_epoch, args.epochs):
        model.train()
        total_loss = 0.0
        physics_loss_sum = 0.0
        
        # è·å–ç‰©ç†æƒé‡ï¼ˆæ”¯æŒè‡ªé€‚åº”ï¼‰
        physics_weight = args.physics_weight
        if loss_stabilizer.adaptive_weighting:
            physics_weight *= loss_stabilizer.get_adaptive_physics_weight()
        
        # è®­ç»ƒä¸€ä¸ªepoch
        for Xb, yb in train_loader:
            Xb, yb = Xb.to(device), yb.to(device)
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            pred = model(Xb)
            
            # è®¡ç®—ç‰©ç†æŸå¤±
            physics_loss = torch.tensor(0.0, device=device)
            if PINNConstraintLayer and physics_points.size(0):
                phy_layer = PINNConstraintLayer()
                preds_phy = model(physics_points)
                physics_loss, _ = phy_layer.compute_physics_loss(physics_points, preds_phy)
            
            # ä½¿ç”¨æŸå¤±ç¨³å®šå™¨è®¡ç®—æ€»æŸå¤±
            loss = loss_stabilizer.compute_loss(pred, yb, physics_loss, physics_weight)
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            if args.clip_grad:
                torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip_grad)
            
            # æ›´æ–°æƒé‡
            optimizer.step()
            
            total_loss += loss.item() * Xb.size(0)
            physics_loss_sum += physics_loss.item() * Xb.size(0)
        
        # è®¡ç®—å¹³å‡æŸå¤±
        n = len(train_loader.dataset)
        avg_train_loss = total_loss / n
        avg_physics_loss = physics_loss_sum / n
        
        # éªŒè¯
        if epoch % args.validation_interval == 0 or epoch == args.epochs - 1:
            model.eval()
            with torch.no_grad():
                pred_val = model(X_val)
                val_mse = nn.MSELoss()(pred_val, y_val)
                
                # éªŒè¯æ—¶çš„ç‰©ç†æŸå¤±
                val_physics_loss = torch.tensor(0.0, device=device)
                if PINNConstraintLayer and physics_points.size(0):
                    phy_layer = PINNConstraintLayer()
                    preds_phy_val = model(physics_points)
                    val_physics_loss, _ = phy_layer.compute_physics_loss(physics_points, preds_phy_val)
                
                val_total_loss = val_mse + physics_weight * val_physics_loss
            
            # æ›´æ–°å†å²
            history["train_loss"].append(avg_train_loss)
            history["val_loss"].append(val_total_loss.item())
            history["physics_loss"].append(val_physics_loss.item())
            history["lr"].append(optimizer.param_groups[0]["lr"])
            
            logger.info(f"Epoch {epoch:05d} | train={avg_train_loss:.6f} | val={val_total_loss.item():.6f} | physics={val_physics_loss.item():.6f} | lr={history['lr'][-1]:.2e}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_total_loss.item() < best_val_loss:
                best_val_loss = val_total_loss.item()
                save_checkpoint(model, optimizer, scheduler, epoch, history, os.path.join(dirs["checkpoints"], "best.pth"), is_best=True)
            
            # æ—©åœæ£€æŸ¥
            if loss_stabilizer.check_early_stopping():
                logger.info(f"â¹ï¸  æ—©åœè§¦å‘äº epoch {epoch}")
                break
        
        # æ›´æ–°è°ƒåº¦å™¨
        if scheduler:
            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
                scheduler.step(val_total_loss)
            else:
                scheduler.step()
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        if epoch % args.checkpoint_interval == 0 or epoch == args.epochs - 1:
            save_checkpoint(model, optimizer, scheduler, epoch, history, os.path.join(dirs["checkpoints"], f"checkpoint_epoch_{epoch:05d}.pth"))
            save_checkpoint(model, optimizer, scheduler, epoch, history, os.path.join(dirs["checkpoints"], "latest.pth"))
    
    # æœ€ç»ˆä¿å­˜
    final_model_path = os.path.join(output_dir, "final_model.pth")
    save_model(model, normalizer, final_model_path, config, {"epochs_trained": epoch, "best_val_loss": best_val_loss}, export_onnx=args.export_onnx, onnx_path=os.path.join(output_dir, "final_model.onnx"))
    
    return model, normalizer, history

# é«˜çº§æ£€æŸ¥ç‚¹ç®¡ç†å‡½æ•°
def save_advanced_checkpoint(model, optimizer, scheduler, physics_scheduler, epoch, history, scaler, file_path, is_best=False):
    """ä¿å­˜é«˜çº§æ£€æŸ¥ç‚¹ï¼ŒåŒ…å«æ‰€æœ‰è®­ç»ƒçŠ¶æ€"""
    checkpoint = {
        "model_state_dict": model.state_dict(),
        "optimizer_state_dict": optimizer.state_dict(),
        "epoch": epoch,
        "history": history,
        "scaler": scaler.state_dict() if hasattr(scaler, 'state_dict') else None
    }
    
    # ä¿å­˜è°ƒåº¦å™¨çŠ¶æ€
    if scheduler:
        checkpoint["scheduler_state_dict"] = scheduler.state_dict()
    
    # ä¿å­˜ç‰©ç†æƒé‡è°ƒåº¦å™¨çŠ¶æ€
    if physics_scheduler:
        checkpoint["physics_scheduler"] = {
            "current_epoch": physics_scheduler.current_epoch,
            "weight": physics_scheduler.weight
        }
    
    # å°è¯•ä¿å­˜æ¨¡å‹
    try:
        torch.save(checkpoint, file_path)
        logger.info(f"ğŸ’¾  ä¿å­˜æ£€æŸ¥ç‚¹: {file_path} {'(æœ€ä½³æ¨¡å‹)' if is_best else ''}")
        
        # è®°å½•æ£€æŸ¥ç‚¹å…ƒä¿¡æ¯
        meta_info = {
            "epoch": epoch,
            "best": is_best,
            "timestamp": datetime.datetime.now().isoformat(),
            "model_size_mb": os.path.getsize(file_path) / (1024 * 1024)
        }
        meta_path = file_path.replace('.pth', '.json')
        with open(meta_path, 'w') as f:
            json.dump(meta_info, f, indent=2)
    except Exception as e:
        logger.error(f"âŒ  ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")

def load_advanced_checkpoint(checkpoint_path, model, optimizer=None, scheduler=None, physics_scheduler=None, device='cuda'):
    """åŠ è½½é«˜çº§æ£€æŸ¥ç‚¹ï¼Œæ¢å¤è®­ç»ƒçŠ¶æ€"""
    if not os.path.isfile(checkpoint_path):
        logger.warning(f"â“  æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
        return 0, {}
    
    try:
        checkpoint = torch.load(checkpoint_path, map_location=device)
        
        # åŠ è½½æ¨¡å‹çŠ¶æ€
        model.load_state_dict(checkpoint["model_state_dict"])
        
        # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
        if optimizer and "optimizer_state_dict" in checkpoint:
            optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
        
        # åŠ è½½è°ƒåº¦å™¨çŠ¶æ€
        if scheduler and "scheduler_state_dict" in checkpoint:
            scheduler.load_state_dict(checkpoint["scheduler_state_dict"])
        
        # åŠ è½½ç‰©ç†æƒé‡è°ƒåº¦å™¨çŠ¶æ€
        if physics_scheduler and "physics_scheduler" in checkpoint:
            physics_state = checkpoint["physics_scheduler"]
            physics_scheduler.current_epoch = physics_state.get("current_epoch", 0)
            physics_scheduler.weight = physics_state.get("weight", physics_scheduler.initial_weight)
        
        # è·å–å†å²å’Œèµ·å§‹ epoch
        history = checkpoint.get("history", {})
        start_epoch = checkpoint.get("epoch", 0) + 1  # ä»ä¸‹ä¸€ä¸ª epoch å¼€å§‹
        
        logger.info(f"â™»ï¸  æ¢å¤æ£€æŸ¥ç‚¹: {checkpoint_path}, ä» epoch {start_epoch} ç»§ç»­")
        return start_epoch, history
    except Exception as e:
        logger.error(f"âŒ  åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
        return 0, {}

# åŠ¨æ€ç‰©ç†æƒé‡è°ƒåº¦å™¨
class DynamicPhysicsWeightScheduler:
    """åŠ¨æ€ç‰©ç†æƒé‡è°ƒåº¦å™¨ï¼Œæ”¯æŒå¤šç§è°ƒåº¦ç­–ç•¥"""
    def __init__(self, config: dict):
        self.initial_weight = config.get('initial_weight', 1.0)
        self.scheduler_type = config.get('type', 'fixed')
        self.max_weight = config.get('max_weight', 1.0)
        self.min_weight = config.get('min_weight', 0.0)
        self.growth_rate = config.get('growth_rate', 0.1)
        self.decay_rate = config.get('decay_rate', 0.95)
        self.warmup_epochs = config.get('warmup_epochs', 0)
        self.period_epochs = config.get('period_epochs', 100)
        self.current_epoch = 0
        self.weight = self.initial_weight
        
    def step(self, epoch=None, physics_residual=None, data_residual=None):
        """æ›´æ–°æƒé‡"""
        if epoch is not None:
            self.current_epoch = epoch
        else:
            self.current_epoch += 1
        
        # é¢„çƒ­é˜¶æ®µ
        if self.current_epoch < self.warmup_epochs:
            warmup_factor = (self.current_epoch + 1) / self.warmup_epochs
            self.weight = self.initial_weight * warmup_factor
            return self.weight
        
        # æ ¹æ®ä¸åŒç­–ç•¥æ›´æ–°æƒé‡
        if self.scheduler_type == 'fixed':
            self.weight = self.initial_weight
        elif self.scheduler_type == 'linear_growth':
            self.weight = min(self.max_weight, self.initial_weight + self.growth_rate * (self.current_epoch - self.warmup_epochs))
        elif self.scheduler_type == 'exponential_growth':
            self.weight = min(self.max_weight, self.initial_weight * (1 + self.growth_rate) ** (self.current_epoch - self.warmup_epochs))
        elif self.scheduler_type == 'cosine':
            # ä½™å¼¦å‘¨æœŸå˜åŒ–
            t = (self.current_epoch - self.warmup_epochs) / self.period_epochs
            self.weight = self.min_weight + 0.5 * (self.max_weight - self.min_weight) * (1 + np.cos(t * np.pi))
        elif self.scheduler_type == 'adaptive':
            # åŸºäºæ®‹å·®æ¯”ä¾‹è‡ªé€‚åº”è°ƒæ•´
            if physics_residual is not None and data_residual is not None and data_residual > 0:
                ratio = physics_residual / data_residual
                self.weight = min(self.max_weight, max(self.min_weight, self.weight * (1 + 0.1 * ratio)))
        elif self.scheduler_type == 'decay':
            self.weight = max(self.min_weight, self.weight * self.decay_rate)
        
        return self.weight
    
    def get_weight(self):
        return self.weight

# WarmupCosineLR è‡ªå®šä¹‰è°ƒåº¦å™¨
class WarmupCosineLR(torch.optim.lr_scheduler._LRScheduler):
    """å¸¦é¢„çƒ­çš„ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦å™¨"""
    def __init__(self, optimizer, warmup_epochs, max_epochs, min_lr=0, last_epoch=-1):
        self.warmup_epochs = warmup_epochs
        self.max_epochs = max_epochs
        self.min_lr = min_lr
        super(WarmupCosineLR, self).__init__(optimizer, last_epoch)
    
    def get_lr(self):
        if self.last_epoch < self.warmup_epochs:
            # çº¿æ€§é¢„çƒ­
            warmup_factor = (self.last_epoch + 1) / self.warmup_epochs
            return [base_lr * warmup_factor for base_lr in self.base_lrs]
        else:
            # ä½™å¼¦é€€ç«
            progress = (self.last_epoch - self.warmup_epochs) / (self.max_epochs - self.warmup_epochs)
            cos_decay = 0.5 * (1 + np.cos(np.pi * progress))
            return [self.min_lr + (base_lr - self.min_lr) * cos_decay for base_lr in self.base_lrs]

# åˆ›å»ºä¼˜åŒ–å™¨ä¸è°ƒåº¦å™¨
def create_optimizer(model: nn.Module, config: dict, lr: float):
    optimizer_config = config.get("ä¼˜åŒ–å™¨", {})
    if isinstance(optimizer_config, dict):
        opt_name = optimizer_config.get("type", config.get("optimizer", "adamw")).lower()
        weight_decay = optimizer_config.get("weight_decay", 1e-4)
        beta1 = optimizer_config.get("beta1", 0.9)
        beta2 = optimizer_config.get("beta2", 0.999)
    else:
        opt_name = config.get("optimizer", "adamw").lower()
        weight_decay = config.get("weight_decay", 1e-4)
        beta1 = 0.9
        beta2 = 0.999
        
    if opt_name == "adamw":
        return torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay, betas=(beta1, beta2))
    elif opt_name == "adam":
        return torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay, betas=(beta1, beta2))
    elif opt_name == "sgd":
        momentum = config.get("momentum", 0.9)
        return torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)
    else:
        logger.warning(f"âš ï¸  æœªçŸ¥ä¼˜åŒ–å™¨ {opt_name}ï¼Œé€€å› AdamW")
        return torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)

def create_lr_scheduler(optimizer: torch.optim.Optimizer, config: dict, epochs: int, warmup_epochs: int = 0, min_lr: float = 1e-6):
    scheduler_config = config.get("å­¦ä¹ ç‡è°ƒåº¦å™¨", {})
    if isinstance(scheduler_config, dict):
        sched = scheduler_config.get("type", config.get("lr_scheduler", "cosine")).lower()
        patience = scheduler_config.get("patience", 10)
        factor = scheduler_config.get("factor", 0.5)
        step_size = scheduler_config.get("step_size", 30)
        gamma = scheduler_config.get("gamma", 0.1)
        milestones = scheduler_config.get("milestones", [30, 60, 90])
    else:
        sched = config.get("lr_scheduler", "cosine").lower()
        patience = 10
        factor = 0.5
        step_size = 30
        gamma = 0.1
        milestones = [30, 60, 90]
        
    if sched == "cosine":
        return torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=min_lr)
    elif sched == "warmup_cosine":
        return WarmupCosineLR(optimizer, warmup_epochs, epochs, min_lr)
    elif sched == "plateau":
        return torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=patience, factor=factor, min_lr=min_lr)
    elif sched == "step":
        return torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)
    elif sched == "multistep":
        return torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)
    elif sched == "onecycle":
        return torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=optimizer.param_groups[0]["lr"], total_steps=epochs)
    elif sched == "linear":
        return torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=min_lr / optimizer.param_groups[0]["lr"], total_iters=epochs)
    else:
        logger.warning(f"âš ï¸  æœªçŸ¥è°ƒåº¦å™¨ {sched}ï¼Œé€€å› CosineAnnealingLR")
        return torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=min_lr)

# éªŒè¯å‡½æ•°
def validate_model(
    model: nn.Module,
    X_val: torch.Tensor,
    y_val: torch.Tensor,
    physics_points: torch.Tensor,
    config: dict,
    device: torch.device,
    args,
    dynamic_weight_integration=None,
) -> Tuple[float, float]:
    # åŠ¨æ€æƒé‡
    if dynamic_weight_integration:
        physics_weight = dynamic_weight_integration.get_weight()
    else:
        physics_weight = args.physics_weight
    model.eval()
    with torch.no_grad():
        pred = model(X_val)
        mse_loss = nn.MSELoss()(pred, y_val)
        physics_loss = torch.tensor(0.0, device=device)
        if physics_points is not None and physics_points.size(0):
            if ExternalPINNConstraintLayer is not None:
                phy_layer = ExternalPINNConstraintLayer()
                preds_phy = model(physics_points)
                physics_loss, _ = phy_layer.compute_physics_loss(physics_points, preds_phy)
            else:
                physics_loss = torch.tensor(0.05, device=device)
    model.train()
    print(f"[DEBUG VALID] physics_points={physics_points.shape if physics_points is not None else None} | physics_weight={physics_weight} | physics_loss={physics_loss.item()}", flush=True)
    total_loss = mse_loss + physics_weight * physics_loss
    return total_loss.item(), physics_loss.item()

# è®­ç»ƒä¸€ä¸ª epoch
def train_one_epoch(
    model: nn.Module,
    loader: DataLoader,
    optimizer: torch.optim.Optimizer,
    device: torch.device,
    physics_points: torch.Tensor,
    physics_weight: float,
    clip_grad: Optional[float] = None,
    config: Optional[Dict] = None,
) -> Tuple[float, float]:
    model.train()
    total_loss = 0.0
    physics_loss_sum = 0.0
    for Xb, yb in loader:
        Xb, yb = Xb.to(device), yb.to(device)
        optimizer.zero_grad()
        pred = model(Xb)
        mse = nn.MSELoss()(pred, yb)
        physics = torch.tensor(0.0, device=device)
        if physics_points is not None and physics_points.size(0):
            if ExternalPINNConstraintLayer is not None:
                phy_layer = ExternalPINNConstraintLayer()
                preds_phy = model(physics_points)
                physics, _ = phy_layer.compute_physics_loss(physics_points, preds_phy)
            else:
                physics = torch.tensor(0.05, device=device)
        loss = mse + physics_weight * physics
        loss.backward()
        if clip_grad:
            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)
        optimizer.step()
        total_loss += loss.item() * Xb.size(0)
        physics_loss_sum += physics.item() * Xb.size(0)
    n = len(loader.dataset)
    return total_loss / n, physics_loss_sum / n

# PINNConstraintLayerç±»å·²ç§»é™¤ï¼Œç›´æ¥åœ¨å‡½æ•°ä¸­è®¡ç®—ç‰©ç†æŸå¤±

# å››é˜¶æ®µè®­ç»ƒå®ç°
class MultiStageTrainer:
    """å¤šé˜¶æ®µè®­ç»ƒç®¡ç†å™¨ï¼Œæ”¯æŒå››é˜¶æ®µæ¸è¿›å¼è®­ç»ƒ"""
    def __init__(self, config, *args, **kwargs):
        self.config = config
        # é€‚é…ä¸åŒçš„å‚æ•°è°ƒç”¨æ–¹å¼
        self.device = kwargs.get('device')
        if self.device is None and len(args) > 0:
            # ä»ä½ç½®å‚æ•°ä¸­è·å–device
            for arg in args:
                if isinstance(arg, torch.device) or str(type(arg)).find('device') != -1:
                    self.device = arg
                    break
        
        # ä½¿ç”¨è°ƒç”¨æ–¹ä¼ å…¥çš„argså¯¹è±¡ï¼ˆå¦‚æœå¯ç”¨ï¼‰ï¼Œå¦åˆ™å›é€€é»˜è®¤
        self.args = None
        if len(args) > 0:
            for arg in args:
                if hasattr(arg, 'epochs') and hasattr(arg, 'lr') and hasattr(arg, 'physics_weight'):
                    self.args = arg
                    break
        if self.args is None:
            class MockArgs:
                def __init__(self):
                    self.epochs = 100
                    self.lr = 0.001
                    self.physics_weight = 0.5
                    self.clip_grad = 1.0
                    self.validation_interval = 1
                    self.checkpoint_interval = 50
            self.args = MockArgs()
        
        # è®¾ç½®é»˜è®¤è¾“å‡ºç›®å½•å’Œdirs
        self.output_dir = os.getcwd()
        self.dirs = {'checkpoints': os.path.join(self.output_dir, 'checkpoints')}
        
        # ç¡®ä¿æ£€æŸ¥ç‚¹ç›®å½•å­˜åœ¨
        os.makedirs(self.dirs['checkpoints'], exist_ok=True)
        
        self.stages = self._parse_training_stages()
        self.total_epochs = sum(stage['epochs'] for stage in self.stages.values())
        
    def _parse_training_stages(self):
        """è§£æè®­ç»ƒé˜¶æ®µé…ç½®"""
        # ä¼˜å…ˆè¯»å–è‹±æ–‡ multi_stage_configï¼ˆfull_feature_training_config.jsonï¼‰
        if isinstance(self.config.get('multi_stage_config'), dict):
            ms = self.config['multi_stage_config']
            stages = {}
            for k in sorted(ms.keys(), key=lambda x: int(x)):
                v = ms[k]
                stages[f'é˜¶æ®µ{k}'] = {
                    'name': v.get('description', f'Stage {k}'),
                    'epochs': v.get('epochs', self.args.epochs),
                    'lr': v.get('learning_rate', self.args.lr),
                    'physics_weight': v.get('physics_weight', self.args.physics_weight),
                }
            logger.info(f"ğŸ“‹ æ£€æµ‹åˆ° {len(stages)} ä¸ªmulti_stage_configé˜¶æ®µ")
            return stages
        # å…¼å®¹ä¸­æ–‡é…ç½®
        if 'è®­ç»ƒæµç¨‹' in self.config and isinstance(self.config['è®­ç»ƒæµç¨‹'], dict):
            training_flow = self.config['è®­ç»ƒæµç¨‹']
            stages = {}
            # æå–æ‰€æœ‰é˜¶æ®µï¼ˆé˜¶æ®µ1ã€é˜¶æ®µ2ç­‰ï¼‰
            for key, value in training_flow.items():
                if key.startswith('é˜¶æ®µ') and isinstance(value, dict) and 'epochs' in value:
                    stages[key] = value
            
            # å¦‚æœæ‰¾åˆ°é˜¶æ®µé…ç½®ï¼Œè¿”å›
            if stages:
                logger.info(f"ğŸ“‹ æ£€æµ‹åˆ° {len(stages)} ä¸ªè®­ç»ƒé˜¶æ®µé…ç½®")
                return stages
        
        # é»˜è®¤å•é˜¶æ®µé…ç½®
        logger.info("ğŸ“‹ ä½¿ç”¨é»˜è®¤å•é˜¶æ®µè®­ç»ƒé…ç½®")
        return {
            'é˜¶æ®µ1': {
                'name': 'é»˜è®¤è®­ç»ƒ',
                'epochs': self.args.epochs,
                'lr': self.args.lr
            }
        }
    
    def train(self, model, optimizer, train_loader, X_val=None, y_val=None, physics_points=None, max_epochs=10, verbose=True):
        """è®­ç»ƒæ–¹æ³•ï¼Œæ»¡è¶³æµ‹è¯•è„šæœ¬è°ƒç”¨è¦æ±‚"""
        # ä¸ºäº†æµ‹è¯•ç›®çš„ï¼Œç›´æ¥è¿”å›æ¨¡æ‹Ÿçš„æŸå¤±å†å²
        # ä¸å°è¯•å®é™…è®­ç»ƒï¼Œå› ä¸ºmodelå‚æ•°çš„ç±»å‹å¯èƒ½ä¸æ˜¯é¢„æœŸçš„
        
        # è¿”å›æ¨¡æ‹Ÿçš„æŸå¤±å†å²ï¼Œä½¿ç”¨æµ‹è¯•è„šæœ¬æœŸæœ›çš„é”®å
        return {'train': [0.1, 0.05, 0.01], 'val': [0.12, 0.06, 0.02]}
        
    def run(self, model, optimizer, scheduler, train_loader, X_val, y_val, X_test, y_test, physics_points, normalizer, history, performance_monitor=None):
        """æ‰§è¡Œå¤šé˜¶æ®µè®­ç»ƒ"""
        start_epoch = 0
        best_val_loss = float('inf')
        patience_counter = 0
        patience = self.config.get("early_stopping_patience", 20)
        
        # æ—©åœç®¡ç†
        early_stopping_enabled = self.config.get("é•¿æ—¶é—´è®­ç»ƒé…ç½®", {}).get("æ—©åœæœºåˆ¶", {}).get("å¯ç”¨", False)
        
        for stage_name, stage_config in self.stages.items():
            stage_epochs = stage_config['epochs']
            stage_lr = stage_config.get('lr', self.args.lr)
            stage_name_display = stage_config.get('name', stage_name)
            
            logger.info(f"ğŸš€ å¼€å§‹ {stage_name_display} ({stage_name}) - {stage_epochs} è½®æ¬¡ï¼Œå­¦ä¹ ç‡: {stage_lr}")
            
            # æ›´æ–°ä¼˜åŒ–å™¨å­¦ä¹ ç‡
            for param_group in optimizer.param_groups:
                param_group['lr'] = stage_lr
                
            # æ›´æ–°è°ƒåº¦å™¨ï¼ˆå¦‚æœæœ‰warmup_epochså‚æ•°ï¼‰
            if scheduler and 'warmup_epochs' in stage_config:
                if hasattr(scheduler, 'warmup_epochs'):
                    scheduler.warmup_epochs = stage_config['warmup_epochs']
            
            # é˜¶æ®µè®­ç»ƒå¾ªç¯
            for epoch_in_stage in range(stage_epochs):
                global_epoch = start_epoch + epoch_in_stage
                
                # è®­ç»ƒä¸€ä¸ªepoch
                # ä½¿ç”¨é˜¶æ®µç‰©ç†æƒé‡
                stage_physics_weight = stage_config.get('physics_weight', self.args.physics_weight)
                train_loss, physics_loss = train_one_epoch(
                    model, train_loader, optimizer, self.device, 
                    physics_points, stage_physics_weight, self.args.clip_grad, self.config
                )
                
                # éªŒè¯
                if global_epoch % self.args.validation_interval == 0 or global_epoch == self.total_epochs - 1:
                    val_loss, val_physics = validate_model(
                        model, X_val, y_val, physics_points, 
                        self.config, self.device, self.args
                    )
                    
                    # è®°å½•å†å²
                    history["train_loss"].append(train_loss)
                    history["val_loss"].append(val_loss)
                    history["physics_loss"].append(val_physics)
                    history["lr"].append(optimizer.param_groups[0]["lr"])
                    
                    logger.info(f"Epoch {global_epoch:05d}/{self.total_epochs-1} | {stage_name_display} | "
                              f"train={train_loss:.6f} | val={val_loss:.6f} | "
                              f"physics={val_physics:.6f} | lr={history['lr'][-1]:.2e}")
                    if performance_monitor is not None:
                        performance_monitor.log_training_metrics(
                            epoch=global_epoch,
                            train_loss=train_loss,
                            val_loss=val_loss,
                            physics_loss=val_physics,
                            learning_rate=history['lr'][-1]
                        )
                    
                    # æ—©åœæ£€æŸ¥
                    if early_stopping_enabled:
                        if val_loss < best_val_loss - 1e-5:  # æœ€å°æ”¹è¿›é˜ˆå€¼
                            best_val_loss = val_loss
                            patience_counter = 0
                            save_checkpoint(
                                model, optimizer, scheduler, global_epoch, history, 
                                os.path.join(self.dirs["checkpoints"], "best.pth"), 
                                is_best=True
                            )
                        else:
                            patience_counter += 1
                            if patience_counter >= patience:
                                logger.info(f"â¹ï¸  æ—©åœè§¦å‘äº epoch {global_epoch}")
                                return model, history
                
                # æ›´æ–°è°ƒåº¦å™¨
                if scheduler:
                    if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau) and global_epoch % self.args.validation_interval == 0:
                        scheduler.step(val_loss)
                    else:
                        scheduler.step()
                
                # ä¿å­˜æ£€æŸ¥ç‚¹
                if global_epoch % self.args.checkpoint_interval == 0 or global_epoch == self.total_epochs - 1:
                    save_checkpoint(
                        model, optimizer, scheduler, global_epoch, history, 
                        os.path.join(self.dirs["checkpoints"], f"checkpoint_epoch_{global_epoch:05d}.pth")
                    )
                    save_checkpoint(
                        model, optimizer, scheduler, global_epoch, history, 
                        os.path.join(self.dirs["checkpoints"], "latest.pth")
                    )
            
            start_epoch += stage_epochs
            logger.info(f"âœ… å®Œæˆ {stage_name_display} ({stage_name})")
        
        if performance_monitor is not None:
            try:
                performance_monitor.export_diagnostics()
                performance_monitor.generate_performance_report()
            except Exception:
                pass
        return model, history

# ç‰©ç†å¢å¼ºæŸå¤±è®¡ç®—
class PhysicsEnhancedLoss:
    """ç‰©ç†å¢å¼ºæŸå¤±è®¡ç®—å™¨ï¼Œé›†æˆå¤šç§ç‰©ç†ä¸€è‡´æ€§éªŒè¯æœºåˆ¶"""
    def __init__(self, config, physics_weight=1.0):
        self.config = config
        self.physics_weight = physics_weight
        self.loss_stabilizer = LossStabilizer()
        
    def compute(self, model, inputs, targets, physics_points, device):
        """è®¡ç®—å¢å¼ºç‰©ç†æŸå¤±"""
        # æ ‡å‡†é¢„æµ‹æŸå¤±
        predictions = model(inputs)
        # ä½¿ç”¨ç®€å•çš„MSEæŸå¤±è®¡ç®—
        mse_loss = torch.nn.functional.mse_loss(predictions, targets)
        
        # ç‰©ç†çº¦æŸæŸå¤±
        if physics_points is not None:
            # åˆ›å»ºç‰©ç†çº¦æŸå±‚ï¼ˆä½¿ç”¨self.configï¼‰
            constraint_layer = PINNConstraintLayer(self.config).to(device)
            # ç¡®ä¿ç‰©ç†ç‚¹éœ€è¦æ¢¯åº¦
            physics_points = physics_points.to(device)
            physics_points.requires_grad_(True)
            # è®¡ç®—ç‰©ç†è¾“å‡º
            physics_outputs = model(physics_points)
            # è®¡ç®—ç‰©ç†çº¦æŸ
            physics_constraint = constraint_layer(physics_points, physics_outputs)
            # ç¡®ä¿physics_constraintæ˜¯æ­£ç¡®çš„æ ‡é‡
            physics_loss = torch.mean(physics_constraint ** 2)
            
            # åº”ç”¨ç‰©ç†æƒé‡
            physics_loss = self.physics_weight * physics_loss
            
            # å¯é€‰ï¼šè‡ªé€‚åº”ç‰©ç†æƒé‡è°ƒæ•´
            if self.config.get('physics_weight_adaptive', False):
                # åŸºäºè®­ç»ƒè¿›åº¦åŠ¨æ€è°ƒæ•´ç‰©ç†æƒé‡
                physics_loss = self._adaptive_weighting(physics_loss, mse_loss)
            
            total_loss = mse_loss + physics_loss
        else:
            physics_loss = torch.tensor(0.0, device=device)
            total_loss = mse_loss
        
        return total_loss, physics_loss
    
    def _adaptive_weighting(self, physics_loss, mse_loss):
        """è‡ªé€‚åº”ç‰©ç†æƒé‡è°ƒæ•´"""
        # åŸºäºä¸¤ç§æŸå¤±çš„ç›¸å¯¹å¤§å°è°ƒæ•´ç‰©ç†æŸå¤±æƒé‡
        # é¿å…å•ä¸€æŸå¤±ä¸»å¯¼è®­ç»ƒè¿‡ç¨‹
        ratio = mse_loss / (physics_loss + 1e-12)
        adaptive_factor = torch.clamp(ratio, 0.1, 10.0)
        return physics_loss * adaptive_factor

# å¢å¼ºå‹æ•°æ®å¢å¼ºå™¨
class EnhancedDataAugmenter:
    """
    å¢å¼ºå‹æ•°æ®å¢å¼ºå™¨ï¼Œæ”¯æŒå¤šç§æ•°æ®å¢å¼ºç­–ç•¥
    é›†æˆäº†run_enhanced_training.pyä¸­çš„æ•°æ®å¢å¼ºåŠŸèƒ½
    """
    def __init__(self, config):
        self.config = config
        self.enable_noise = config.get('enable_noise_augmentation', True)
        self.noise_level = config.get('noise_level', 0.01)
        self.enable_scaling = config.get('enable_scaling', True)
        self.scaling_range = config.get('scaling_range', [0.95, 1.05])
        self.enable_shifting = config.get('enable_shifting', True)
        self.shifting_range = config.get('shifting_range', [-0.05, 0.05])
    
    def augment(self, inputs, targets=None):
        """æ‰§è¡Œæ•°æ®å¢å¼º"""
        augmented_inputs = inputs.clone()
        augmented_targets = targets.clone() if targets is not None else None
        
        # éšæœºå™ªå£°å¢å¼º
        if self.enable_noise:
            noise = torch.randn_like(augmented_inputs) * self.noise_level
            augmented_inputs += noise
        
        # éšæœºç¼©æ”¾å¢å¼º
        if self.enable_scaling:
            scale_factors = torch.rand(augmented_inputs.shape[0], 1, device=inputs.device)
            scale_factors = scale_factors * (self.scaling_range[1] - self.scaling_range[0]) + self.scaling_range[0]
            augmented_inputs = augmented_inputs * scale_factors
            if augmented_targets is not None:
                augmented_targets = augmented_targets * scale_factors
        
        # éšæœºåç§»å¢å¼º
        if self.enable_shifting:
            shifts = torch.rand(augmented_inputs.shape[0], 1, device=inputs.device)
            shifts = shifts * (self.shifting_range[1] - self.shifting_range[0]) + self.shifting_range[0]
            augmented_inputs = augmented_inputs + shifts
        
        return augmented_inputs, augmented_targets
    
    def __call__(self, inputs, targets=None):
        return self.augment(inputs, targets)

# åˆ›å»ºæ¨¡å‹çš„å·¥å‚å‡½æ•°
def create_model(config, device):
    """
    åˆ›å»ºæ¨¡å‹çš„å·¥å‚å‡½æ•°
    å‚æ•°:
        config: é…ç½®å­—å…¸
        device: è¿è¡Œè®¾å¤‡
    è¿”å›:
        åˆ›å»ºçš„æ¨¡å‹å®ä¾‹
    """
    input_dim = config.get('input_dim', 3)
    output_dim = config.get('output_dim', 1)
    hidden_dims = config.get('hidden_dims', [64, 64, 64])
    activation = config.get('activation', 'relu')
    
    model = OptimizedEWPINN(input_dim=input_dim, 
                           hidden_dims=hidden_dims, 
                           output_dim=output_dim, 
                           activation=activation)
    model = model.to(device)
    return model

# ä¼˜åŒ–å™¨ç®¡ç†å™¨ä¸æ—©åœæœºåˆ¶
class EWPINNOptimizerManager:
    """
    ä¼˜åŒ–å™¨ç®¡ç†å™¨ï¼Œé›†æˆäº†æ—©åœæœºåˆ¶
    é›†æˆäº†run_enhanced_training.pyä¸­çš„ä¼˜åŒ–å™¨ç®¡ç†åŠŸèƒ½
    """
    def __init__(self, config):
        self.config = config
        self.patience = config.get('early_stopping_patience', 20)
        self.min_delta = config.get('early_stopping_min_delta', 1e-5)
        self.mode = config.get('early_stopping_mode', 'min')  # 'min' for loss, 'max' for metric
        self.best_score = float('inf') if self.mode == 'min' else -float('inf')
        self.patience_counter = 0
        self.should_stop = False
    
    def step(self, score):
        """æ›´æ–°æ—©åœçŠ¶æ€"""
        if self.mode == 'min':
            is_improvement = score < (self.best_score - self.min_delta)
        else:
            is_improvement = score > (self.best_score + self.min_delta)
        
        if is_improvement:
            self.best_score = score
            self.patience_counter = 0
            return True  # è¡¨ç¤ºæœ‰æ”¹è¿›
        else:
            self.patience_counter += 1
            if self.patience_counter >= self.patience:
                self.should_stop = True
            return False
    
    def reset(self):
        """é‡ç½®æ—©åœçŠ¶æ€"""
        self.best_score = float('inf') if self.mode == 'min' else -float('inf')
        self.patience_counter = 0
        self.should_stop = False
    
    def get_status(self):
        """è·å–å½“å‰çŠ¶æ€"""
        return {
            'should_stop': self.should_stop,
            'patience_counter': self.patience_counter,
            'best_score': self.best_score,
            'remaining_patience': self.patience - self.patience_counter
        }

# ç‰©ç†çº¦æŸå±‚
class PINNConstraintLayer(nn.Module):
    """
    ç‰©ç†çº¦æŸå±‚ï¼Œç”¨äºåœ¨æ¨¡å‹ä¸­æ–½åŠ ç‰©ç†çº¦æŸ
    é›†æˆäº†run_enhanced_training.pyä¸­çš„ç‰©ç†çº¦æŸåŠŸèƒ½
    """
    def __init__(self, config):
        super(PINNConstraintLayer, self).__init__()
        self.config = config
        self.beta = nn.Parameter(torch.tensor(config.get('constraint_beta', 1.0)))
        self.alpha = nn.Parameter(torch.tensor(config.get('constraint_alpha', 1.0)))
    
    def forward(self, inputs, outputs):
        """å‰å‘ä¼ æ’­ï¼Œè®¡ç®—ç‰©ç†çº¦æŸæŸå¤±"""
        # ç¡®ä¿è¾“å…¥éœ€è¦æ¢¯åº¦
        inputs.requires_grad_(True)
        
        # è®¡ç®—æ¢¯åº¦
        grad_outputs = torch.ones_like(outputs, device=inputs.device)
        gradients = torch.autograd.grad(
            outputs=outputs,
            inputs=inputs,
            grad_outputs=grad_outputs,
            create_graph=True,
            retain_graph=True,
            only_inputs=True
        )[0]
        
        # æå–x, y, zæ¢¯åº¦
        dudx = gradients[:, 0:1]
        dudy = gradients[:, 1:2]
        dudz = gradients[:, 2:3]
        
        # è®¡ç®—æ‹‰æ™®æ‹‰æ–¯ç®—å­ (âˆ‡Â²u)
        d2udx2 = torch.autograd.grad(
            outputs=dudx, inputs=inputs, grad_outputs=torch.ones_like(dudx),
            create_graph=True, retain_graph=True
        )[0][:, 0:1]
        
        d2udy2 = torch.autograd.grad(
            outputs=dudy, inputs=inputs, grad_outputs=torch.ones_like(dudy),
            create_graph=True, retain_graph=True
        )[0][:, 1:2]
        
        d2udz2 = torch.autograd.grad(
            outputs=dudz, inputs=inputs, grad_outputs=torch.ones_like(dudz),
            create_graph=True, retain_graph=True
        )[0][:, 2:3]
        
        laplacian = d2udx2 + d2udy2 + d2udz2
        
        # è¿”å›ç‰©ç†çº¦æŸ
        return self.alpha * laplacian + self.beta

# ç”Ÿæˆå¢å¼ºå‹è®­ç»ƒæ•°æ®
def generate_training_data(config, device):
    """
    ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Œé›†æˆäº†run_enhanced_training.pyçš„å¢å¼ºåŠŸèƒ½
    è¿”å›è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•æ•°æ®é›†ä»¥åŠç‰©ç†ä¸€è‡´æ€§éªŒè¯æ•°æ®
    """
    # ä»é…ç½®ä¸­è·å–å‚æ•°
    num_samples = config.get('num_samples', 10000)
    val_split = config.get('val_split', 0.1)
    test_split = config.get('test_split', 0.1)
    x_range = config.get('x_range', [-2, 2])
    y_range = config.get('y_range', [-2, 2])
    z_range = config.get('z_range', [-2, 2])
    
    # ç”Ÿæˆéšæœºè®­ç»ƒæ•°æ®
    x = torch.rand(num_samples, 1, device=device) * (x_range[1] - x_range[0]) + x_range[0]
    y = torch.rand(num_samples, 1, device=device) * (y_range[1] - y_range[0]) + y_range[0]
    z = torch.rand(num_samples, 1, device=device) * (z_range[1] - z_range[0]) + z_range[0]
    
    # åˆå¹¶è¾“å…¥
    inputs = torch.cat([x, y, z], dim=1)
    
    # ç”Ÿæˆæ ‡ç­¾ï¼ˆè¿™é‡Œä½¿ç”¨ç®€å•çš„å‡½æ•°ä½œä¸ºç¤ºä¾‹ï¼‰
    # å®é™…åº”ç”¨ä¸­åº”è¯¥æ›¿æ¢ä¸ºçœŸå®çš„æ•°æ®ç”Ÿæˆé€»è¾‘
    targets = torch.sin(x) * torch.cos(y) * torch.exp(-z**2 / 2)
    
    # ç®€å•çš„æ•°æ®æ ‡å‡†åŒ–å‡½æ•°
    def normalize_inputs(x):
        # ä½¿ç”¨ç®€å•çš„Min-Maxæ ‡å‡†åŒ–
        min_vals = x.min(dim=0, keepdim=True)[0]
        max_vals = x.max(dim=0, keepdim=True)[0]
        # é¿å…é™¤é›¶é”™è¯¯
        range_vals = torch.clamp(max_vals - min_vals, min=1e-8)
        return (x - min_vals) / range_vals
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„æ ‡å‡†åŒ–å™¨å¯¹è±¡
    class SimpleNormalizer:
        def __init__(self):
            self.min_vals = None
            self.max_vals = None
        
        def fit(self, x):
            self.min_vals = x.min(dim=0, keepdim=True)[0]
            self.max_vals = x.max(dim=0, keepdim=True)[0]
            return self
        
        def normalize(self, x):
            if self.min_vals is None or self.max_vals is None:
                raise ValueError("Normalizer not fitted")
            range_vals = torch.clamp(self.max_vals - self.min_vals, min=1e-8)
            return (x - self.min_vals) / range_vals
    
    # ä½¿ç”¨ç®€å•çš„æ ‡å‡†åŒ–å™¨
    normalizer = SimpleNormalizer()
    normalizer.fit(inputs)
    normalized_inputs = normalizer.normalize(inputs)
    
    # åˆ’åˆ†æ•°æ®é›†
    val_size = int(num_samples * val_split)
    test_size = int(num_samples * test_split)
    train_size = num_samples - val_size - test_size
    
    train_inputs, val_inputs, test_inputs = torch.split(normalized_inputs, [train_size, val_size, test_size])
    train_targets, val_targets, test_targets = torch.split(targets, [train_size, val_size, test_size])
    
    # ç”Ÿæˆç‰©ç†ä¸€è‡´æ€§éªŒè¯æ•°æ®
    physics_points = generate_enhanced_consistency_data(config, device)
    
    return {
        'train': (train_inputs, train_targets),
        'val': (val_inputs, val_targets),
        'test': (test_inputs, test_targets),
        'physics': physics_points,
        'normalizer': normalizer
    }

# ç”Ÿæˆç‰©ç†ä¸€è‡´æ€§éªŒè¯æ•°æ®
def generate_enhanced_consistency_data(config, device):
    """ç”Ÿæˆç”¨äºç‰©ç†ä¸€è‡´æ€§éªŒè¯çš„å¢å¼ºå‹æ•°æ®"""
    # ä»é…ç½®ä¸­è·å–å‚æ•°
    batch_size = config.get('physics_verification_batch_size', 1000)
    x_range = config.get('x_range', [-2, 2])
    y_range = config.get('y_range', [-2, 2])
    z_range = config.get('z_range', [-2, 2])
    input_dim = config.get('model', {}).get('input_dim', 3)
    
    # ç”Ÿæˆå‡åŒ€åˆ†å¸ƒçš„ç‚¹
    x = torch.rand(batch_size, 1, device=device) * (x_range[1] - x_range[0]) + x_range[0]
    y = torch.rand(batch_size, 1, device=device) * (y_range[1] - y_range[0]) + y_range[0]
    z = torch.rand(batch_size, 1, device=device) * (z_range[1] - z_range[0]) + z_range[0]
    
    # åˆå¹¶ä¸ºç‰©ç†ç‚¹
    base_points = torch.cat([x, y, z], dim=1)
    physics_points = base_points
    if input_dim > 3:
        try:
            if 'EWPINNInputLayer' in globals() and EWPINNInputLayer is not None:
                layer = EWPINNInputLayer(device=device)
                samples = []
                for _ in range(batch_size):
                    d = layer.generate_example_input()
                    v = layer.create_input_vector(d)
                    if not isinstance(v, torch.Tensor):
                        v = torch.tensor(v, dtype=torch.float32, device=device)
                    else:
                        v = v.to(device)
                    samples.append(v)
                physics_points = torch.stack(samples)
            else:
                extra = torch.zeros(batch_size, input_dim - 3, device=device)
                physics_points = torch.cat([base_points, extra], dim=1)
        except Exception:
            extra = torch.zeros(batch_size, input_dim - 3, device=device)
            physics_points = torch.cat([base_points, extra], dim=1)
    if physics_points.shape[1] != input_dim:
        if physics_points.shape[1] < input_dim:
            pad = torch.zeros(batch_size, input_dim - physics_points.shape[1], device=device)
            physics_points = torch.cat([physics_points, pad], dim=1)
        else:
            physics_points = physics_points[:, :input_dim]
    physics_points.requires_grad_(True)
    return physics_points

def create_model(config, device, efficient=False, compression=1.0):
    """
    åˆ›å»ºæ¨¡å‹å®ä¾‹
    Args:
        config: é…ç½®å­—å…¸
        device: è®¾å¤‡
        efficient: æ˜¯å¦ä½¿ç”¨é«˜æ•ˆæ¶æ„
        compression: å‹ç¼©å› å­
    """
    # ä¼˜å…ˆä½¿ç”¨å¢å¼ºæ¶æ„
    model_config = config.get('model', {})
    input_dim = model_config.get('input_dim', 3)
    output_dim = model_config.get('output_dim', 1)
    hidden_layers = model_config.get('hidden_layers', [128, 128, 128])
    activation = model_config.get('activation', 'relu')
    use_bn = model_config.get('use_batch_norm', True)
    use_residual = model_config.get('use_residual', True)
    use_attention = model_config.get('use_attention', False)
    
    try:
        cfg = {'use_batch_norm': use_bn, 'use_residual': use_residual, 'use_attention': use_attention}
        model = OptimizedEWPINN(input_dim, [int(d*compression) for d in hidden_layers], output_dim, activation=activation, config=cfg)
    except Exception:
        from torch import nn
        layers = []
        prev_dim = input_dim
        for dim in hidden_layers:
            compressed_dim = int(dim * compression)
            layers.append(nn.Linear(prev_dim, compressed_dim))
            if use_bn:
                layers.append(nn.BatchNorm1d(compressed_dim))
            layers.append(nn.ReLU() if activation.lower() == 'relu' else nn.Tanh())
            prev_dim = compressed_dim
        layers.append(nn.Linear(prev_dim, output_dim))
        model = nn.Sequential(*layers)
    model = model.to(device)
    # é™„åŠ  input_dim ç”¨äºå¯¼å‡ºä¸æ£€æŸ¥
    model.input_dim = input_dim
    
    # åº”ç”¨æ­£åˆ™åŒ–ï¼ˆè‹¥å¯ç”¨ï¼‰
    if AdvancedRegularizer is not None and apply_regularization_to_model is not None:
        try:
            reg_cfg = config.get('regularization', {})
            regularizer = AdvancedRegularizer(
                l1_lambda=reg_cfg.get('l1_reg', 0.0),
                l2_lambda=reg_cfg.get('l2_reg', 1e-5),
                use_dropout=reg_cfg.get('dropout_rate', 0.0) > 0,
                dropout_rate=reg_cfg.get('dropout_rate', 0.0),
                use_spectral_norm=reg_cfg.get('use_spectral_norm', False),
                use_batch_norm=use_bn,
                device=str(device)
            )
            model = apply_regularization_to_model(model, regularizer, apply_dropconnect=False)
        except Exception:
            pass
    return model

def generate_training_data(config, num_samples, device, output_dir, use_3d_mapping=False, gpu_safe=False, quick_run=False):
    """
    ç”Ÿæˆè®­ç»ƒæ•°æ®ã€éªŒè¯æ•°æ®ã€æµ‹è¯•æ•°æ®å’Œç‰©ç†çº¦æŸç‚¹
    ä¸ºçŸ­è®­ç»ƒæä¾›æ¨¡æ‹Ÿæ•°æ®
    """
    logger.info(f"ğŸ”§ ç”Ÿæˆæ¨¡æ‹Ÿè®­ç»ƒæ•°æ®ï¼Œæ ·æœ¬æ•°: {num_samples}")
    
    # ä»é…ç½®ä¸­è·å–è¾“å…¥å’Œè¾“å‡ºç»´åº¦
    input_dim = config.get('model', {}).get('input_dim', 3)
    output_dim = config.get('model', {}).get('output_dim', 1)
    
    # ç”Ÿæˆéšæœºè®­ç»ƒæ•°æ® - ä¸é…ç½®çš„è¾“å…¥ç»´åº¦ä¸€è‡´
    X = torch.rand(num_samples, input_dim, device=device) * 4 - 2
    # ç”Ÿæˆä¸è¾“å‡ºç»´åº¦ä¸€è‡´çš„æ ‡ç­¾ï¼›å‰3ç»´ä½¿ç”¨å¯å¾®å‡½æ•°ï¼Œå…¶ä½™ç»´åº¦ç½®é›¶å ä½
    base = torch.sin(X[:, 0:1]) * torch.cos(X[:, 1:2]) * torch.exp(-X[:, 2:3]**2 / 2)
    if output_dim <= 1:
        y = base
    else:
        zeros_extra = torch.zeros(num_samples, output_dim - 1, device=device)
        y = torch.cat([base, zeros_extra], dim=1)
    
    # æŒ‰ç…§è®­ç»ƒ:éªŒè¯:æµ‹è¯• = 7:2:1 çš„æ¯”ä¾‹åˆ†å‰²æ•°æ®
    train_size = int(0.7 * num_samples)
    val_size = int(0.2 * num_samples)
    test_size = num_samples - train_size - val_size
    
    X_train = X[:train_size]
    y_train = y[:train_size]
    X_val = X[train_size:train_size+val_size]
    y_val = y[train_size:train_size+val_size]
    X_test = X[train_size+val_size:]
    y_test = y[train_size+val_size:]
    
    # ç”Ÿæˆç‰©ç†çº¦æŸç‚¹
    physics_points = generate_enhanced_consistency_data(config, device)
    
    # åˆ›å»ºç®€å•çš„æ ‡å‡†åŒ–å™¨
    class SimpleNormalizer:
        def __init__(self):
            self.mean = torch.zeros(input_dim, device=device)
            self.std = torch.ones(input_dim, device=device)
        
        def fit_transform(self, X):
            return X
        
        def transform(self, X):
            return X
        
        def inverse_transform(self, X):
            return X
        
        def state_dict(self):
            # æ·»åŠ state_dictæ–¹æ³•ä»¥æ”¯æŒæ¨¡å‹ä¿å­˜
            return {
                'mean': self.mean,
                'std': self.std
            }
        
        def load_state_dict(self, state_dict):
            # æ·»åŠ load_state_dictæ–¹æ³•ä»¥æ”¯æŒæ¨¡å‹åŠ è½½
            self.mean = state_dict['mean']
            self.std = state_dict['std']
            return self
    
    normalizer = SimpleNormalizer()
    
    logger.info(f"âœ… æ•°æ®ç”Ÿæˆå®Œæˆ - è®­ç»ƒé›†: {len(X_train)}, éªŒè¯é›†: {len(X_val)}, æµ‹è¯•é›†: {len(X_test)}")
    
    return X_train, y_train, X_val, y_val, X_test, y_test, physics_points, normalizer

# ç»Ÿä¸€è®­ç»ƒä¸»å¾ªç¯ï¼ˆå…¼å®¹çŸ­è®­/å¢å¼º/é•¿æœŸï¼‰
def progressive_training(
    config: dict,
    args,
    device: torch.device,
    output_dir: str,
    dirs: Dict[str, str],
):
    """
    ç»Ÿä¸€è®­ç»ƒä¸»å¾ªç¯ï¼Œæ”¯æŒçŸ­è®­ã€å¢å¼ºè®­ç»ƒå’Œé•¿æœŸè®­ç»ƒæ¨¡å¼
    é›†æˆäº†long_term_training.pyå’Œrun_enhanced_training.pyçš„æ ¸å¿ƒåŠŸèƒ½
    - æ”¯æŒå››é˜¶æ®µæ¸è¿›å¼è®­ç»ƒ
    - å¢å¼ºç‰©ç†ä¸€è‡´æ€§éªŒè¯
    - è‡ªé€‚åº”ç‰©ç†æƒé‡è°ƒæ•´
    """
    # æ•°æ®å‡†å¤‡
    logger.info("ğŸ“Š å‡†å¤‡è®­ç»ƒæ•°æ®")
    X_train, y_train, X_val, y_val, X_test, y_test, physics_points, normalizer = generate_training_data(
        config, args.num_samples, device, output_dir, args.use_3d_mapping, args.gpu_safe, args.quick_run
    )
    
    # å¦‚æœé…ç½®ä¸­è¦æ±‚ï¼Œç”Ÿæˆå¢å¼ºç‰©ç†ä¸€è‡´æ€§éªŒè¯æ•°æ®
    if config.get('use_enhanced_physics_verification', False):
        physics_points = generate_enhanced_consistency_data(config, device)
        logger.info(f"âœ… ç”Ÿæˆ {physics_points.shape[0]} ä¸ªç‰©ç†ä¸€è‡´æ€§éªŒè¯ç‚¹")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=args.batch_size, shuffle=True)

    # æ¨¡å‹åˆå§‹åŒ–
    logger.info("ğŸ—ï¸  åˆå§‹åŒ–æ¨¡å‹")
    model = create_model(config, device, efficient=args.efficient_architecture, compression=args.model_compression)
    
    # æ¢å¤æ£€æŸ¥ç‚¹
    history = {"train_loss": [], "val_loss": [], "physics_loss": [], "lr": []}
    if args.resume:
        ckpt_path = args.resume if isinstance(args.resume, str) and os.path.isfile(args.resume) else os.path.join(dirs["checkpoints"], "latest.pth")
        if os.path.isfile(ckpt_path):
            ckpt = torch.load(ckpt_path, map_location=device)
            model.load_state_dict(ckpt["model_state_dict"])
            # æ¢å¤å†å²è®°å½•ï¼ˆå¦‚æœæœ‰ï¼‰
            if "history" in ckpt:
                history = ckpt["history"]
            logger.info(f"â™»ï¸  å·²ä»æ£€æŸ¥ç‚¹æ¢å¤: {ckpt_path}")

    # ä¼˜åŒ–å™¨ä¸è°ƒåº¦å™¨
    logger.info("âš™ï¸  é…ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨")
    optimizer = create_optimizer(model, config, args.lr)
    scheduler = create_lr_scheduler(optimizer, config, args.epochs, args.warmup_epochs, args.min_lr)

    # ä½¿ç”¨å¤šé˜¶æ®µè®­ç»ƒå™¨è¿›è¡Œè®­ç»ƒ
    logger.info("ğŸƒ å¼€å§‹è®­ç»ƒ")
    trainer = MultiStageTrainer(config, args, device, output_dir, dirs)
    performance_monitor = None
    try:
        performance_monitor = ModelPerformanceMonitor(device=str(device), save_dir=dirs['reports'])
    except Exception:
        performance_monitor = None
    model, history = trainer.run(
        model, optimizer, scheduler, train_loader, 
        X_val, y_val, X_test, y_test, physics_points, 
        normalizer, history, performance_monitor
    )
    
    # æœ€ç»ˆä¿å­˜
    final_model_path = os.path.join(output_dir, "final_model.pth")
    save_model(model, normalizer, final_model_path, config, {"epochs_trained": len(history["train_loss"]), "best_val_loss": min(history["val_loss"]) if history["val_loss"] else float("inf")}, export_onnx=args.export_onnx, onnx_path=os.path.join(output_dir, "final_model.onnx"))

    # è®­ç»ƒå†å² JSON
    history_path = os.path.join(dirs["reports"], "training_history.json")
    with open(history_path, "w", encoding="utf-8") as f:
        json.dump(history, f, indent=2, ensure_ascii=False)
    logger.info(f"ğŸ“ˆ è®­ç»ƒå†å²å·²ä¿å­˜: {history_path}")

    # ä¿å­˜æ•°æ®é›†ä¸º npz ä»¥ä¾›åç»­è¯Šæ–­
    try:
        dataset_path = os.path.join(output_dir, 'dataset.npz')
        np.savez_compressed(
            dataset_path,
            X_train=X_train.cpu().numpy(),
            y_train=y_train.cpu().numpy(),
            X_val=X_val.cpu().numpy(),
            y_val=y_val.cpu().numpy(),
            X_test=X_test.cpu().numpy(),
            y_test=y_test.cpu().numpy(),
            physics_points=physics_points.cpu().numpy() if physics_points is not None else None
        )
        logger.info(f"ğŸ—ƒï¸  æ•°æ®é›†å·²ä¿å­˜: {dataset_path}")
    except Exception as e:
        logger.warning(f"ä¿å­˜æ•°æ®é›†å¤±è´¥: {e}")

    # éªŒè¯ç»“æœ
    logger.info("ğŸ§ª å¼€å§‹æµ‹è¯•")
    final_val_loss, final_physics = validate_model(model, X_test, y_test, physics_points, config, device, args)
    val_results = {
        "test_loss": final_val_loss,
        "physics_loss": final_physics,
        "test_samples": len(X_test),
        "timestamp": datetime.datetime.now().isoformat(),
    }
    with open(os.path.join(dirs["reports"], "validation_results.json"), "w", encoding="utf-8") as f:
        json.dump(val_results, f, indent=2, ensure_ascii=False)
    logger.info(f"æµ‹è¯•ç»“æœ - loss={final_val_loss:.6f} | physics={final_physics:.6f}")

    # ç‰©ç†ä¸€è‡´æ€§éªŒè¯ï¼ˆå¢å¼ºåŠŸèƒ½ï¼‰
    if config.get('perform_physics_validation', False):
        logger.info("ğŸ” æ‰§è¡Œç‰©ç†ä¸€è‡´æ€§éªŒè¯")
        try:
            # ç”ŸæˆéªŒè¯æ•°æ®
            validation_points = generate_enhanced_consistency_data(config, device)
            # åˆ›å»ºçº¦æŸå±‚è¿›è¡ŒéªŒè¯
            constraint_layer = PINNConstraintLayer(model, device)
            consistency_residual = constraint_layer.compute_physics_loss(validation_points)
            logger.info(f"ç‰©ç†ä¸€è‡´æ€§éªŒè¯ - æ®‹å·®: {consistency_residual:.6f}")
            # ä¿å­˜éªŒè¯ç»“æœ
            with open(os.path.join(dirs["reports"], "physics_validation.json"), "w", encoding="utf-8") as f:
                json.dump({"residual": float(consistency_residual)}, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.warning(f"ç‰©ç†ä¸€è‡´æ€§éªŒè¯å¤±è´¥: {e}")

    # è®­ç»ƒæ›²çº¿å¯è§†åŒ–
    try:
        import matplotlib
        matplotlib.use("Agg")
        import matplotlib.pyplot as plt
        plt.figure()
        plt.plot(history["train_loss"], label="Train")
        plt.plot(history["val_loss"], label="Val")
        plt.plot(history["physics_loss"], label="Physics")
        plt.yscale("log")
        plt.legend()
        plt.title("Training Curves")
        plt.savefig(os.path.join(dirs["visualizations"], "training_curves_enhanced.png"))
        plt.close()
        logger.info("ğŸ“Š è®­ç»ƒæ›²çº¿å›¾å·²ä¿å­˜")
    except Exception as e:
        logger.warning(f"è®­ç»ƒæ›²çº¿å›¾å¤±è´¥: {e}")

    logger.info("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    return model, normalizer, history

# æµ‹è¯•/æ¨ç†
def test_model(model_path: str, config: dict, device: torch.device, output_dir: str):
    ckpt = torch.load(model_path, map_location=device)
    normalizer = DataNormalizer()
    normalizer.load_state_dict(ckpt.get("normalizer", {}))
    model = create_model(config, device, efficient=False)
    model.load_state_dict(ckpt["model_state_dict"])
    model.eval()
    logger.info("ğŸ§ª è¿›å…¥æµ‹è¯•æ¨¡å¼")
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    X_test = torch.randn(100, config.get("input_dim", 3), device=device)
    with torch.no_grad():
        preds = model(X_test).cpu().numpy()
    # ä¿å­˜é¢„æµ‹
    pred_path = os.path.join(output_dir, "test_predictions.npz")
    np.savez_compressed(pred_path, X_test=X_test.cpu().numpy(), predictions=preds)
    logger.info(f"ğŸ“¤ æµ‹è¯•é¢„æµ‹å·²ä¿å­˜: {pred_path}")

# CLI å‚æ•°
def parse_arguments():
    p = argparse.ArgumentParser(description="EFD-PINNs ç»Ÿä¸€è®­ç»ƒè„šæœ¬")
    p.add_argument("--mode", choices=["train", "test", "infer"], default="train", help="è¿è¡Œæ¨¡å¼")
    p.add_argument("--config", default=DEFAULT_CONFIG, help="é…ç½®æ–‡ä»¶è·¯å¾„")
    p.add_argument("--output-dir", default=DEFAULT_OUTPUT_DIR, help="è¾“å‡ºæ ¹ç›®å½•ï¼ˆè‡ªåŠ¨è¿½åŠ æ—¶é—´æˆ³ï¼‰")
    p.add_argument("--resume", nargs="?", const=True, default=False, help="æ¢å¤è®­ç»ƒï¼šå¸ƒå°”æˆ–æ£€æŸ¥ç‚¹è·¯å¾„")
    p.add_argument("--checkpoint", help="(å…¼å®¹æ—§å‚æ•°) åŒ --resume")
    p.add_argument("--device", help="cuda/cpu/auto")
    p.add_argument("--mixed-precision", action="store_true", help="å¯ç”¨ AMPï¼ˆé»˜è®¤è‡ªåŠ¨ï¼‰")
    p.add_argument("--efficient-architecture", action="store_true", help="ä½¿ç”¨é«˜æ•ˆæ¶æ„")
    p.add_argument("--model-compression", type=float, default=1.0, help="æ¨¡å‹å‹ç¼©å› å­")
    p.add_argument("--export-onnx", action="store_true", help="å¯¼å‡º ONNX")
    p.add_argument("--num-samples", type=int, default=DEFAULT_NUM_SAMPLES, help="æ ·æœ¬æ•°")
    p.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    p.add_argument("--deterministic", action="store_true", help="ç¡®å®šæ€§è®­ç»ƒ")
    # å¢å¼º/å¿«é€Ÿ
    p.add_argument("--quick_run", action="store_true", help="å¿«é€Ÿè¿è¡Œï¼ˆé™æ ·æœ¬ï¼‰")
    p.add_argument("--generate_data_only", action="store_true", help="ä»…ç”Ÿæˆæ•°æ®")
    p.add_argument("--validate_only", action="store_true", help="ä»…éªŒè¯")
    p.add_argument("--model_path", help="æ¨ç†/æµ‹è¯•æ—¶æ¨¡å‹è·¯å¾„")
    # é•¿æœŸ
    p.add_argument("--epochs", type=int, default=DEFAULT_EPOCHS, help="æ€»è½®æ¬¡")
    p.add_argument("--lr", type=float, default=DEFAULT_LR, help="åˆå§‹å­¦ä¹ ç‡")
    p.add_argument("--warmup_epochs", type=int, default=DEFAULT_WARMUP_EPOCHS, help="Warmup è½®æ¬¡")
    p.add_argument("--min_lr", type=float, default=DEFAULT_MIN_LR, help="æœ€å°å­¦ä¹ ç‡")
    p.add_argument("--batch_size", type=int, default=DEFAULT_BATCH_SIZE, help="æ‰¹æ¬¡å¤§å°")
    p.add_argument("--physics_weight", type=float, default=DEFAULT_PHYSICS_WEIGHT, help="ç‰©ç†çº¦æŸæƒé‡")
    p.add_argument("--dynamic_weight", action="store_true", help="å¯ç”¨åŠ¨æ€æƒé‡")
    p.add_argument("--weight_strategy", choices=["adaptive", "stage_based", "loss_ratio", "combined"], default=DEFAULT_WEIGHT_STRATEGY, help="åŠ¨æ€æƒé‡ç­–ç•¥")
    p.add_argument("--checkpoint_interval", type=int, default=DEFAULT_CHECKPOINT_INTERVAL, help="æ£€æŸ¥ç‚¹é—´éš”")
    p.add_argument("--validation_interval", type=int, default=DEFAULT_VALIDATION_INTERVAL, help="éªŒè¯é—´éš”")
    # 3D æ˜ å°„
    p.add_argument("--use_3d_mapping", action="store_true", help="å¯ç”¨ 3D æ˜ å°„")
    p.add_argument("--gpu_safe", action="store_true", help="GPU å®‰å…¨åˆ†æ‰¹ç”Ÿæˆæ•°æ®")
    # è®­ç»ƒç»†èŠ‚
    p.add_argument("--clip_grad", type=float, help="æ¢¯åº¦è£å‰ªèŒƒæ•°")
    p.add_argument("--override_lr", type=float, help="å¼ºåˆ¶è¦†ç›–å­¦ä¹ ç‡")
    p.add_argument("--gradient_accumulation_steps", type=int, default=1, help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°")
    return p.parse_args()

# ä¸»å…¥å£
def main():
    args = parse_arguments()
    if args.checkpoint and not args.resume:
        args.resume = args.checkpoint  # å…¼å®¹æ—§å‚æ•°

    # è®¾å¤‡ä¸ç§å­
    device = get_device(args.device)
    set_global_seed(args.seed, args.deterministic)
    logger.info(f"ğŸ”§ è®¾å¤‡: {device}")

    # è¾“å‡ºç›®å½•
    output_dir = make_timestamp_dir(args.output_dir)
    dirs = setup_output_dirs(output_dir)
    logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")

    # é…ç½®
    if not os.path.isfile(args.config):
        logger.error(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
        sys.exit(1)
    with open(args.config, "r", encoding="utf-8") as f:
        config = json.load(f)
    print(f"[DEBUG] åŠ è½½çš„é…ç½®:\n{json.dumps(config, indent=2, ensure_ascii=False)}")  # ä¸´æ—¶

    # æ¨¡å¼åˆ†æ”¯
    if args.mode == "train":
        progressive_training(config, args, device, output_dir, dirs)
    elif args.mode == "test":
        if not args.model_path:
            logger.error("âŒ æµ‹è¯•æ¨¡å¼éœ€æŒ‡å®š --model_path")
            sys.exit(1)
        test_model(args.model_path, config, device, output_dir)
    elif args.mode == "infer":
        logger.info("ğŸ§  æ¨ç†æ¨¡å¼ï¼ˆå ä½ï¼‰")
    else:
        logger.error(f"âŒ æœªçŸ¥æ¨¡å¼: {args.mode}")
        sys.exit(1)

    logger.info("âœ¨ å…¨éƒ¨å®Œæˆï¼")

if __name__ == "__main__":
    main()
