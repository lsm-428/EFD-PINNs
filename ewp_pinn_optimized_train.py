#!/usr/bin/env python3
"""
EWPINNä¼˜åŒ–è®­ç»ƒè„šæœ¬ - è§£å†³æŸå¤±è¿‡é«˜é—®é¢˜
åŒ…å«æ•°æ®æ ‡å‡†åŒ–ã€æŸå¤±ç¨³å®šåŒ–ã€æ¸è¿›å¼è®­ç»ƒ
"""
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
import time
import os
import json
import copy
from datetime import datetime
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import sklearn.exceptions
sklearn.exceptions.EfficiencyWarning = FutureWarning  # å¿½ç•¥sklearnè­¦å‘Š
from ewp_pinn_input_layer import EWPINNInputLayer
from ewp_pinn_output_layer import EWPINNOutputLayer
from ewp_data_interface import create_dataset, create_dataloader
from ewp_data_interface import validate_units
from ewp_pinn_physics import PINNConstraintLayer, PhysicsEnhancedLoss
from ewp_pinn_adaptive_hyperoptimizer import AdaptiveHyperparameterOptimizer
from ewp_pinn_performance_monitor import ModelPerformanceMonitor
from ewp_pinn_regularization import AdvancedRegularizer, GradientNoiseRegularizer, apply_regularization_to_model
from ewp_pinn_optimized_architecture import EfficientEWPINN, create_optimized_model, get_model_optimization_suggestions

class OptimizedEWPINN(nn.Module):
    """
    ä¼˜åŒ–ç‰ˆEWPINNæ¨¡å‹ - å¢å¼ºå‹ç¥ç»ç½‘ç»œæ¶æ„ï¼Œæ”¯æŒé…ç½®æ–‡ä»¶åŠ è½½
    ç‰¹æ€§ï¼šæ‰¹é‡æ ‡å‡†åŒ–ã€æ”¹è¿›çš„åˆå§‹åŒ–ã€çµæ´»çš„æ¶æ„é…ç½®
    """
    def __init__(self, input_dim=62, output_dim=24, hidden_layers=None, dropout_rate=0.1,
                 activation='ReLU', batch_norm=True, config_path=None, device='cpu'):
        super(OptimizedEWPINN, self).__init__()
        
        self.device = device
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.dropout_rate = dropout_rate
        self.batch_norm = batch_norm
        
        # æ¨¡å‹é…ç½®ä¿¡æ¯ï¼Œç”¨äºç‰ˆæœ¬æ§åˆ¶å’Œå…¼å®¹æ€§æ£€æŸ¥
        self.model_info = {
            'version': '1.0.0',
            'input_dim': input_dim,
            'output_dim': output_dim,
            'hidden_layers': hidden_layers if hidden_layers else [128, 64, 32],
            'dropout_rate': dropout_rate,
            'activation': activation,
            'batch_norm': batch_norm,
            'architecture': 'EWPINN',
            'created_at': datetime.now().isoformat()
        }
        
        # ä»é…ç½®æ–‡ä»¶åŠ è½½å‚æ•°ï¼ˆå¦‚æœæä¾›ï¼‰
        if config_path and os.path.exists(config_path):
            self._load_config(config_path)
        
        # é»˜è®¤éšè—å±‚é…ç½®
        if hidden_layers is None:
            hidden_layers = [128, 64, 32]
        self.hidden_layers = hidden_layers
        
        # æ›´æ–°model_infoä¸­çš„hidden_layers
        self.model_info['hidden_layers'] = hidden_layers
        self.model_info['activation'] = activation
        
        # é€‰æ‹©æ¿€æ´»å‡½æ•°
        activation_map = {
            'ReLU': nn.ReLU,
            'LeakyReLU': nn.LeakyReLU,
            'GELU': nn.GELU,
            'SiLU': nn.SiLU
        }
        activation_fn = activation_map.get(activation, nn.ReLU)
        
        # æ„å»ºç½‘ç»œ
        layers = []
        prev_dim = input_dim
        
        # æ„å»ºéšè—å±‚
        for i, hidden_dim in enumerate(hidden_layers):
            # çº¿æ€§å±‚
            layers.append(nn.Linear(prev_dim, hidden_dim))
            
            # æ‰¹é‡æ ‡å‡†åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if batch_norm:
                layers.append(nn.BatchNorm1d(hidden_dim))
            
            # æ¿€æ´»å‡½æ•°
            layers.append(activation_fn())
            
            # Dropoutï¼ˆé™¤äº†æœ€åä¸€å±‚ï¼‰
            if i < len(hidden_layers) - 1 and dropout_rate > 0:
                layers.append(nn.Dropout(dropout_rate))
            
            prev_dim = hidden_dim
        
        # è¾“å‡ºå±‚
        layers.append(nn.Linear(prev_dim, output_dim))
        
        self.model = nn.Sequential(*layers).to(device)
        
        # åˆå§‹åŒ–æƒé‡
        self._initialize_weights()
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        self._print_model_info(activation)
    
    def _load_config(self, config_path):
        """ä»JSONé…ç½®æ–‡ä»¶åŠ è½½æ¨¡å‹å‚æ•°"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
                
            if 'æ¨¡å‹é…ç½®' in config:
                model_config = config['æ¨¡å‹é…ç½®']
                if 'è¾“å…¥ç»´åº¦' in model_config:
                    self.input_dim = model_config['è¾“å…¥ç»´åº¦']
                if 'è¾“å‡ºç»´åº¦' in model_config:
                    self.output_dim = model_config['è¾“å‡ºç»´åº¦']
                if 'ç½‘ç»œæ¶æ„' in model_config:
                    net_config = model_config['ç½‘ç»œæ¶æ„']
                    if 'éšè—å±‚' in net_config:
                        self.hidden_layers = net_config['éšè—å±‚']
                    if 'Dropout' in net_config:
                        self.dropout_rate = net_config['Dropout']
                print(f"âœ… æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
        except Exception as e:
            print(f"âš ï¸  åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {str(e)}")
            print("   å°†ä½¿ç”¨é»˜è®¤é…ç½®")
    
    def _initialize_weights(self):
        """ä½¿ç”¨Heåˆå§‹åŒ–æ–¹æ³•æ”¹è¿›æƒé‡åˆå§‹åŒ–"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                # ä½¿ç”¨Heåˆå§‹åŒ–
                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0.01)
    
    def _print_model_info(self, activation):
        """æ‰“å°æ¨¡å‹æ¶æ„ä¿¡æ¯"""
        print(f"ğŸš€ ä¼˜åŒ–EWPINNæ¨¡å‹å·²åˆå§‹åŒ– - è®¾å¤‡: {self.device}")
        print(f"   è¾“å…¥ç»´åº¦: {self.input_dim}, è¾“å‡ºç»´åº¦: {self.output_dim}")
        print(f"   æ¿€æ´»å‡½æ•°: {activation}")
        print(f"   æ‰¹é‡æ ‡å‡†åŒ–: {'å¯ç”¨' if self.batch_norm else 'ç¦ç”¨'}")
        print(f"   Dropoutç‡: {self.dropout_rate}")
        
        # æ‰“å°ç½‘ç»œç»“æ„
        structure_str = f"{self.input_dim}"
        for dim in self.hidden_layers:
            structure_str += f" -> {dim}"
        structure_str += f" -> {self.output_dim}"
        print(f"   ç½‘ç»œç»“æ„: {structure_str}")
        
        # è®¡ç®—å‚æ•°æ•°é‡
        param_count = sum(p.numel() for p in self.parameters())
        print(f"   å‚æ•°æ•°é‡: {param_count:,}")
    
    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        return self.model(x)
    
    def get_model_summary(self):
        """è¿”å›æ¨¡å‹æ‘˜è¦ä¿¡æ¯"""
        summary = {
            'input_dim': self.input_dim,
            'output_dim': self.output_dim,
            'hidden_layers': self.hidden_layers,
            'dropout_rate': self.dropout_rate,
            'batch_norm': self.batch_norm,
            'parameter_count': sum(p.numel() for p in self.parameters()),
            'device': str(self.device)
        }
        return summary

class LossStabilizer:
    """
    é«˜çº§æŸå¤±ç¨³å®šå™¨ - å¢å¼ºæ•°å€¼ç¨³å®šæ€§å’Œçµæ´»æ€§
    ç‰¹æ€§ï¼šå¤šç§æŸå¤±å‡½æ•°ã€è‡ªé€‚åº”ç¨³å®šåŒ–ã€é…ç½®æ–‡ä»¶æ”¯æŒ
    """
    def __init__(self, epsilon=1e-10, loss_type='mse', safe_clamp=True,
                 config_path=None, patience=5, reduction_factor=0.5):
        self.epsilon = epsilon
        self.loss_type = loss_type
        self.safe_clamp = safe_clamp
        self.patience = patience
        self.reduction_factor = reduction_factor
        self.current_loss = float('inf')
        self.loss_history = []
        self.best_loss = float('inf')
        self.patience_counter = 0
        
        # ä»é…ç½®æ–‡ä»¶åŠ è½½å‚æ•°ï¼ˆå¦‚æœæä¾›ï¼‰
        if config_path and os.path.exists(config_path):
            self._load_config(config_path)
        
        print(f"ğŸ“Š é«˜çº§æŸå¤±ç¨³å®šå™¨å·²åˆå§‹åŒ–")
        print(f"   æŸå¤±ç±»å‹: {loss_type}")
        print(f"   æ•°å€¼å®‰å…¨å‚æ•°: {epsilon}")
        print(f"   å®‰å…¨è£å‰ª: {'å¯ç”¨' if safe_clamp else 'ç¦ç”¨'}")
        print(f"   è‡ªé€‚åº”ç¨³å®šåŒ–: {'å¯ç”¨' if patience > 0 else 'ç¦ç”¨'} (è€å¿ƒ: {patience})")
    
    def _load_config(self, config_path):
        """ä»JSONé…ç½®æ–‡ä»¶åŠ è½½æŸå¤±ç¨³å®šå™¨å‚æ•°"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
                
            if 'ç‰©ç†çº¦æŸ' in config:
                constraint_config = config['ç‰©ç†çº¦æŸ']
                if 'æ•°å€¼ç¨³å®šæ€§å‚æ•°' in constraint_config:
                    self.epsilon = constraint_config['æ•°å€¼ç¨³å®šæ€§å‚æ•°']
                if 'æŸå¤±ç±»å‹' in constraint_config:
                    self.loss_type = constraint_config['æŸå¤±ç±»å‹']
                if 'å®‰å…¨è£å‰ª' in constraint_config:
                    self.safe_clamp = constraint_config['å®‰å…¨è£å‰ª']
                if 'è‡ªé€‚åº”ç¨³å®šåŒ–' in constraint_config:
                    adapt_config = constraint_config['è‡ªé€‚åº”ç¨³å®šåŒ–']
                    if 'è€å¿ƒ' in adapt_config:
                        self.patience = adapt_config['è€å¿ƒ']
                    if 'å‡å°‘å› å­' in adapt_config:
                        self.reduction_factor = adapt_config['å‡å°‘å› å­']
            print(f"âœ… æˆåŠŸåŠ è½½æŸå¤±ç¨³å®šå™¨é…ç½®")
        except Exception as e:
            print(f"âš ï¸  åŠ è½½æŸå¤±ç¨³å®šå™¨é…ç½®å¤±è´¥: {str(e)}")
    
    def safe_mse_loss(self, pred, target, max_loss_value=1e6):
        """å®‰å…¨çš„MSEæŸå¤±è®¡ç®—"""
        # 1. æ•°æ®é¢„è£å‰ª - é¿å…æç«¯å€¼
        if self.safe_clamp:
            pred_clipped = torch.clamp(pred, -1000, 1000)
            target_clipped = torch.clamp(target, -1000, 1000)
        else:
            pred_clipped, target_clipped = pred, target
        
        # 2. è®¡ç®—MSE
        mse = nn.functional.mse_loss(pred_clipped, target_clipped)
        
        # 3. ç«‹å³è£å‰ªæŸå¤±å€¼
        safe_mse = torch.clamp(mse, 0, max_loss_value)
        
        # 4. å¯¹æ•°å˜æ¢ç¨³å®šï¼ˆå¯é€‰ï¼‰
        if safe_mse > 1.0:
            stable_loss = torch.log(1 + safe_mse)
        else:
            stable_loss = safe_mse
            
        return stable_loss
    
    def relative_loss(self, pred, target, epsilon=None):
        """ç›¸å¯¹æŸå¤± - å¯¹æ•°æ®é‡çº§ä¸æ•æ„Ÿ"""
        # ä½¿ç”¨å®ä¾‹æˆ–å‚æ•°epsilon
        eps = epsilon if epsilon is not None else self.epsilon
        
        # è®¡ç®—ç›¸å¯¹è¯¯å·®
        denominator = torch.abs(target) + eps
        relative_error = torch.abs(pred - target) / denominator
        
        # è¿”å›å¹³å‡ç›¸å¯¹è¯¯å·®
        return torch.mean(relative_error)
    
    def huber_loss(self, pred, target, delta=1.0):
        """HuberæŸå¤±ï¼Œç»“åˆMSEå’ŒMAEçš„ä¼˜ç‚¹"""
        if self.safe_clamp:
            pred = torch.clamp(pred, -1000, 1000)
            target = torch.clamp(target, -1000, 1000)
        
        # è®¡ç®—ç»å¯¹è¯¯å·®
        abs_error = torch.abs(pred - target)
        
        # å¯¹å°è¯¯å·®ä½¿ç”¨å¹³æ–¹ï¼Œå¤§è¯¯å·®ä½¿ç”¨çº¿æ€§
        quadratic = torch.clamp(abs_error, max=delta)
        linear = abs_error - quadratic
        
        loss = 0.5 * quadratic.pow(2) + delta * linear
        
        return torch.mean(loss)
    
    def combined_loss(self, pred, target, mse_weight=0.5, relative_weight=0.5):
        """ç»„åˆæŸå¤±å‡½æ•°ï¼Œå¹³è¡¡MSEå’Œç›¸å¯¹è¯¯å·®"""
        mse = self.safe_mse_loss(pred, target)
        relative = self.relative_loss(pred, target)
        
        # å½’ä¸€åŒ–æŸå¤±å€¼
        total_weight = mse_weight + relative_weight
        if total_weight > 0:
            mse_weight /= total_weight
            relative_weight /= total_weight
        
        return mse_weight * mse + relative_weight * relative
    
    def compute_loss(self, pred, target):
        """æ ¹æ®é…ç½®è®¡ç®—æŸå¤±"""
        loss_mapping = {
            'mse': self.safe_mse_loss,
            'relative': self.relative_loss,
            'huber': self.huber_loss,
            'combined': self.combined_loss
        }
        
        loss_fn = loss_mapping.get(self.loss_type, self.safe_mse_loss)
        loss = loss_fn(pred, target)
        
        # è®°å½•æŸå¤±å†å²
        self.loss_history.append(loss.item())
        self.current_loss = loss.item()
        
        return loss
    
    def adaptive_stabilization(self, current_loss):
        """è‡ªé€‚åº”ç¨³å®šåŒ–æœºåˆ¶ï¼Œæ£€æµ‹å¹¶å¤„ç†è®­ç»ƒä¸ç¨³å®šæƒ…å†µ"""
        if self.patience <= 0:
            return False, 1.0  # ä¸å¯ç”¨è‡ªé€‚åº”ç¨³å®šåŒ–
        
        # æ›´æ–°æœ€ä½³æŸå¤±
        if current_loss < self.best_loss:
            self.best_loss = current_loss
            self.patience_counter = 0
            return False, 1.0
        
        # å¢åŠ è€å¿ƒè®¡æ•°å™¨
        self.patience_counter += 1
        
        # å¦‚æœè¶…è¿‡è€å¿ƒé˜ˆå€¼ï¼Œè§¦å‘ç¨³å®šåŒ–
        if self.patience_counter >= self.patience:
            self.patience_counter = 0
            scale_factor = self.reduction_factor
            print(f"âš ï¸  æ£€æµ‹åˆ°è®­ç»ƒä¸ç¨³å®šï¼Œåº”ç”¨ç¨³å®šåŒ–å› å­: {scale_factor}")
            return True, scale_factor
        
        return False, 1.0
    
    def should_stop_early(self, threshold=1e-6):
        """æ—©åœæœºåˆ¶ï¼Œæ£€æµ‹è®­ç»ƒæ˜¯å¦æ”¶æ•›"""
        if len(self.loss_history) < 10:
            return False
        
        # æ£€æŸ¥æœ€è¿‘10æ¬¡æŸå¤±çš„å˜åŒ–
        recent_losses = self.loss_history[-10:]
        loss_std = np.std(recent_losses)
        
        return loss_std < threshold

class DataNormalizer:
    """
    é«˜çº§æ•°æ®æ ‡å‡†åŒ–å™¨ - å¢å¼ºçš„æ•°æ®é¢„å¤„ç†å’Œæ ‡å‡†åŒ–åŠŸèƒ½
    ç‰¹æ€§ï¼šå¤šç§æ ‡å‡†åŒ–æ–¹æ³•ã€å¼‚å¸¸å€¼å¤„ç†ã€é…ç½®æ–‡ä»¶æ”¯æŒ
    """
    def __init__(self, feature_method='standard', label_method='minmax',
                 handle_outliers=True, outlier_threshold=3.0, config_path=None):
        self.feature_method = feature_method
        self.label_method = label_method
        self.handle_outliers = handle_outliers
        self.outlier_threshold = outlier_threshold
        self.is_fitted = False
        
        # åˆå§‹åŒ–æ ‡å‡†åŒ–å™¨
        self.input_scaler = self._create_scaler(feature_method)
        self.output_scaler = self._create_scaler(label_method)
        
        # ä»é…ç½®æ–‡ä»¶åŠ è½½å‚æ•°ï¼ˆå¦‚æœæä¾›ï¼‰
        if config_path and os.path.exists(config_path):
            self._load_config(config_path)
        
        print(f"ğŸ”„ é«˜çº§æ•°æ®æ ‡å‡†åŒ–å™¨å·²åˆå§‹åŒ–")
        print(f"   ç‰¹å¾æ ‡å‡†åŒ–æ–¹æ³•: {feature_method}")
        print(f"   æ ‡ç­¾æ ‡å‡†åŒ–æ–¹æ³•: {label_method}")
        print(f"   å¼‚å¸¸å€¼å¤„ç†: {'å¯ç”¨' if handle_outliers else 'ç¦ç”¨'} (é˜ˆå€¼: {outlier_threshold})")
    
    def _create_scaler(self, method):
        """æ ¹æ®æ–¹æ³•åˆ›å»ºæ ‡å‡†åŒ–å™¨"""
        if method == 'minmax':
            return MinMaxScaler(feature_range=(0, 1))
        elif method == 'robust':
            from sklearn.preprocessing import RobustScaler
            return RobustScaler()
        elif method == 'power':
            from sklearn.preprocessing import PowerTransformer
            return PowerTransformer(method='yeo-johnson')
        else:  # standard
            return StandardScaler()
    
    def _load_config(self, config_path):
        """ä»JSONé…ç½®æ–‡ä»¶åŠ è½½æ•°æ®æ ‡å‡†åŒ–å™¨å‚æ•°"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
                
            if 'æ•°æ®å¤„ç†' in config:
                data_config = config['æ•°æ®å¤„ç†']
                if 'ç‰¹å¾æ ‡å‡†åŒ–æ–¹æ³•' in data_config:
                    self.feature_method = data_config['ç‰¹å¾æ ‡å‡†åŒ–æ–¹æ³•']
                if 'æ ‡ç­¾æ ‡å‡†åŒ–æ–¹æ³•' in data_config:
                    self.label_method = data_config['æ ‡ç­¾æ ‡å‡†åŒ–æ–¹æ³•']
                if 'å¼‚å¸¸å€¼å¤„ç†' in data_config:
                    outlier_config = data_config['å¼‚å¸¸å€¼å¤„ç†']
                    if 'å¯ç”¨' in outlier_config:
                        self.handle_outliers = outlier_config['å¯ç”¨']
                    if 'é˜ˆå€¼' in outlier_config:
                        self.outlier_threshold = outlier_config['é˜ˆå€¼']
                
                # é‡æ–°åˆ›å»ºæ ‡å‡†åŒ–å™¨ä»¥åº”ç”¨æ–°æ–¹æ³•
                self.input_scaler = self._create_scaler(self.feature_method)
                self.output_scaler = self._create_scaler(self.label_method)
                
            print(f"âœ… æˆåŠŸåŠ è½½æ•°æ®æ ‡å‡†åŒ–å™¨é…ç½®")
        except Exception as e:
            print(f"âš ï¸  åŠ è½½æ•°æ®æ ‡å‡†åŒ–å™¨é…ç½®å¤±è´¥: {str(e)}")
    
    def _handle_outliers(self, data, threshold=None):
        """å¤„ç†å¼‚å¸¸å€¼"""
        if not self.handle_outliers:
            return data
        
        threshold = threshold if threshold is not None else self.outlier_threshold
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        data_np = data.cpu().numpy() if torch.is_tensor(data) else data
        
        # ä½¿ç”¨IQRæ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼
        q1 = np.percentile(data_np, 25, axis=0)
        q3 = np.percentile(data_np, 75, axis=0)
        iqr = q3 - q1
        
        lower_bound = q1 - threshold * iqr
        upper_bound = q3 + threshold * iqr
        
        # è£å‰ªå¼‚å¸¸å€¼
        data_clipped = np.clip(data_np, lower_bound, upper_bound)
        
        # è®¡ç®—å¼‚å¸¸å€¼æ•°é‡
        outliers = np.sum((data_np < lower_bound) | (data_np > upper_bound))
        total_values = data_np.size
        
        if outliers > 0:
            outlier_percent = (outliers / total_values) * 100
            print(f"âš ï¸  æ£€æµ‹å¹¶å¤„ç†äº† {outliers} ä¸ªå¼‚å¸¸å€¼ ({outlier_percent:.2f}%)")
        
        return data_clipped
    
    def fit(self, features, labels):
        """æ‹Ÿåˆä¸€åŒ–å™¨"""
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        features_np = features.cpu().numpy() if torch.is_tensor(features) else features
        labels_np = labels.cpu().numpy() if torch.is_tensor(labels) else labels
        
        # å¤„ç†å¼‚å¸¸å€¼
        if self.handle_outliers:
            features_np = self._handle_outliers(features_np)
            labels_np = self._handle_outliers(labels_np)
        
        # æ ‡å‡†åŒ–è¾“å…¥ç‰¹å¾
        self.input_scaler.fit(features_np)
        
        # å½’ä¸€åŒ–è¾“å‡ºæ ‡ç­¾
        self.output_scaler.fit(labels_np)
        
        self.is_fitted = True
        print(f"âœ… æ•°æ®æ ‡å‡†åŒ–å™¨æ‹Ÿåˆå®Œæˆ")
        
        # æ‰“å°ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯
        if hasattr(self.input_scaler, 'mean_'):
            print(f"   ç‰¹å¾å‡å€¼èŒƒå›´: {self.input_scaler.mean_.min():.4f} ~ {self.input_scaler.mean_.max():.4f}")
        
        # æ‰“å°æ ‡ç­¾ç»Ÿè®¡ä¿¡æ¯
        if hasattr(self.output_scaler, 'mean_'):
            print(f"   æ ‡ç­¾å‡å€¼èŒƒå›´: {self.output_scaler.mean_.min():.4f} ~ {self.output_scaler.mean_.max():.4f}")
    
    def transform_features(self, features):
        """æ ‡å‡†åŒ–ç‰¹å¾"""
        if not self.is_fitted:
            raise ValueError("æ ‡å‡†åŒ–å™¨æœªæ‹Ÿåˆ")
        
        is_tensor = torch.is_tensor(features)
        if is_tensor:
            device = features.device
            features_np = features.cpu().numpy()
            if self.handle_outliers:
                features_np = self._handle_outliers(features_np)
            features_normalized = self.input_scaler.transform(features_np)
            return torch.tensor(features_normalized, dtype=torch.float32, device=device)
        else:
            data_np = features
            if self.handle_outliers:
                data_np = self._handle_outliers(data_np)
            features_normalized = self.input_scaler.transform(data_np)
            return features_normalized
    
    def transform_labels(self, labels):
        """æ ‡å‡†åŒ–æ ‡ç­¾"""
        if not self.is_fitted:
            raise ValueError("æ ‡å‡†åŒ–å™¨æœªæ‹Ÿåˆ")
        
        is_tensor = torch.is_tensor(labels)
        if is_tensor:
            device = labels.device
            labels_np = labels.cpu().numpy()
            if self.handle_outliers:
                labels_np = self._handle_outliers(labels_np)
            labels_normalized = self.output_scaler.transform(labels_np)
            labels_tensor = torch.tensor(labels_normalized, dtype=torch.float32, device=device)
            labels_tensor = torch.clamp(labels_tensor, 0.0, 1.0)
            return labels_tensor
        else:
            data_np = labels
            if self.handle_outliers:
                data_np = self._handle_outliers(data_np)
            labels_normalized = self.output_scaler.transform(data_np)
            import numpy as np
            labels_normalized = np.clip(labels_normalized, 0.0, 1.0)
            return labels_normalized
    
    def inverse_transform_labels(self, labels_normalized):
        """åå½’ä¸€åŒ–æ ‡ç­¾ - å¢å¼ºç‰ˆæœ¬ï¼Œç¡®ä¿æ•°å€¼ç¨³å®šæ€§"""
        if not self.is_fitted:
            raise ValueError("æ ‡å‡†åŒ–å™¨æœªæ‹Ÿåˆ")
        
        # å¤„ç†PyTorchå¼ é‡
        if torch.is_tensor(labels_normalized):
            device = labels_normalized.device
            labels_np = labels_normalized.cpu().numpy()
        else:
            labels_np = labels_normalized.copy()  # åˆ›å»ºå‰¯æœ¬ä»¥é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
        
        # å…³é”®æ”¹è¿›ï¼šåœ¨é€†æ ‡å‡†åŒ–å‰å°†æ•°æ®è£å‰ªåˆ°åˆç†èŒƒå›´
        # ç‰¹åˆ«æ˜¯å¯¹äºminmaxæ ‡å‡†åŒ–ï¼Œç¡®ä¿æ•°æ®åœ¨[0,1]èŒƒå›´å†…
        if isinstance(self.output_scaler, MinMaxScaler):
            labels_np = np.clip(labels_np, 0.0, 1.0)
        else:
            # å¯¹äºå…¶ä»–æ ‡å‡†åŒ–æ–¹æ³•ï¼Œä½¿ç”¨æ›´å®½æ¾çš„è£å‰ªèŒƒå›´
            # æ‰¾å‡ºæ•°æ®çš„å››åˆ†ä½æ•°ä»¥ç¡®å®šåˆç†èŒƒå›´
            q1 = np.percentile(labels_np, 25, axis=0)
            q3 = np.percentile(labels_np, 75, axis=0)
            iqr = q3 - q1
            # ä½¿ç”¨æ›´å®½æ¾çš„èŒƒå›´ï¼Œé¿å…è¿‡åº¦è£å‰ª
            lower_bound = q1 - 5 * iqr
            upper_bound = q3 + 5 * iqr
            # å¤„ç†å¯èƒ½çš„é›¶IQRæƒ…å†µ
            if np.any(iqr == 0):
                # ä½¿ç”¨æ ‡å‡†å·®æ¥ç¡®å®šèŒƒå›´
                std = np.std(labels_np, axis=0)
                mean = np.mean(labels_np, axis=0)
                # æ›´æ–°é›¶IQRç»´åº¦çš„è¾¹ç•Œ
                for i in range(len(iqr)):
                    if iqr[i] == 0:
                        lower_bound[i] = mean[i] - 5 * std[i] if std[i] > 0 else mean[i] - 1.0
                        upper_bound[i] = mean[i] + 5 * std[i] if std[i] > 0 else mean[i] + 1.0
            
            # è£å‰ªåˆ°è®¡ç®—çš„èŒƒå›´å†…
            labels_np = np.clip(labels_np, lower_bound, upper_bound)
        
        # æ·»åŠ æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        # æ£€æŸ¥å¹¶æ›¿æ¢NaNå’Œæ— ç©·å¤§å€¼
        labels_np = np.nan_to_num(labels_np)
        
        # è¿›è¡Œé€†æ ‡å‡†åŒ–
        try:
            labels_original = self.output_scaler.inverse_transform(labels_np)
            
            # å†æ¬¡æ£€æŸ¥é€†æ ‡å‡†åŒ–åçš„æ•°å€¼ç¨³å®šæ€§
            labels_original = np.nan_to_num(labels_original)
            
            # è¿›ä¸€æ­¥é˜²æ­¢æç«¯å€¼
            # è®¡ç®—é€†æ ‡å‡†åŒ–åæ•°æ®çš„åˆç†èŒƒå›´
            if labels_original.size > 0:  # ç¡®ä¿æ•°ç»„ä¸ä¸ºç©º
                # ä½¿ç”¨ç¨³å¥çš„ç»Ÿè®¡é‡ç¡®å®šèŒƒå›´
                q1_inv = np.percentile(labels_original, 25, axis=0)
                q3_inv = np.percentile(labels_original, 75, axis=0)
                iqr_inv = q3_inv - q1_inv
                
                # ä½¿ç”¨æ›´ä¸¥æ ¼çš„èŒƒå›´æ¥é˜²æ­¢å¼‚å¸¸å¤§çš„å€¼
                lower_bound_inv = q1_inv - 3 * iqr_inv
                upper_bound_inv = q3_inv + 3 * iqr_inv
                
                # å¤„ç†é›¶IQRæƒ…å†µ
                if np.any(iqr_inv == 0):
                    std_inv = np.std(labels_original, axis=0)
                    mean_inv = np.mean(labels_original, axis=0)
                    for i in range(len(iqr_inv)):
                        if iqr_inv[i] == 0:
                            lower_bound_inv[i] = mean_inv[i] - 3 * std_inv[i] if std_inv[i] > 0 else mean_inv[i] - 10.0
                            upper_bound_inv[i] = mean_inv[i] + 3 * std_inv[i] if std_inv[i] > 0 else mean_inv[i] + 10.0
                
                # æœ€ç»ˆè£å‰ªï¼Œç¡®ä¿å€¼ä¸ä¼šè¿‡å¤§
                labels_original = np.clip(labels_original, lower_bound_inv, upper_bound_inv)
            
            # å¦‚æœè¾“å…¥æ˜¯å¼ é‡ï¼Œè½¬æ¢å›å¼ é‡
            if torch.is_tensor(labels_normalized):
                return torch.tensor(labels_original, dtype=torch.float32, device=device)
            else:
                return labels_original
        except Exception as e:
            print(f"âš ï¸  é€†æ ‡å‡†åŒ–è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
            # å‡ºé”™æ—¶è¿”å›ä¸€ä¸ªåˆç†çš„é»˜è®¤å€¼ï¼ˆåŸå§‹èŒƒå›´çš„ä¸­ä½æ•°é™„è¿‘ï¼‰
            if hasattr(self.output_scaler, 'data_min_'):
                default_value = (self.output_scaler.data_min_ + self.output_scaler.data_max_) / 2
                return np.full_like(labels_np, default_value) if isinstance(labels_normalized, np.ndarray) else \
                       torch.full_like(labels_normalized, default_value, device=device)
            else:
                # è¿”å›é›¶çŸ©é˜µä½œä¸ºåå¤‡
                return np.zeros_like(labels_np) if isinstance(labels_normalized, np.ndarray) else \
                       torch.zeros_like(labels_normalized, device=device)
    
    def get_scaler_info(self):
        """è·å–æ ‡å‡†åŒ–å™¨ä¿¡æ¯"""
        info = {
            'feature_method': self.feature_method,
            'label_method': self.label_method,
            'handle_outliers': self.handle_outliers,
            'is_fitted': self.is_fitted
        }
        
        # æ·»åŠ ç‰¹å¾æ ‡å‡†åŒ–å™¨å‚æ•°
        if hasattr(self.input_scaler, 'mean_'):
            info['feature_mean'] = self.input_scaler.mean_.tolist()
        if hasattr(self.input_scaler, 'scale_'):
            info['feature_scale'] = self.input_scaler.scale_.tolist()
        
        # æ·»åŠ æ ‡ç­¾æ ‡å‡†åŒ–å™¨å‚æ•°
        if hasattr(self.output_scaler, 'mean_'):
            info['label_mean'] = self.output_scaler.mean_.tolist()
        if hasattr(self.output_scaler, 'scale_'):
            info['label_scale'] = self.output_scaler.scale_.tolist()
        
        return info

def generate_realistic_data(model, num_samples=200, config_path=None, seed=None, data_augmentation=True):
    """
    é«˜çº§æ•°æ®ç”Ÿæˆå™¨ - åŸºäºç‰©ç†çº¦æŸçš„æ•°æ®ç”Ÿæˆå’Œå¢å¼º
    ç‰¹æ€§ï¼šé…ç½®æ–‡ä»¶æ”¯æŒã€æ•°æ®å¢å¼ºã€è´¨é‡æ§åˆ¶ã€å‚æ•°ä¼˜åŒ–
    """
    # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§
    if seed is not None:
        torch.manual_seed(seed)
        np.random.seed(seed)
    
    print(f"ğŸ”„ é«˜çº§æ•°æ®ç”Ÿæˆå™¨å¯åŠ¨ - {num_samples}ä¸ªæ ·æœ¬")
    print(f"   æ•°æ®å¢å¼º: {'å¯ç”¨' if data_augmentation else 'ç¦ç”¨'}")
    
    # é»˜è®¤é…ç½®
    data_config = {
        'implementation_stage': 3,
        'parameter_ranges': {
            'frequency': {'min': 0.1, 'max': 10.0},
            'power': {'min': 0.01, 'max': 100.0},
            'dimension': {'min': 1e-6, 'max': 1e-3},
            'size': {'min': 1e-6, 'max': 1e-3},
            'default': {'min': None, 'max': None}
        },
        'augmentation_level': 0.1,
        'noise_level': 0.05,
        'correlation_strength': 0.7,
        'validation_ratio': 0.1
    }
    
    # ä»é…ç½®æ–‡ä»¶åŠ è½½å‚æ•°
    if config_path and os.path.exists(config_path):
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
                if 'æ•°æ®å¤„ç†' in config and 'æ•°æ®ç”Ÿæˆ' in config['æ•°æ®å¤„ç†']:
                    gen_config = config['æ•°æ®å¤„ç†']['æ•°æ®ç”Ÿæˆ']
                    for key, value in gen_config.items():
                        if key in data_config:
                            if key == 'parameter_ranges' and isinstance(value, dict):
                                # åˆå¹¶å‚æ•°èŒƒå›´é…ç½®
                                for param, ranges in value.items():
                                    if param not in data_config['parameter_ranges']:
                                        data_config['parameter_ranges'][param] = ranges
                                    else:
                                        data_config['parameter_ranges'][param].update(ranges)
                            else:
                                data_config[key] = value
                    print(f"âœ… æˆåŠŸåŠ è½½æ•°æ®ç”Ÿæˆé…ç½®")
        except Exception as e:
            print(f"âš ï¸  åŠ è½½æ•°æ®ç”Ÿæˆé…ç½®å¤±è´¥: {str(e)}")
    
    # åˆå§‹åŒ–è¾“å…¥è¾“å‡ºå±‚
    device = model.device
    input_layer = EWPINNInputLayer(device=device)
    output_layer = EWPINNOutputLayer(device=device)
    
    # è®¾ç½®å®ç°é˜¶æ®µ
    stage = data_config['implementation_stage']
    input_layer.set_implementation_stage(stage)
    output_layer.set_implementation_stage(stage)
    print(f"   å®ç°é˜¶æ®µ: {stage}")
    
    features_list = []
    labels_list = []
    success_count = 0
    
    # ç”Ÿæˆæ•°æ®
    start_time = time.time()
    for i in range(num_samples):
        try:
            # åˆ›å»ºè¾“å…¥å­—å…¸
            input_dict = input_layer.generate_example_input()
            
            # å¯¹è¾“å…¥å‚æ•°è¿›è¡Œåˆç†åŒ–è°ƒæ•´
            input_dict = normalize_input_parameters(input_dict, data_config['parameter_ranges'])
            
            # è½¬æ¢ä¸ºè¾“å…¥å‘é‡
            input_vector = input_layer.create_input_vector(input_dict)
            
            # ç¡®ä¿æ˜¯torchå¼ é‡
            if not isinstance(input_vector, torch.Tensor):
                input_vector = torch.tensor(input_vector, dtype=torch.float32, device=device)
            
            # åº”ç”¨æ•°æ®å¢å¼º
            if data_augmentation and np.random.random() < 0.7:  # 70%æ¦‚ç‡åº”ç”¨å¢å¼º
                input_vector = apply_data_augmentation(input_vector, 
                                                      level=data_config['augmentation_level'])
            
            # æ·»åŠ è½»å¾®å™ªå£°
            if data_config['noise_level'] > 0:
                noise = torch.randn_like(input_vector) * data_config['noise_level']
                input_vector = input_vector + noise
            
            features_list.append(input_vector)
            
            # ç”Ÿæˆå¯¹åº”çš„è¾“å‡ºæ ‡ç­¾
            random_output = output_layer.generate_random_output(batch_size=1)
            if isinstance(random_output, torch.Tensor):
                label_vector = random_output[0]
            else:
                # ç¡®ä¿è½¬æ¢ä¸ºtorchå¼ é‡
                label_vector = torch.tensor(random_output[0], dtype=torch.float32, device=device)
            
            # æ·»åŠ è¾“å‡ºå™ªå£°
            if data_config['noise_level'] > 0:
                output_noise = torch.randn_like(label_vector) * (data_config['noise_level'] * 0.5)
                label_vector = label_vector + output_noise
            
            labels_list.append(label_vector)
            success_count += 1
            
            # è¿›åº¦æ˜¾ç¤º
            if (i + 1) % 100 == 0 or i == num_samples - 1:
                elapsed = time.time() - start_time
                rate = (i + 1) / elapsed if elapsed > 0 else 0
                print(f"   è¿›åº¦: {i+1}/{num_samples} ({rate:.1f}æ ·æœ¬/ç§’)")
                
        except Exception as e:
            if (i + 1) % 50 == 0:
                print(f"âš ï¸  æ ·æœ¬ {i} ç”Ÿæˆå¤±è´¥: {str(e)}")
            # ä½¿ç”¨é›¶å‘é‡ä½œä¸ºå¤‡é€‰
            zero_vector = torch.zeros(24, dtype=torch.float32, device=device)
            zero_feature = torch.zeros(62, dtype=torch.float32, device=device)
            features_list.append(zero_feature)
            labels_list.append(zero_vector)
    
    # è½¬æ¢ä¸ºå¼ é‡
    features = torch.stack(features_list)
    labels = torch.stack(labels_list)
    
    # è®¡ç®—æ•°æ®ç»Ÿè®¡ä¿¡æ¯
    gen_time = time.time() - start_time
    success_rate = (success_count / num_samples) * 100 if num_samples > 0 else 0
    
    print(f"âœ… æ•°æ®ç”Ÿæˆå®Œæˆ")
    print(f"   æ ·æœ¬æ•°é‡: {features.shape[0]}")
    print(f"   æˆåŠŸç”Ÿæˆç‡: {success_rate:.1f}%")
    print(f"   ç”Ÿæˆæ—¶é—´: {gen_time:.2f}ç§’ ({num_samples/gen_time:.1f}æ ·æœ¬/ç§’)")
    print(f"   è¾“å…¥å½¢çŠ¶: {features.shape}, è¾“å‡ºå½¢çŠ¶: {labels.shape}")
    
    # åˆ†ç¦»è®­ç»ƒé›†å’ŒéªŒè¯é›†
    val_size = int(num_samples * data_config['validation_ratio'])
    if val_size > 0:
        indices = torch.randperm(num_samples)
        train_indices = indices[val_size:]
        val_indices = indices[:val_size]
        
        X_train, y_train = features[train_indices], labels[train_indices]
        X_val, y_val = features[val_indices], labels[val_indices]
        
        print(f"   è®­ç»ƒé›†: {X_train.shape[0]}æ ·æœ¬, éªŒè¯é›†: {X_val.shape[0]}æ ·æœ¬")
        
        return X_train, y_train, X_val, y_val
    else:
        return features, labels

def normalize_input_parameters(input_dict, parameter_ranges):
    """
    æ ‡å‡†åŒ–è¾“å…¥å‚æ•°ï¼Œç¡®ä¿åœ¨åˆç†èŒƒå›´å†…
    """
    for key, value in input_dict.items():
        if isinstance(value, (int, float)):
            # æŸ¥æ‰¾å‚æ•°èŒƒå›´
            param_range = None
            for param_key, ranges in parameter_ranges.items():
                if param_key.lower() in key.lower():
                    param_range = ranges
                    break
            
            # å¦‚æœæ‰¾åˆ°èŒƒå›´ï¼Œåº”ç”¨é™åˆ¶
            if param_range:
                min_val = param_range.get('min')
                max_val = param_range.get('max')
                
                if min_val is not None and value < min_val:
                    input_dict[key] = min_val
                if max_val is not None and value > max_val:
                    input_dict[key] = max_val
    
    return input_dict

def apply_data_augmentation(input_vector, level=0.1):
    """
    åº”ç”¨æ•°æ®å¢å¼ºåˆ°è¾“å…¥å‘é‡
    åŒ…æ‹¬éšæœºç¼©æ”¾ã€è½»å¾®æ—‹è½¬å’Œéçº¿æ€§å˜æ¢
    """
    augmented = input_vector.clone()
    
    # éšæœºç¼©æ”¾ - å¯¹ä¸åŒç»´åº¦åº”ç”¨ä¸åŒçš„ç¼©æ”¾å› å­
    scale_factors = 1.0 + (torch.randn_like(augmented) * level)
    augmented = augmented * scale_factors
    
    # é€‰æ‹©éƒ¨åˆ†ç»´åº¦è¿›è¡Œéçº¿æ€§å˜æ¢
    num_transformed = min(5, augmented.size(0))
    transform_indices = torch.randperm(augmented.size(0))[:num_transformed]
    
    for idx in transform_indices:
        # åº”ç”¨æ­£å¼¦å˜æ¢ä½œä¸ºéçº¿æ€§å¢å¼º
        augmented[idx] = augmented[idx] + torch.sin(augmented[idx]) * level
    
    return augmented

def progressive_training(config_path='model_config.json', resume_training=False, resume_checkpoint=None, mixed_precision=True, model_init_seed=None, use_adaptive_hyperopt=False, enable_performance_monitor=True, enable_advanced_regularization=True, use_efficient_architecture=True, model_compression_factor=1.0):
    """
    é«˜çº§æ¸è¿›å¼è®­ç»ƒç­–ç•¥ - é›†æˆPINNç‰©ç†çº¦æŸå’Œé›†æˆå­¦ä¹ æ”¯æŒ
    ç‰¹æ€§ï¼šé…ç½®æ–‡ä»¶æ”¯æŒã€å¤šç§ä¼˜åŒ–å™¨ã€é«˜çº§å­¦ä¹ ç‡è°ƒåº¦ã€æ—©åœæœºåˆ¶ã€æ··åˆç²¾åº¦è®­ç»ƒã€ç‰©ç†çº¦æŸã€é›†æˆå­¦ä¹ æ”¯æŒã€è‡ªé€‚åº”è¶…å‚æ•°ä¼˜åŒ–ã€æ¨¡å‹æ€§èƒ½ç›‘æ§ä¸è¯Šæ–­
    
    å‚æ•°:
    - config_path: é…ç½®æ–‡ä»¶è·¯å¾„
    - resume_training: æ˜¯å¦ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
    - resume_checkpoint: æ¢å¤è®­ç»ƒçš„æ£€æŸ¥ç‚¹è·¯å¾„
    - mixed_precision: æ˜¯å¦å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
    - model_init_seed: æ¨¡å‹åˆå§‹åŒ–ç§å­ï¼Œç”¨äºåˆ›å»ºå…·æœ‰ä¸åŒåˆå§‹æƒé‡çš„æ¨¡å‹ï¼ˆæ”¯æŒé›†æˆå­¦ä¹ ï¼‰
    - use_adaptive_hyperopt: æ˜¯å¦å¯ç”¨è‡ªé€‚åº”è¶…å‚æ•°ä¼˜åŒ–
    - enable_performance_monitor: æ˜¯å¦å¯ç”¨æ¨¡å‹æ€§èƒ½ç›‘æ§ä¸è¯Šæ–­å·¥å…·
    """
    print("ğŸš€ EWPINNé«˜çº§ä¼˜åŒ–è®­ç»ƒç³»ç»Ÿå¯åŠ¨ - é›†æˆç‰©ç†çº¦æŸ")
    print("=" * 60)
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {device}")
    print(f"âš¡ æ··åˆç²¾åº¦è®­ç»ƒ: {'å¯ç”¨' if mixed_precision and torch.cuda.is_available() else 'ç¦ç”¨'}")
    print(f"ğŸ”¬ PINNç‰©ç†çº¦æŸ: å·²å¯ç”¨")
    print(f"ğŸ¯ è‡ªé€‚åº”è¶…å‚æ•°ä¼˜åŒ–: {'å¯ç”¨' if use_adaptive_hyperopt else 'ç¦ç”¨'}")
    print(f"ğŸ“Š æ¨¡å‹æ€§èƒ½ç›‘æ§: {'å¯ç”¨' if enable_performance_monitor else 'ç¦ç”¨'}")

    # æŠ‘åˆ¶ç‰©ç†æ¨¡å—æ—¥å¿—è¾“å‡ºï¼ˆä»…æ˜¾ç¤ºé”™è¯¯ï¼‰
    import logging
    physics_logger = logging.getLogger('EWPINN_Physics')
    physics_logger.setLevel(logging.ERROR)
    physics_logger.propagate = True
    
    # åˆ›å»ºæ—¶é—´æˆ³
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # æ¨¡å‹åˆå§‹åŒ–ç§å­ï¼ˆç”¨äºé›†æˆå­¦ä¹ ï¼Œåˆ›å»ºä¸åŒçš„åˆå§‹æƒé‡ï¼‰
    print(f"ğŸ² æ¨¡å‹åˆå§‹åŒ–ç§å­: {'éšæœº' if model_init_seed is None else model_init_seed} (ç”¨äºé›†æˆå­¦ä¹ )")
    
    # é»˜è®¤é…ç½®
    default_config = {
        'æ¨¡å‹': {
            'è¾“å…¥ç»´åº¦': 62,
            'è¾“å‡ºç»´åº¦': 24,
            'éšè—å±‚': [128, 64, 32],
            'æ¿€æ´»å‡½æ•°': 'ReLU',
            'æ‰¹æ ‡å‡†åŒ–': True,
            'Dropoutç‡': 0.1
        },
        'è®­ç»ƒ': {
            'æ¸è¿›å¼è®­ç»ƒ': [
                {
                    'åç§°': 'é¢„çƒ­é˜¶æ®µ',
                    'è½®æ¬¡': 10,
                    'å­¦ä¹ ç‡': 1e-4,
                    'æ‰¹æ¬¡å¤§å°': 16,
                    'æƒé‡è¡°å‡': 1e-5,
                    'ä¼˜åŒ–å™¨': 'AdamW',
                    'è°ƒåº¦ç­–ç•¥': 'CosineAnnealing',
                    'è°ƒåº¦å‚æ•°': {'T_max': 10},
                    'æè¿°': 'å°å­¦ä¹ ç‡é¢„çƒ­ï¼Œæ¿€æ´»å‡½æ•°é€‚åº”',
                    'ç‰©ç†çº¦æŸæƒé‡': 0.05
                },
                {
                    'åç§°': 'ä¸»è®­ç»ƒé˜¶æ®µ',
                    'è½®æ¬¡': 20,
                    'å­¦ä¹ ç‡': 5e-4,
                    'æ‰¹æ¬¡å¤§å°': 32,
                    'æƒé‡è¡°å‡': 1e-5,
                    'ä¼˜åŒ–å™¨': 'AdamW',
                    'è°ƒåº¦ç­–ç•¥': 'CosineAnnealing',
                    'è°ƒåº¦å‚æ•°': {'T_max': 20},
                    'æè¿°': 'ä¸»è¦è®­ç»ƒé˜¶æ®µï¼Œå¹³è¡¡æ”¶æ•›é€Ÿåº¦ä¸ç¨³å®šæ€§',
                    'ç‰©ç†çº¦æŸæƒé‡': 0.1
                },
                {
                    'åç§°': 'ç²¾ç»†è°ƒä¼˜',
                    'è½®æ¬¡': 10,
                    'å­¦ä¹ ç‡': 1e-4,
                    'æ‰¹æ¬¡å¤§å°': 32,
                    'æƒé‡è¡°å‡': 1e-6,
                    'ä¼˜åŒ–å™¨': 'AdamW',
                    'è°ƒåº¦ç­–ç•¥': 'CosineAnnealing',
                    'è°ƒåº¦å‚æ•°': {'T_max': 10},
                    'æè¿°': 'ç²¾ç»†è°ƒä¼˜ï¼Œæé«˜ç²¾åº¦',
                    'ç‰©ç†çº¦æŸæƒé‡': 0.2
                }
            ],
            'æ—©åœé…ç½®': {
                'å¯ç”¨': True,
                'è€å¿ƒå€¼': 5,
                'æœ€å°æ”¹è¿›': 5e-4,
                'æ¢å¤æœ€ä½³æ¨¡å‹': True
            },
            'æ¢¯åº¦è£å‰ª': 1.0,
            'æ¢¯åº¦ç´¯ç§¯æ­¥æ•°': 1
        },
        'æ•°æ®': {
            'æ ·æœ¬æ•°é‡': 300,
            'æ•°æ®å¢å¼º': True,
            'è®­ç»ƒæ¯”ä¾‹': 0.8,
            'éªŒè¯æ¯”ä¾‹': 0.1,
            'æµ‹è¯•æ¯”ä¾‹': 0.1
        },
        'ç‰©ç†çº¦æŸ': {
            'å¯ç”¨': True,
            'åˆå§‹æƒé‡': 0.1,
            'æƒé‡è¡°å‡': 0.99,
            'ç‰©ç†ç‚¹æ•°é‡': 500,
            'æ®‹å·®æƒé‡': {
                'è¿ç»­æ€§': 1.0,
                'åŠ¨é‡_u': 0.1,
                'åŠ¨é‡_v': 0.1,
                'åŠ¨é‡_w': 0.1
            },
            'è‡ªé€‚åº”æƒé‡': True
        }
    }
    
    # åŠ è½½é…ç½®æ–‡ä»¶
    config = default_config.copy()
    if config_path and os.path.exists(config_path):
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                user_config = json.load(f)
                # æ·±åº¦åˆå¹¶é…ç½®
                if 'æ¨¡å‹' in user_config:
                    config['æ¨¡å‹'].update(user_config['æ¨¡å‹'])
                if 'è®­ç»ƒ' in user_config:
                    if 'æ¸è¿›å¼è®­ç»ƒ' in user_config['è®­ç»ƒ']:
                        config['è®­ç»ƒ']['æ¸è¿›å¼è®­ç»ƒ'] = user_config['è®­ç»ƒ']['æ¸è¿›å¼è®­ç»ƒ']
                    if 'æ—©åœé…ç½®' in user_config['è®­ç»ƒ']:
                        config['è®­ç»ƒ']['æ—©åœé…ç½®'].update(user_config['è®­ç»ƒ']['æ—©åœé…ç½®'])
                    for key, value in user_config['è®­ç»ƒ'].items():
                        if key not in ['æ¸è¿›å¼è®­ç»ƒ', 'æ—©åœé…ç½®']:
                            config['è®­ç»ƒ'][key] = value
                if 'æ•°æ®' in user_config:
                    config['æ•°æ®'].update(user_config['æ•°æ®'])
            print(f"âœ… æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
        except Exception as e:
            print(f"âš ï¸  åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {str(e)}")
    else:
        print(f"â„¹ï¸  é…ç½®æ–‡ä»¶ä¸å­˜åœ¨æˆ–æœªæŒ‡å®šï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
    
    # åˆå§‹åŒ–æ¨¡å‹
    # å¦‚æœè®¾ç½®äº†æ¨¡å‹åˆå§‹åŒ–ç§å­ï¼Œä½¿ç”¨å®ƒæ¥åˆ›å»ºä¸åŒçš„åˆå§‹æƒé‡åˆ†å¸ƒ
    if model_init_seed is not None:
        # ä¸´æ—¶è®¾ç½®éšæœºç§å­ç”¨äºæ¨¡å‹åˆå§‹åŒ–
        original_state = torch.get_rng_state()
        torch.manual_seed(model_init_seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(model_init_seed)
    
    # æ ¹æ®å‚æ•°é€‰æ‹©æ¨¡å‹æ¶æ„
    if use_efficient_architecture:
        print(f"ğŸ”§ ä½¿ç”¨é«˜æ•ˆEWPINNæ¶æ„ï¼Œå‹ç¼©å› å­: {model_compression_factor}")
        model = EfficientEWPINN(
            input_dim=config['æ¨¡å‹']['è¾“å…¥ç»´åº¦'],
            output_dim=config['æ¨¡å‹']['è¾“å‡ºç»´åº¦'],
            hidden_layers=config['æ¨¡å‹']['éšè—å±‚'],
            dropout_rate=config['æ¨¡å‹']['Dropoutç‡'],
            activation=config['æ¨¡å‹']['æ¿€æ´»å‡½æ•°'],
            batch_norm=config['æ¨¡å‹']['æ‰¹æ ‡å‡†åŒ–'],
            device=device,
            compression_factor=model_compression_factor,
            use_residual=True,
            use_attention=True,
            gradient_checkpointing=False
        )
        
        # è·å–æ¨¡å‹ä¼˜åŒ–å»ºè®®
        optimization_suggestions = get_model_optimization_suggestions(model)
        for suggestion in optimization_suggestions:
            print(f"ğŸ’¡ ä¼˜åŒ–å»ºè®®: {suggestion}")
    else:
        model = OptimizedEWPINN(
            input_dim=config['æ¨¡å‹']['è¾“å…¥ç»´åº¦'],
            output_dim=config['æ¨¡å‹']['è¾“å‡ºç»´åº¦'],
            hidden_layers=config['æ¨¡å‹']['éšè—å±‚'],
            dropout_rate=config['æ¨¡å‹']['Dropoutç‡'],
            activation=config['æ¨¡å‹']['æ¿€æ´»å‡½æ•°'],
            batch_norm=config['æ¨¡å‹']['æ‰¹æ ‡å‡†åŒ–'],
            device=device
        )
    
    # æ¢å¤åŸå§‹éšæœºç§å­çŠ¶æ€
    if model_init_seed is not None:
        torch.set_rng_state(original_state)
        if torch.cuda.is_available():
            torch.cuda.set_rng_state_all(original_state)
    
    # åˆå§‹åŒ–è‡ªé€‚åº”è¶…å‚æ•°ä¼˜åŒ–å™¨
    hyperoptimizer = None
    if use_adaptive_hyperopt:
        hyperoptimizer = AdaptiveHyperparameterOptimizer(
            config=config,
            device=device,
            patience=5,
            reduction_factor=0.5,
            verbose=True
        )
    
    # åˆå§‹åŒ–æ¨¡å‹æ€§èƒ½ç›‘æ§å™¨
    performance_monitor = None
    if enable_performance_monitor:
        perf_dir = os.path.join(os.getcwd(), 'performance_reports')
        try:
            os.makedirs(perf_dir, exist_ok=True)
        except Exception:
            pass
        performance_monitor = ModelPerformanceMonitor(
            device=device,
            save_dir=perf_dir
        )
    
    # åˆå§‹åŒ–é«˜çº§æ­£åˆ™åŒ–å™¨
    regularizer = None
    gradient_noise_reg = None
    if enable_advanced_regularization:
        # ä»é…ç½®ä¸­è·å–æ­£åˆ™åŒ–å‚æ•°
        reg_config = config.get('æ­£åˆ™åŒ–é…ç½®', {})
        regularizer = AdvancedRegularizer(
            config_path=None,  # å¯ä»¥ä»é…ç½®æ–‡ä»¶åŠ è½½
            l1_lambda=reg_config.get('L1æ­£åˆ™åŒ–ç³»æ•°', 0.0),
            l2_lambda=reg_config.get('L2æ­£åˆ™åŒ–ç³»æ•°', 0.001),
            dropout_rate=reg_config.get('Dropoutç‡', config['æ¨¡å‹'].get('Dropoutç‡', 0.1)),
            use_weight_clipping=reg_config.get('ä½¿ç”¨æƒé‡è£å‰ª', False),
            weight_clip_value=reg_config.get('æƒé‡è£å‰ªé˜ˆå€¼', 1.0),
            use_spectral_norm=reg_config.get('ä½¿ç”¨è°±å½’ä¸€åŒ–', False),
            enable_early_stopping=reg_config.get('å¯ç”¨æ—©åœ', True),
            patience=reg_config.get('æ—©åœè€å¿ƒå€¼', 10),
            device=device
        )
        
        # åˆå§‹åŒ–æ¢¯åº¦å™ªå£°æ­£åˆ™åŒ–å™¨
        apply_gradient_noise = reg_config.get('åº”ç”¨æ¢¯åº¦å™ªå£°', False)
        if apply_gradient_noise:
            gradient_noise_reg = GradientNoiseRegularizer(
                eta=reg_config.get('æ¢¯åº¦å™ªå£°ç³»æ•°', 0.01),
                gamma=reg_config.get('æ¢¯åº¦å™ªå£°è¡°å‡ç‡', 0.55)
            )
        
        # åº”ç”¨æ­£åˆ™åŒ–åˆ°æ¨¡å‹
        apply_dropconnect = reg_config.get('åº”ç”¨DropConnect', False)
        if apply_dropconnect:
            model = apply_regularization_to_model(
                model,
                regularizer,
                apply_dropconnect=True,
                dropconnect_rate=reg_config.get('DropConnectç‡', 0.2)
            )
        
        print(f"âœ… é«˜çº§æ­£åˆ™åŒ–å·²å¯ç”¨")
    
    # åˆå§‹åŒ–ç‰©ç†çº¦æŸå±‚ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    pinn_layer = None
    physics_enabled = config.get('ç‰©ç†çº¦æŸ', {}).get('å¯ç”¨', True)
    if physics_enabled:
        # åˆ›å»ºç‰©ç†çº¦æŸå±‚
        residual_weights = config['ç‰©ç†çº¦æŸ'].get('æ®‹å·®æƒé‡', {})
        pinn_layer = PINNConstraintLayer(
            residual_weights=residual_weights,
        )
        pinn_layer.adaptive_weights = config['ç‰©ç†çº¦æŸ'].get('è‡ªé€‚åº”æƒé‡', True)
        pinn_layer = pinn_layer.to(device)
        print(f"âœ… ç‰©ç†çº¦æŸå±‚å·²åˆå§‹åŒ–: è‡ªé€‚åº”æƒé‡={pinn_layer.adaptive_weights}")
    
    # ç”Ÿæˆæ•°æ®ï¼ˆä½¿ç”¨æ”¹è¿›åçš„ç‰ˆæœ¬ï¼‰
    num_samples = config['æ•°æ®']['æ ·æœ¬æ•°é‡']
    data_augmentation = config['æ•°æ®']['æ•°æ®å¢å¼º']
    
    # å¦‚æœæ•°æ®ç”Ÿæˆå‡½æ•°æ”¯æŒè®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†åˆ†ç¦»ï¼Œç›´æ¥è·å–
    if hasattr(generate_realistic_data, '__code__') and 'validation_ratio' in generate_realistic_data.__code__.co_varnames:
        X_train_raw, y_train_raw, X_val_raw, y_val_raw = generate_realistic_data(
            model, 
            num_samples=num_samples, 
            config_path=config_path,
            seed=42,  # è®¾ç½®å›ºå®šç§å­ç¡®ä¿å¯é‡å¤æ€§
            data_augmentation=data_augmentation
        )
        # åˆå§‹åŒ–æ ‡å‡†åŒ–å™¨
        normalizer = DataNormalizer(
            feature_method='robust',  # ä½¿ç”¨robustæ–¹æ³•æ›´é€‚åˆå¤„ç†å¼‚å¸¸å€¼
            label_method='minmax',
            handle_outliers=True,
            outlier_threshold=2.5  # é™ä½é˜ˆå€¼ä»¥å¤„ç†æ›´å¤šå¼‚å¸¸å€¼
        )
        # å¯¹æ‰€æœ‰æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–
        # ä»…åœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆæ ‡å‡†åŒ–å™¨
        combined_features = torch.cat([X_train_raw, X_val_raw])
        combined_labels = torch.cat([y_train_raw, y_val_raw])
        normalizer.fit(combined_features, combined_labels)
        
        # æ ‡å‡†åŒ–å„ä¸ªæ•°æ®é›†
        X_train = normalizer.transform_features(X_train_raw)
        y_train = normalizer.transform_labels(y_train_raw)
        X_val = normalizer.transform_features(X_val_raw)
        y_val = normalizer.transform_labels(y_val_raw)
        
        # æ‰‹åŠ¨åˆ›å»ºæµ‹è¯•é›†
        test_size = int(len(X_train) * (config['æ•°æ®']['æµ‹è¯•æ¯”ä¾‹'] / config['æ•°æ®']['è®­ç»ƒæ¯”ä¾‹']))
        X_test, y_test = X_train[:test_size], y_train[:test_size]
        X_train, y_train = X_train[test_size:], y_train[test_size:]
    else:
        # å¤„ç†æ•°æ®ç”Ÿæˆé€»è¾‘ï¼Œæ”¯æŒè¿”å›2ä¸ªæˆ–4ä¸ªå€¼
        data_result = generate_realistic_data(model, num_samples=num_samples)
        if len(data_result) == 4:  # è¿”å›çš„æ˜¯X_train, y_train, X_val, y_val
            features, labels = data_result[0], data_result[1]  # åªä½¿ç”¨è®­ç»ƒé›†ä½œä¸ºåŸå§‹ç‰¹å¾å’Œæ ‡ç­¾
        else:  # è¿”å›çš„æ˜¯features, labels
            features, labels = data_result
        
        # æ•°æ®æ ‡å‡†åŒ–
        normalizer = DataNormalizer()
        normalizer.fit(features, labels)
        
        # æ ‡å‡†åŒ–æ•°æ®
        features_normalized = normalizer.transform_features(features)
        labels_normalized = normalizer.transform_labels(labels)
        
        # åˆ†å‰²æ•°æ®é›†
        train_size = int(config['æ•°æ®']['è®­ç»ƒæ¯”ä¾‹'] * len(features_normalized))
        val_size = int(config['æ•°æ®']['éªŒè¯æ¯”ä¾‹'] * len(features_normalized))
        
        X_train = features_normalized[:train_size]
        y_train = labels_normalized[:train_size]
        X_val = features_normalized[train_size:train_size+val_size]
        y_val = labels_normalized[train_size:train_size+val_size]
        X_test = features_normalized[train_size+val_size:]
        y_test = labels_normalized[train_size+val_size:]

    # ç»Ÿä¸€æ•°æ®æ¥å£ï¼šåˆ›å»ºæµ‹è¯•æ•°æ®é›†
    device = next(model.parameters()).device
    test_dataset = create_dataset(X_test, y_test, input_layer=None, stage=None, device=device)
    
    # ç”Ÿæˆç‰©ç†ç‚¹ï¼ˆå¦‚æœå¯ç”¨ç‰©ç†çº¦æŸï¼‰
    X_phys = None
    if physics_enabled:
        num_phys_samples = config['ç‰©ç†çº¦æŸ'].get('ç‰©ç†ç‚¹æ•°é‡', 500)
        print(f"ğŸ”¬ ç”Ÿæˆç‰©ç†çº¦æŸç‚¹: {num_phys_samples}ä¸ª")
        # ä½¿ç”¨æ•°æ®ç”Ÿæˆå™¨ç”Ÿæˆç‰©ç†ç‚¹ï¼Œä½†ä¸éœ€è¦å¯¹åº”çš„æ ‡ç­¾
        phys_input_layer = EWPINNInputLayer(device=device)
        phys_input_layer.set_implementation_stage(config['æ•°æ®'].get('implementation_stage', 3))
        
        X_phys_list = []
        for _ in range(num_phys_samples):
            input_dict = phys_input_layer.generate_example_input()
            input_vector = phys_input_layer.create_input_vector(input_dict)
            if not isinstance(input_vector, torch.Tensor):
                input_vector = torch.tensor(input_vector, dtype=torch.float32, device=device)
            else:
                input_vector = input_vector.to(device)
            X_phys_list.append(input_vector)
        
        X_phys = torch.stack(X_phys_list).to(device)
        print(f"âœ… ç‰©ç†çº¦æŸç‚¹ç”Ÿæˆå®Œæˆ: {X_phys.shape[0]}ä¸ªæ ·æœ¬")
    
    # ç¡®ä¿æœ‰normalizerå®ä¾‹
    if 'normalizer' not in locals():
        normalizer = DataNormalizer()
        normalizer.fit(X_train, y_train)
    
    print(f"ğŸ“Š æ ‡å‡†åŒ–åæ•°æ®èŒƒå›´:")
    print(f"   è¾“å…¥: [{X_train.min():.3f}, {X_train.max():.3f}]")
    print(f"   è¾“å‡º: [{y_train.min():.3f}, {y_train.max():.3f}]")
    print(f"ğŸ“ˆ æ•°æ®é›†åˆ’åˆ†: è®­ç»ƒ{len(X_train)}, éªŒè¯{len(X_val)}, æµ‹è¯•{len(X_test)}")
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    if resume_training and resume_checkpoint:
        # ä»æ£€æŸ¥ç‚¹è·¯å¾„æ¨æ–­ä¿å­˜ç›®å½•
        save_dir = os.path.dirname(resume_checkpoint)
        print(f"ğŸ”„ ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ: {resume_checkpoint}")
    else:
        save_dir = f"checkpoints_optimized_{timestamp}"
        os.makedirs(save_dir, exist_ok=True)
        print(f"ğŸ“ åˆ›å»ºä¿å­˜ç›®å½•: {save_dir}")
    
    # è®­ç»ƒå†å²
    train_history = []
    val_history = []
    best_val_loss = float('inf')
    best_model_state = None
    no_improve_count = 0
    
    # æ—©åœé…ç½®
    early_stopping = config['è®­ç»ƒ']['æ—©åœé…ç½®']['å¯ç”¨']
    # å¢åŠ è€å¿ƒå€¼ï¼Œç»™ç‰©ç†çº¦æŸPINNæ¨¡å‹æ›´å¤šæ”¶æ•›æ—¶é—´
    patience = config['è®­ç»ƒ']['æ—©åœé…ç½®'].get('è€å¿ƒå€¼', 30)
    min_improvement = config['è®­ç»ƒ']['æ—©åœé…ç½®'].get('æœ€å°æ”¹è¿›', 1e-6)
    restore_best = config['è®­ç»ƒ']['æ—©åœé…ç½®'].get('æ¢å¤æœ€ä½³æ¨¡å‹', True)
    
    # æ¢¯åº¦è£å‰ªå’Œç´¯ç§¯
    gradient_clip = config['è®­ç»ƒ']['æ¢¯åº¦è£å‰ª']
    gradient_accumulation_steps = config['è®­ç»ƒ']['æ¢¯åº¦ç´¯ç§¯æ­¥æ•°']
    
    # å‡†å¤‡æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = torch.cuda.amp.GradScaler() if mixed_precision and torch.cuda.is_available() else None
    
    # åˆå§‹åŒ–LossStabilizer
    loss_stabilizer = LossStabilizer(config_path=config_path)
    
    # æ¢å¤è®­ç»ƒçŠ¶æ€ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    start_stage = 0
    if resume_training and resume_checkpoint and os.path.exists(resume_checkpoint):
        try:
            checkpoint = torch.load(resume_checkpoint, map_location=device)
            model.load_state_dict(checkpoint['model_state_dict'])
            train_history = checkpoint.get('train_history', [])
            val_history = checkpoint.get('val_history', [])
            best_val_loss = checkpoint.get('best_val_loss', float('inf'))
            start_stage = checkpoint.get('last_stage', 0)
            print(f"âœ… æˆåŠŸæ¢å¤è®­ç»ƒçŠ¶æ€ï¼Œä»é˜¶æ®µ {start_stage + 1} ç»§ç»­")
        except Exception as e:
            print(f"âš ï¸  æ¢å¤è®­ç»ƒçŠ¶æ€å¤±è´¥: {str(e)}")
    
    # æ¸è¿›å¼è®­ç»ƒé˜¶æ®µ
    training_stages = config['è®­ç»ƒ']['æ¸è¿›å¼è®­ç»ƒ']
    
    # å¼€å§‹æ¸è¿›å¼è®­ç»ƒ
    for stage_idx, stage_config in enumerate(training_stages):
        # å¦‚æœæ˜¯æ¢å¤è®­ç»ƒï¼Œè·³è¿‡å·²å®Œæˆçš„é˜¶æ®µ
        if stage_idx < start_stage:
            continue
            
        print(f"\nğŸ¯ {stage_config['åç§°']} (é˜¶æ®µ {stage_idx + 1}/{len(training_stages)})")
        print(f"   {stage_config['æè¿°']}")
        print(f"   ä¼˜åŒ–å™¨: {stage_config['ä¼˜åŒ–å™¨']}, è°ƒåº¦ç­–ç•¥: {stage_config['è°ƒåº¦ç­–ç•¥']}")
        
        # åˆ›å»ºä¼˜åŒ–å™¨ - æ”¯æŒå¤šç§ä¼˜åŒ–å™¨
        optimizer = create_optimizer(model, stage_config)
        
        # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨ - æ”¯æŒå¤šç§è°ƒåº¦ç­–ç•¥
        scheduler = create_lr_scheduler(optimizer, stage_config)

        # ä½¿ç”¨ç»Ÿä¸€æ•°æ®æ¥å£åˆ›å»ºæœ¬é˜¶æ®µçš„æ•°æ®åŠ è½½å™¨
        train_loader = create_dataloader(
            X_train, y_train, batch_size=stage_config.get('æ‰¹æ¬¡å¤§å°', 16), shuffle=True,
            device=device, num_workers=0, drop_last=True, pin_memory=False
        )
        val_loader = create_dataloader(
            X_val, y_val, batch_size=stage_config.get('æ‰¹æ¬¡å¤§å°', 16), shuffle=False,
            device=device, num_workers=0, drop_last=False, pin_memory=False
        )
        
        # è®­ç»ƒå¾ªç¯
        stage_start_time = time.time()
        stage_train_losses = []
        stage_val_losses = []
        
        for epoch in range(stage_config['è½®æ¬¡']):
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            train_loss = 0.0
            train_data_loss = 0.0
            train_physics_loss = 0.0
            train_mae = 0.0
            num_train_batches = 0
            batch_start_time = time.time()
            
            # ä½¿ç”¨ç»Ÿä¸€çš„DataLoaderè¿›è¡Œæ‰¹å¤„ç†
            num_train_batches = len(train_loader)
            optimizer.zero_grad()
            for batch_idx, (batch_features, batch_labels) in enumerate(train_loader):
                batch_features = batch_features.to(device)
                batch_labels = batch_labels.to(device)
                stage_physics_weight = stage_config.get('ç‰©ç†çº¦æŸæƒé‡', config['ç‰©ç†çº¦æŸ'].get('åˆå§‹æƒé‡', 0.1))
                if physics_enabled and X_phys is not None:
                    phys_indices = torch.randperm(len(X_phys))[:batch_features.size(0)]
                    X_phys_batch = X_phys[phys_indices].to(device)
                with torch.cuda.amp.autocast(enabled=scaler is not None):
                    predictions = model(batch_features)
                    if physics_enabled and X_phys is not None:
                        data_loss = loss_stabilizer.compute_loss(predictions, batch_labels)
                        phys_outputs = model(X_phys_batch)
                        physics_loss_val, _ = pinn_layer.compute_physics_loss(X_phys_batch, phys_outputs)
                        total_loss = data_loss + stage_physics_weight * physics_loss_val
                    else:
                        data_loss = loss_stabilizer.compute_loss(predictions, batch_labels)
                        total_loss = data_loss
                        physics_loss_val = torch.tensor(0.0, device=device)
                    if regularizer is not None:
                        total_loss = total_loss + regularizer.compute_regularization_loss(model)
                    mae = torch.mean(torch.abs(predictions - batch_labels))
                combined_loss = total_loss + mae * 0.1
                if scaler is not None:
                    scaler.scale(combined_loss / gradient_accumulation_steps).backward()
                else:
                    (combined_loss / gradient_accumulation_steps).backward()
                if ((batch_idx + 1) % gradient_accumulation_steps == 0) or (batch_idx == num_train_batches - 1):
                    if gradient_clip > 0:
                        if scaler is not None:
                            scaler.unscale_(optimizer)
                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=gradient_clip)
                    if scaler is not None:
                        scaler.step(optimizer)
                        scaler.update()
                    else:
                        optimizer.step()
                    optimizer.zero_grad()
                train_loss += total_loss.item() * batch_features.size(0)
                train_data_loss += data_loss.item() * batch_features.size(0)
                physics_loss_value = physics_loss_val.item() if hasattr(physics_loss_val, 'item') else physics_loss_val
                if physics_enabled and X_phys is not None:
                    train_physics_loss += physics_loss_value * stage_physics_weight * batch_features.size(0)
                train_mae += mae.item() * batch_features.size(0)
            
            # æ›´æ–°æ€§èƒ½ç›‘æ§å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if enable_performance_monitor and performance_monitor is not None:
                avg_train_loss = train_loss / len(X_train)
                avg_train_mae = train_mae / len(X_train)
                performance_monitor.log_training_metrics(
                    epoch=epoch,
                    train_loss=avg_train_loss,
                    val_loss=0.0,  # ä¸´æ—¶å€¼ï¼Œåç»­å¯æ›¿æ¢ä¸ºå®é™…éªŒè¯æŸå¤±
                    train_mae=avg_train_mae,
                    data_loss=train_data_loss / len(X_train) if physics_enabled else avg_train_loss,
                    physics_loss=train_physics_loss / len(X_train) if physics_enabled else 0.0,
                    learning_rate=scheduler.get_last_lr()[0] if scheduler is not None else stage_config['å­¦ä¹ ç‡']
                )
            
            # éªŒè¯é˜¶æ®µ
            model.eval()
            val_loss = 0.0
            val_data_loss = 0.0
            val_physics_loss = 0.0
            val_mae = 0.0
            num_val_batches = 0
            
            with torch.no_grad():
                for batch_features, batch_labels in val_loader:
                    batch_features = batch_features.to(device)
                    batch_labels = batch_labels.to(device)
                    
                    # éªŒè¯æ—¶ä¹Ÿå¯ä»¥è®¡ç®—ç‰©ç†çº¦æŸæŸå¤±
                    if physics_enabled and X_phys is not None:
                        phys_val_indices = torch.randperm(len(X_phys))[:len(batch_features)]
                        X_phys_val_batch = X_phys[phys_val_indices].to(device)
                    
                    with torch.cuda.amp.autocast(enabled=scaler is not None):
                        predictions = model(batch_features)
                        
                        # é›†æˆç‰©ç†çº¦æŸçš„æŸå¤±è®¡ç®—
                        if physics_enabled and X_phys is not None:
                            # è®¡ç®—æ•°æ®æŸå¤±
                            data_val_loss = loss_stabilizer.compute_loss(predictions, batch_labels)
                            
                            # è®¡ç®—ç‰©ç†çº¦æŸæŸå¤±ï¼ˆåœ¨ç‰©ç†ç‚¹ä¸Šï¼‰
                            phys_val_outputs = model(X_phys_val_batch)
                            physics_val_loss, _ = pinn_layer.compute_physics_loss(
                                X_phys_val_batch, phys_val_outputs
                            )
                            
                            # ç»„åˆæŸå¤±
                            total_val_loss = data_val_loss + stage_physics_weight * physics_val_loss
                        else:
                            # ä»…æ•°æ®æŸå¤±
                            total_val_loss = loss_stabilizer.compute_loss(predictions, batch_labels)
                            data_val_loss = total_val_loss
                            physics_val_loss = torch.tensor(0.0, device=device)
                        
                        val_batch_mae = torch.mean(torch.abs(predictions - batch_labels))
                    
                    val_loss += total_val_loss.item() * len(batch_features)
                    if physics_enabled and X_phys is not None:
                          val_data_loss += data_val_loss.item() * len(batch_features)
                          # å¤„ç†physics_val_losså¯èƒ½æ˜¯floatç±»å‹çš„æƒ…å†µ
                          physics_val_loss_value = physics_val_loss.item() if hasattr(physics_val_loss, 'item') else physics_val_loss
                          val_physics_loss += physics_val_loss_value * stage_physics_weight * len(batch_features)
                    # å¤„ç†val_batch_maeå¯èƒ½æ˜¯floatç±»å‹çš„æƒ…å†µ
                    val_batch_mae_value = val_batch_mae.item() if hasattr(val_batch_mae, 'item') else val_batch_mae
                    val_mae += val_batch_mae_value * len(batch_features)
                    num_val_batches += 1
            
            # è®¡ç®—å¹³å‡æŸå¤±ï¼ˆè€ƒè™‘æ‰¹æ¬¡å¤§å°ä¸åŒï¼‰
            avg_train_loss = train_loss / len(X_train)
            avg_train_mae = train_mae / len(X_train)
            avg_val_loss = val_loss / len(X_val)
            avg_val_mae = val_mae / len(X_val)
            
            # è®¡ç®—å¹³å‡ç‰©ç†æŸå¤±ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            avg_train_data_loss = 0
            avg_train_physics_loss = 0
            avg_val_data_loss = 0
            avg_val_physics_loss = 0
            if physics_enabled and X_phys is not None:
                avg_train_data_loss = train_data_loss / len(X_train)
                avg_train_physics_loss = train_physics_loss / len(X_train)
                avg_val_data_loss = val_data_loss / len(X_val)
                avg_val_physics_loss = val_physics_loss / len(X_val)
            
            stage_train_losses.append(avg_train_loss)
            stage_val_losses.append(avg_val_loss)
            
            # æ›´æ–°æ€§èƒ½ç›‘æ§å™¨çš„éªŒè¯æŒ‡æ ‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if enable_performance_monitor and performance_monitor is not None:
                performance_monitor.log_training_metrics(
                    epoch=epoch,
                    train_loss=0.0,  # ä¸´æ—¶å€¼ï¼Œä¸»è¦è®°å½•éªŒè¯æŒ‡æ ‡
                    val_loss=avg_val_loss,
                    val_mae=avg_val_mae,
                    data_loss=avg_val_data_loss if physics_enabled else avg_val_loss,
                    physics_loss=avg_val_physics_loss if physics_enabled else 0.0
                )
                
                # å®šæœŸç”Ÿæˆæ€§èƒ½è¯Šæ–­æŠ¥å‘Šå’Œå¯è§†åŒ–
                if (epoch + 1) % 10 == 0 or epoch == stage_config['è½®æ¬¡'] - 1:
                    # ç¡®ä¿metrics_historyä¸­æœ‰è¶³å¤Ÿçš„æ•°æ®ç‚¹ä¸”æ•°ç»„é•¿åº¦åŒ¹é…
                    if len(performance_monitor.metrics_history.get('epoch', [])) > 0:
                        # æ£€æŸ¥å…³é”®æ•°ç»„é•¿åº¦æ˜¯å¦åŒ¹é…
                        has_consistent_lengths = True
                        epoch_len = len(performance_monitor.metrics_history['epoch'])
                        for key in ['train_mae', 'val_mae']:
                            if key in performance_monitor.metrics_history and len(performance_monitor.metrics_history[key]) != epoch_len:
                                has_consistent_lengths = False
                                break
                        
                        if has_consistent_lengths:
                            performance_monitor.export_diagnostics()
                    performance_monitor.generate_performance_report()
            
            # è‡ªé€‚åº”è¶…å‚æ•°ä¼˜åŒ–å™¨æ›´æ–°
            if hyperoptimizer is not None and (epoch + 1) % 5 == 0:  # æ¯5ä¸ªepochè°ƒæ•´ä¸€æ¬¡è¶…å‚æ•°
                # æ”¶é›†å½“å‰è®­ç»ƒçŠ¶æ€ä¿¡æ¯
                train_state = {
                    'epoch': epoch,
                    'train_loss': avg_train_loss,
                    'val_loss': avg_val_loss,
                    'train_mae': avg_train_mae,
                    'val_mae': avg_val_mae,
                    'learning_rate': current_lr,
                    'physics_weight': stage_physics_weight,
                    'batch_size': stage_config['æ‰¹æ¬¡å¤§å°']
                }
                
                # è°ƒç”¨è¶…å‚æ•°ä¼˜åŒ–å™¨è°ƒæ•´å‚æ•°
                updated_params = hyperoptimizer.adjust_hyperparameters(
                    train_state=train_state,
                    model=model,
                    optimizer=optimizer,
                    stage_config=stage_config
                )
                
                # æ›´æ–°é˜¶æ®µé…ç½®ä¸­çš„å‚æ•°
                if 'learning_rate' in updated_params:
                    for param_group in optimizer.param_groups:
                        param_group['lr'] = updated_params['learning_rate']
                
                if 'batch_size' in updated_params:
                    stage_config['æ‰¹æ¬¡å¤§å°'] = updated_params['batch_size']
                
                if 'physics_weight' in updated_params:
                    stage_physics_weight = updated_params['physics_weight']
                
                if 'dropout_rate' in updated_params and hasattr(model, 'dropout'):
                    model.dropout.p = updated_params['dropout_rate']
            
            # æ—©åœé€»è¾‘
            if avg_val_loss < best_val_loss - min_improvement:
                best_val_loss = avg_val_loss
                best_model_state = copy.deepcopy(model.state_dict())
                no_improve_count = 0
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                best_checkpoint = {
                    'model_state_dict': best_model_state,
                    'normalizer': normalizer,
                    'train_history': train_history + stage_train_losses,
                    'val_history': val_history + stage_val_losses,
                    'best_val_loss': best_val_loss,
                    'best_epoch': len(train_history) + epoch,
                    'last_stage': stage_idx
                }
                torch.save(best_checkpoint, f"{save_dir}/best_model.pth")
            else:
                no_improve_count += 1
            
            # å­¦ä¹ ç‡è°ƒåº¦
            if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):
                scheduler.step(avg_val_loss)
            else:
                scheduler.step()
            
            # è®¡ç®—æ‰¹æ¬¡å¤„ç†æ—¶é—´
            epoch_time = time.time() - batch_start_time
            batches_per_sec = num_train_batches / epoch_time if epoch_time > 0 else 0
            
            # æ‰“å°è¿›åº¦
            current_lr = optimizer.param_groups[0]['lr']
            if epoch % 10 == 0 or epoch == stage_config['è½®æ¬¡'] - 1:
                if physics_enabled and X_phys is not None:
                    print(f"   Epoch {epoch:3d}/{stage_config['è½®æ¬¡']} | "
                          f"Train: {avg_train_loss:.6f} (æ•°æ®: {avg_train_data_loss:.6f}, ç‰©ç†: {avg_train_physics_loss:.6f}) | "
                          f"Val: {avg_val_loss:.6f} (æ•°æ®: {avg_val_data_loss:.6f}, ç‰©ç†: {avg_val_physics_loss:.6f}) | "
                          f"LR: {current_lr:.2e}, ç‰©ç†æƒé‡: {stage_physics_weight:.3f} | "
                          f"é€Ÿåº¦: {batches_per_sec:.1f}æ‰¹/ç§’")
                else:
                    print(f"   Epoch {epoch:3d}/{stage_config['è½®æ¬¡']} | "
                          f"Train: {avg_train_loss:.6f} (MAE: {avg_train_mae:.6f}) | "
                          f"Val: {avg_val_loss:.6f} (MAE: {avg_val_mae:.6f}) | "
                          f"LR: {current_lr:.2e} | "
                          f"é€Ÿåº¦: {batches_per_sec:.1f}æ‰¹/ç§’")
            
            # æ—©åœæ£€æŸ¥
            if early_stopping and no_improve_count >= patience:
                print(f"âš ï¸  æ—©åœè§¦å‘: éªŒè¯æŸå¤± {patience} è½®æœªæ”¹å–„")
                break
        
        # ä¿å­˜é˜¶æ®µç»“æœ
        stage_info = {
            'stage': stage_config['åç§°'],
            'train_losses': stage_train_losses,
            'val_losses': stage_val_losses,
            'final_train_loss': stage_train_losses[-1],
            'final_val_loss': stage_val_losses[-1],
            'duration': time.time() - stage_start_time
        }
        
        train_history.extend(stage_train_losses)
        val_history.extend(stage_val_losses)
        
        # ä¿å­˜é˜¶æ®µæ£€æŸ¥ç‚¹
        checkpoint = {
            'stage': stage_config['åç§°'],
            'epoch': stage_config['è½®æ¬¡'],
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'train_loss': stage_train_losses[-1],
            'val_loss': stage_val_losses[-1],
            'normalizer': normalizer,
            'config': stage_config,
            'train_history': train_history,
            'val_history': val_history,
            'last_stage': stage_idx,
            'ç‰©ç†çº¦æŸå¯ç”¨': physics_enabled,
            'ç‰©ç†çº¦æŸæƒé‡': stage_physics_weight
        }
        torch.save(checkpoint, f"{save_dir}/stage_{stage_idx+1}_{stage_config['åç§°'].replace(' ', '_')}.pth")
        print(f"âœ… é˜¶æ®µ {stage_idx+1} æ£€æŸ¥ç‚¹å·²ä¿å­˜: stage_{stage_idx+1}_{stage_config['åç§°'].replace(' ', '_')}.pth")
        try:
            from scripts.generate_constraint_report import compute_constraint_stats
            import json, os
            rep = compute_constraint_stats(model, X_train, y_train, X_phys, device)
            out_dir = os.path.join(save_dir, 'consistency_data')
            os.makedirs(out_dir, exist_ok=True)
            with open(os.path.join(out_dir, f'constraint_diagnostics_stage_{stage_idx+1}.json'), 'w', encoding='utf-8') as f:
                json.dump(rep, f, indent=2, ensure_ascii=False)
            try:
                from scripts.visualize_constraint_report import plot_residual_stats, plot_weight_series
                plot_residual_stats(rep, out_dir)
                plot_weight_series(rep, out_dir)
            except Exception:
                pass
        except Exception:
            pass
        
        print(f"âœ… {stage_config['åç§°']} å®Œæˆ: è®­ç»ƒæŸå¤±={stage_train_losses[-1]:.6f}, éªŒè¯æŸå¤±={stage_val_losses[-1]:.6f}")
        print(f"   é˜¶æ®µç”¨æ—¶: {time.time() - stage_start_time:.2f}ç§’")
    
    # æ¢å¤æœ€ä½³æ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if restore_best and best_model_state is not None:
        model.load_state_dict(best_model_state)
        print(f"ğŸ”„ æ¢å¤æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯æŸå¤±: {best_val_loss:.6f}")
    
    # æœ€ç»ˆæµ‹è¯•
    print(f"\nğŸ§ª æœ€ç»ˆæµ‹è¯•...")
    model.eval()
    test_loss = 0.0
    test_data_loss = 0.0
    test_physics_loss = 0.0
    test_mae = 0.0
    
    with torch.no_grad():
        for batch_features, batch_labels in torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False):
            batch_features = batch_features.to(device)
            batch_labels = batch_labels.to(device)
            
            # æµ‹è¯•æ—¶ä¹Ÿå¯ä»¥è®¡ç®—ç‰©ç†çº¦æŸæŸå¤±
            if physics_enabled and X_phys is not None:
                phys_test_indices = torch.randperm(len(X_phys))[:len(batch_features)]
                X_phys_test_batch = X_phys[phys_test_indices].to(device)
            
            with torch.cuda.amp.autocast(enabled=scaler is not None):
                predictions = model(batch_features)
                
                # é›†æˆç‰©ç†çº¦æŸçš„æŸå¤±è®¡ç®—
                if physics_enabled and X_phys is not None:
                    # è·å–æœ€åä¸€ä¸ªé˜¶æ®µçš„ç‰©ç†æƒé‡
                    last_stage_physics_weight = training_stages[-1].get('ç‰©ç†çº¦æŸæƒé‡', 
                                                                      config['ç‰©ç†çº¦æŸ'].get('åˆå§‹æƒé‡', 0.1))
                    
                    # è®¡ç®—æ•°æ®æŸå¤±
                    data_test_loss = loss_stabilizer.compute_loss(predictions, batch_labels)
                    
                    # è®¡ç®—ç‰©ç†çº¦æŸæŸå¤±ï¼ˆåœ¨ç‰©ç†ç‚¹ä¸Šï¼‰
                    phys_test_outputs = model(X_phys_test_batch)
                    physics_test_loss, _ = pinn_layer.compute_physics_loss(
                        X_phys_test_batch, phys_test_outputs
                    )
                    
                    # ç»„åˆæŸå¤±
                    total_test_loss = data_test_loss + last_stage_physics_weight * physics_test_loss
                else:
                    # ä»…æ•°æ®æŸå¤±
                    total_test_loss = loss_stabilizer.compute_loss(predictions, batch_labels)
                    data_test_loss = total_test_loss
                    physics_test_loss = torch.tensor(0.0, device=device)
                    
                test_batch_mae = torch.mean(torch.abs(predictions - batch_labels))
            
            test_loss += total_test_loss.item() * len(batch_features)
            if physics_enabled and X_phys is not None:
                    test_data_loss += data_test_loss.item() * len(batch_features)
                    last_stage_physics_weight = training_stages[-1].get('ç‰©ç†çº¦æŸæƒé‡', 
                                                                       config['ç‰©ç†çº¦æŸ'].get('åˆå§‹æƒé‡', 0.1))
                    # æ£€æŸ¥physics_test_lossæ˜¯å¦ä¸ºå¼ é‡ï¼Œå¦‚æœæ˜¯åˆ™è°ƒç”¨item()ï¼Œå¦åˆ™ç›´æ¥ä½¿ç”¨
                    if hasattr(physics_test_loss, 'item'):
                        test_physics_loss += physics_test_loss.item() * last_stage_physics_weight * len(batch_features)
                    else:
                        test_physics_loss += physics_test_loss * last_stage_physics_weight * len(batch_features)
            test_mae += test_batch_mae.item() * len(batch_features)
    
    # è®¡ç®—å¹³å‡æµ‹è¯•æŒ‡æ ‡
    avg_test_loss = test_loss / len(X_test)
    avg_test_mae = test_mae / len(X_test)
    
    # è®¡ç®—å¹³å‡ç‰©ç†æŸå¤±ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    avg_test_data_loss = 0
    avg_test_physics_loss = 0
    if physics_enabled and X_phys is not None:
        avg_test_data_loss = test_data_loss / len(X_test)
        avg_test_physics_loss = test_physics_loss / len(X_test)
    
    print(f"ğŸ“Š æœ€ç»ˆæµ‹è¯•ç»“æœ:")
    if physics_enabled and X_phys is not None:
        print(f"   æµ‹è¯•æŸå¤±: {avg_test_loss:.6f} (æ•°æ®: {avg_test_data_loss:.6f}, ç‰©ç†: {avg_test_physics_loss:.6f})")
    else:
        print(f"   æµ‹è¯•æŸå¤±: {avg_test_loss:.6f}")
    print(f"   æµ‹è¯•MAE: {avg_test_mae:.6f}")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_checkpoint = {
        'model_state_dict': model.state_dict(),
        'normalizer': normalizer,
        'train_history': train_history,
        'val_history': val_history,
        'best_val_loss': best_val_loss,
        'final_train_loss': train_history[-1],
        'final_val_loss': val_history[-1],
        'test_loss': avg_test_loss,
        'test_mae': avg_test_mae,
        'test_data_loss': avg_test_data_loss,
        'test_physics_loss': avg_test_physics_loss,
        'training_stages': training_stages,
        'config': config,
        'ç‰©ç†çº¦æŸå¯ç”¨': physics_enabled,
        'ç‰©ç†çº¦æŸæƒé‡ç³»åˆ—': [stage.get('ç‰©ç†çº¦æŸæƒé‡', config['ç‰©ç†çº¦æŸ'].get('åˆå§‹æƒé‡', 0.1)) for stage in training_stages],
        'æ¨¡å‹ç‰ˆæœ¬': model.model_info['version'],
        'æ¨¡å‹åˆå§‹åŒ–ç§å­': model_init_seed,
        'è®­ç»ƒå®Œæˆæ—¶é—´': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    }
    
    # æ·»åŠ è¶…å‚æ•°ä¼˜åŒ–å†å²è®°å½•
    if 'hyperoptimizer' in locals() and hyperoptimizer is not None:
        final_checkpoint['hyperparameter_optimization_history'] = hyperoptimizer.get_optimization_history()
        final_checkpoint['best_hyperparameters'] = hyperoptimizer.get_best_hyperparameters()
    
    # ä¿å­˜æœ€ç»ˆæ£€æŸ¥ç‚¹
    final_checkpoint_path = os.path.join(save_dir, "final_optimized_model.pth")
    torch.save(final_checkpoint, final_checkpoint_path)
    print(f"âœ… æœ€ç»ˆæ¨¡å‹æ£€æŸ¥ç‚¹å·²ä¿å­˜: {final_checkpoint_path}")
    try:
        from scripts.generate_constraint_report import compute_constraint_stats
        import json, os
        rep = compute_constraint_stats(model, X_train, y_train, X_phys, device)
        out_dir = os.path.join(save_dir, 'consistency_data')
        os.makedirs(out_dir, exist_ok=True)
        with open(os.path.join(out_dir, f'constraint_diagnostics_final.json'), 'w', encoding='utf-8') as f:
            json.dump(rep, f, indent=2, ensure_ascii=False)
        try:
            from scripts.visualize_constraint_report import plot_residual_stats, plot_weight_series
            plot_residual_stats(rep, out_dir)
            plot_weight_series(rep, out_dir)
        except Exception:
            pass
    except Exception:
        pass
    
    # ç”Ÿæˆå¹¶ä¿å­˜æœ€ç»ˆæ€§èƒ½æŠ¥å‘Šï¼ˆå¦‚æœå¯ç”¨äº†æ€§èƒ½ç›‘æ§ï¼‰
    if enable_performance_monitor and performance_monitor is not None:
        # ç›´æ¥å°†æµ‹è¯•æŒ‡æ ‡æ·»åŠ åˆ°metrics_historyå­—å…¸ä¸­
        performance_monitor.metrics_history['test_loss'] = [avg_test_loss]
        performance_monitor.metrics_history['test_mae'] = [avg_test_mae]
        if physics_enabled:
            performance_monitor.metrics_history['test_data_loss'] = [avg_test_data_loss]
            performance_monitor.metrics_history['test_physics_loss'] = [avg_test_physics_loss]
        # è¿½åŠ  CSV æŒ‡æ ‡æ—¥å¿—
        try:
            import csv
            csv_path = os.path.join(save_dir, 'metrics_summary.csv')
            headers = ['metric', 'value']
            rows = [
                ['test_loss', avg_test_loss],
                ['test_mae', avg_test_mae],
                ['test_data_loss', avg_test_data_loss],
                ['test_physics_loss', avg_test_physics_loss]
            ]
            with open(csv_path, 'w', newline='') as f:
                w = csv.writer(f)
                w.writerow(headers)
                w.writerows(rows)
        except Exception:
            pass
        # ä½¿ç”¨å®é™…å­˜åœ¨çš„æ–¹æ³•ç”ŸæˆæŠ¥å‘Šå’Œè¯Šæ–­
        performance_monitor.generate_performance_report()
        performance_monitor.export_diagnostics()
        
        print(f"ğŸ“Š æ¨¡å‹æ€§èƒ½ç›‘æ§æŠ¥å‘Šå·²ç”Ÿæˆå¹¶ä¿å­˜")
    
    # ä½¿ç”¨save_modelå‡½æ•°ä¿å­˜æ¨¡å‹
    metadata = {
        'training_stages': training_stages,
        'physics_enabled': physics_enabled,
        'best_val_loss': best_val_loss,
        'test_metrics': {
            'loss': avg_test_loss,
            'mae': avg_test_mae,
            'data_loss': avg_test_data_loss,
            'physics_loss': avg_test_physics_loss
        },
        'model_init_seed': model_init_seed,
        'training_timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    }
    try:
        unit_meta = validate_units(X_train.cpu().numpy())
        metadata['unit_checks'] = unit_meta.get('unit_checks', [])
    except Exception:
        metadata['unit_checks'] = []
    
    # æ·»åŠ è¶…å‚æ•°ä¼˜åŒ–ä¿¡æ¯åˆ°metadata
    if 'hyperoptimizer' in locals() and hyperoptimizer is not None:
        metadata['hyperparameter_optimization'] = {
            'enabled': True,
            'best_hyperparameters': hyperoptimizer.get_best_hyperparameters(),
            'optimization_rounds': len(hyperoptimizer.get_optimization_history())
        }
    else:
        metadata['hyperparameter_optimization'] = {'enabled': False}
    
    # æ·»åŠ é¢å¤–çš„è®­ç»ƒä¿¡æ¯
    metadata.update({
        'æ€»è½®æ¬¡': sum(stage['è½®æ¬¡'] for stage in training_stages),
        'æœ€ä½³éªŒè¯æŸå¤±': best_val_loss,
        'æœ€ç»ˆæµ‹è¯•æŸå¤±': avg_test_loss,
        'æœ€ç»ˆæµ‹è¯•MAE': avg_test_mae,
        'æ—¶é—´æˆ³': datetime.now().strftime("%Y%m%d_%H%M%S"),
        'æ¨¡å‹ç‰ˆæœ¬': model.model_info['version'],
        'ç‰©ç†çº¦æŸå¯ç”¨': physics_enabled,
        'PINNé›†æˆ': physics_enabled,
        'è®­ç»ƒé˜¶æ®µæ•°': len(training_stages),
        'æ¨¡å‹åˆå§‹åŒ–ç§å­': model_init_seed
    })
    
    # ä½¿ç”¨save_modelå‡½æ•°ä¿å­˜æ¨¡å‹
    save_model(
        model=model,
        normalizer=normalizer,
        save_path=os.path.join(save_dir, "optimized_model_with_physics.pth"),
        config=config,
        metadata=metadata,
        export_onnx=config.get('å¯¼å‡ºONNX', False),
        onnx_path=os.path.join(save_dir, "optimized_model_with_physics.onnx")
    )
    
    # ç”Ÿæˆå¹¶ä¿å­˜è®­ç»ƒæ›²çº¿å›¾
    plot_training_curves(train_history, val_history, save_dir)
    
    print(f"\nğŸ è®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ“ æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {save_dir}")
    print(f"ğŸ“Š æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
    print(f"ğŸ“ˆ ç‰©ç†çº¦æŸé›†æˆ: {'âœ… å·²å¯ç”¨' if physics_enabled else 'âŒ æœªå¯ç”¨'}")
    if model_init_seed is not None:
        print(f"ğŸ² æ¨¡å‹åˆå§‹åŒ–ç§å­: {model_init_seed} (é€‚ç”¨äºé›†æˆå­¦ä¹ )")
    if physics_enabled:
        print(f"âš–ï¸  ç‰©ç†çº¦æŸæƒé‡ç³»åˆ—: {[stage.get('ç‰©ç†çº¦æŸæƒé‡', config['ç‰©ç†çº¦æŸ'].get('åˆå§‹æƒé‡', 0.1)) for stage in training_stages]}")
    
    return model, normalizer, config
    torch.save(final_checkpoint, f"{save_dir}/final_optimized_model.pth")
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plot_training_curves(train_history, val_history, save_dir)
    
    # è®¡ç®—æ€»ä½“è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
    total_epochs = len(train_history)
    total_duration = sum(stage_info.get('duration', 0) for stage_info in locals().get('stage_info', []))
    loss_improvement = (train_history[0] - train_history[-1]) / train_history[0] * 100 if train_history else 0
    
    print(f"\nğŸ‰ ä¼˜åŒ–è®­ç»ƒå®Œæˆ!")
    print(f"ğŸ’¾ æ¨¡å‹ä¿å­˜è‡³: {save_dir}")
    print(f"ğŸ“ˆ æ€»è®­ç»ƒè½®æ¬¡: {total_epochs}")
    print(f"â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {total_duration:.2f}ç§’")
    print(f"ğŸ“‰ è®­ç»ƒæŸå¤±æ”¹å–„: {train_history[0]:.6f} â†’ {train_history[-1]:.6f} ({loss_improvement:.1f}%)")
    print(f"ğŸ“‰ éªŒè¯æŸå¤±æ”¹å–„: {val_history[0]:.6f} â†’ {val_history[-1]:.6f}")
    print(f"ğŸ” æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
    
    return model, normalizer, final_checkpoint

def create_optimizer(model, stage_config):
    """
    åˆ›å»ºä¼˜åŒ–å™¨ï¼Œæ”¯æŒå¤šç§ä¼˜åŒ–å™¨ç±»å‹
    """
    optimizer_type = stage_config.get('ä¼˜åŒ–å™¨', 'AdamW').lower()
    lr = stage_config.get('å­¦ä¹ ç‡', 1e-4)
    weight_decay = stage_config.get('æƒé‡è¡°å‡', 1e-5)
    
    if optimizer_type == 'adam':
        return optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    elif optimizer_type == 'adamw':
        return optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
    elif optimizer_type == 'sgd':
        momentum = stage_config.get('åŠ¨é‡', 0.9)
        return optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)
    elif optimizer_type == 'radam':
        # å°è¯•å¯¼å…¥RAdamï¼Œå¦‚æœä¸å¯ç”¨åˆ™å›é€€åˆ°AdamW
        try:
            from torch_optimizer import RAdam
            return RAdam(model.parameters(), lr=lr, weight_decay=weight_decay)
        except ImportError:
            print("âš ï¸  RAdamä¸å¯ç”¨ï¼Œä½¿ç”¨AdamWä»£æ›¿")
            return optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
    else:
        # é»˜è®¤ä½¿ç”¨AdamW
        print(f"âš ï¸  æœªçŸ¥ä¼˜åŒ–å™¨ç±»å‹: {optimizer_type}ï¼Œä½¿ç”¨AdamWä»£æ›¿")
        return optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)

def create_lr_scheduler(optimizer, stage_config):
    """
    åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œæ”¯æŒå¤šç§è°ƒåº¦ç­–ç•¥
    """
    scheduler_type = stage_config.get('è°ƒåº¦ç­–ç•¥', 'CosineAnnealing').lower()
    epochs = stage_config.get('è½®æ¬¡', 100)
    base_lr = stage_config.get('å­¦ä¹ ç‡', 1e-4)
    
    if scheduler_type == 'cosineannealing':
        T_max = stage_config.get('è°ƒåº¦å‚æ•°', {}).get('T_max', epochs)
        return optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max)
    elif scheduler_type == 'reducelronplateau':
        patience = stage_config.get('è°ƒåº¦å‚æ•°', {}).get('patience', 5)
        factor = stage_config.get('è°ƒåº¦å‚æ•°', {}).get('factor', 0.5)
        min_lr = stage_config.get('è°ƒåº¦å‚æ•°', {}).get('min_lr', base_lr * 0.01)
        return optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', patience=patience, factor=factor, min_lr=min_lr
        )
    elif scheduler_type == 'onecycle':
        max_lr = stage_config.get('è°ƒåº¦å‚æ•°', {}).get('max_lr', base_lr * 10)
        return optim.lr_scheduler.OneCycleLR(
            optimizer, max_lr=max_lr, total_steps=epochs
        )
    elif scheduler_type == 'lineardecay':
        # çº¿æ€§è¡°å‡åˆ°åŸºç¡€å­¦ä¹ ç‡çš„10%
        final_lr = base_lr * 0.1
        return optim.lr_scheduler.LinearLR(
            optimizer, start_factor=1.0, end_factor=final_lr/base_lr, total_iters=epochs
        )
    else:
        # é»˜è®¤ä½¿ç”¨CosineAnnealing
        print(f"âš ï¸  æœªçŸ¥è°ƒåº¦ç­–ç•¥: {scheduler_type}ï¼Œä½¿ç”¨CosineAnnealingä»£æ›¿")
        return optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)

def plot_training_curves(train_losses, val_losses, save_dir):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
    plt.figure(figsize=(12, 4))
    
    plt.subplot(1, 2, 1)
    plt.plot(train_losses, label='è®­ç»ƒæŸå¤±', alpha=0.8)
    plt.plot(val_losses, label='éªŒè¯æŸå¤±', alpha=0.8)
    plt.xlabel('è®­ç»ƒè½®æ¬¡')
    plt.ylabel('æŸå¤±å€¼')
    plt.title('è®­ç»ƒè¿‡ç¨‹ - æŸå¤±æ›²çº¿')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.subplot(1, 2, 2)
    # åªæ˜¾ç¤ºå50%çš„æ•°æ®ï¼Œæ›´æ¸…æ¥šåœ°çœ‹åˆ°æ”¶æ•›è¿‡ç¨‹
    mid_point = len(train_losses) // 2
    plt.plot(range(mid_point, len(train_losses)), train_losses[mid_point:], label='è®­ç»ƒæŸå¤±', alpha=0.8)
    plt.plot(range(mid_point, len(val_losses)), val_losses[mid_point:], label='éªŒè¯æŸå¤±', alpha=0.8)
    plt.xlabel('è®­ç»ƒè½®æ¬¡')
    plt.ylabel('æŸå¤±å€¼')
    plt.title('è®­ç»ƒè¿‡ç¨‹ - åæœŸæ”¶æ•›')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(f"{save_dir}/training_curves.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"ğŸ“Š è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {save_dir}/training_curves.png")

def quantize_model(model, quantized_path, calibration_dataset=None, dynamic_quantization=True):
    """
    æ¨¡å‹é‡åŒ–åŠŸèƒ½ï¼Œå‡å°æ¨¡å‹ä½“ç§¯å’ŒåŠ é€Ÿæ¨ç†
    
    å‚æ•°:
    - model: åŸå§‹æ¨¡å‹
    - quantized_path: é‡åŒ–åä¿å­˜è·¯å¾„
    - calibration_dataset: æ ¡å‡†æ•°æ®é›†ï¼ˆåŠ¨æ€é‡åŒ–ä¸éœ€è¦ï¼‰
    - dynamic_quantization: æ˜¯å¦ä½¿ç”¨åŠ¨æ€é‡åŒ–
    """
    try:
        print(f"âš¡ å¼€å§‹æ¨¡å‹é‡åŒ–...")
        
        # ç¡®ä¿æ¨¡å‹å¤„äºè¯„ä¼°æ¨¡å¼
        model.eval()
        
        if dynamic_quantization:
            # åŠ¨æ€é‡åŒ–ï¼ˆæ›´ç®€å•ï¼Œé€‚ç”¨èŒƒå›´æ›´å¹¿ï¼‰
            quantized_model = torch.quantization.quantize_dynamic(
                model,
                {nn.Linear, nn.LSTM, nn.GRU},
                dtype=torch.qint8
            )
            print("âœ… åŠ¨æ€é‡åŒ–å®Œæˆ")
        else:
            # é™æ€é‡åŒ–ï¼ˆéœ€è¦æ ¡å‡†ï¼Œç²¾åº¦æ›´é«˜ï¼‰
            if calibration_dataset is None:
                raise ValueError("é™æ€é‡åŒ–éœ€è¦æä¾›æ ¡å‡†æ•°æ®é›†")
            
            # å‡†å¤‡é‡åŒ–é…ç½®
            model.qconfig = torch.quantization.get_default_qconfig('fbgemm')
            torch.quantization.prepare(model, inplace=True)
            
            # æ ¡å‡†
            print("ğŸ” æ­£åœ¨è¿›è¡Œæ¨¡å‹æ ¡å‡†...")
            with torch.no_grad():
                for data in calibration_dataset:
                    if isinstance(data, tuple):
                        model(data[0].to(model.device))
                    else:
                        model(data.to(model.device))
            
            # æ‰§è¡Œé‡åŒ–
            quantized_model = torch.quantization.convert(model, inplace=False)
            print("âœ… é™æ€é‡åŒ–å®Œæˆ")
        
        # ä¿å­˜é‡åŒ–æ¨¡å‹
        torch.save(quantized_model.state_dict(), quantized_path)
        
        # è®¡ç®—æ¨¡å‹å¤§å°å‡å°æ¯”ä¾‹
        original_size = os.path.getsize(quantized_path.replace('.quantized.pth', '.pth')) / (1024 * 1024)
        quantized_size = os.path.getsize(quantized_path) / (1024 * 1024)
        size_reduction = (1 - quantized_size / original_size) * 100
        
        print(f"âœ… é‡åŒ–æ¨¡å‹å·²ä¿å­˜è‡³: {quantized_path}")
        print(f"ğŸ“‰ æ¨¡å‹å¤§å°: {original_size:.2f}MB â†’ {quantized_size:.2f}MB (-{size_reduction:.1f}%)")
        
        return quantized_model
    except Exception as e:
        print(f"âŒ æ¨¡å‹é‡åŒ–å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

def compare_model_performance(model1, model2, test_data, test_labels, device='cpu'):
    """
    æ¯”è¾ƒä¸¤ä¸ªæ¨¡å‹çš„æ€§èƒ½ï¼ˆåŸå§‹æ¨¡å‹å’Œé‡åŒ–æ¨¡å‹ï¼‰
    """
    try:
        print("âš–ï¸  æ¯”è¾ƒæ¨¡å‹æ€§èƒ½...")
        
        # å‡†å¤‡æ•°æ®
        test_data = test_data.to(device)
        test_labels = test_labels.to(device)
        
        # æ€§èƒ½è¯„ä¼°å‡½æ•°
        def evaluate_model(model):
            model.eval()
            with torch.no_grad():
                start_time = time.time()
                predictions = model(test_data)
                inference_time = time.time() - start_time
                
                # è®¡ç®—æŸå¤±å’Œç²¾åº¦
                mse_loss = nn.MSELoss()(predictions, test_labels).item()
                mae_loss = nn.L1Loss()(predictions, test_labels).item()
                
                return {
                    'inference_time': inference_time,
                    'mse_loss': mse_loss,
                    'mae_loss': mae_loss
                }
        
        # è¯„ä¼°ä¸¤ä¸ªæ¨¡å‹
        perf1 = evaluate_model(model1)
        perf2 = evaluate_model(model2)
        
        # æ‰“å°æ¯”è¾ƒç»“æœ
        print("ğŸ“Š æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ:")
        print(f"  åŸå§‹æ¨¡å‹ - æ¨ç†æ—¶é—´: {perf1['inference_time']:.4f}ç§’, MSE: {perf1['mse_loss']:.6f}, MAE: {perf1['mae_loss']:.6f}")
        print(f"  ä¼˜åŒ–æ¨¡å‹ - æ¨ç†æ—¶é—´: {perf2['inference_time']:.4f}ç§’, MSE: {perf2['mse_loss']:.6f}, MAE: {perf2['mae_loss']:.6f}")
        
        # è®¡ç®—æ”¹è¿›æ¯”ä¾‹
        speedup = perf1['inference_time'] / perf2['inference_time'] if perf2['inference_time'] > 0 else float('inf')
        mse_diff = (perf1['mse_loss'] - perf2['mse_loss']) / perf1['mse_loss'] * 100 if perf1['mse_loss'] > 0 else 0
        mae_diff = (perf1['mae_loss'] - perf2['mae_loss']) / perf1['mae_loss'] * 100 if perf1['mae_loss'] > 0 else 0
        
        print(f"ğŸ“ˆ æ€§èƒ½æ”¹è¿›:")
        print(f"  æ¨ç†é€Ÿåº¦æå‡: {speedup:.2f}x")
        print(f"  MSEå˜åŒ–: {mse_diff:+.1f}%")
        print(f"  MAEå˜åŒ–: {mae_diff:+.1f}%")
        
        return perf1, perf2
    except Exception as e:
        print(f"âŒ æ¨¡å‹æ€§èƒ½æ¯”è¾ƒå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return None, None

def create_model_summary(model, input_size=(1, 62), device='cpu'):
    """
    åˆ›å»ºæ¨¡å‹ç»“æ„æ‘˜è¦
    """
    try:
        # å‡†å¤‡è¾“å…¥
        input_tensor = torch.randn(*input_size).to(device)
        
        # æ‰“å°æ¨¡å‹ç»“æ„
        print("ğŸ“‹ æ¨¡å‹ç»“æ„æ‘˜è¦:")
        print(model)
        
        # è®¡ç®—å‚æ•°é‡
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        print(f"ğŸ“Š å‚æ•°é‡ç»Ÿè®¡:")
        print(f"  æ€»å‚æ•°é‡: {total_params:,}")
        print(f"  å¯è®­ç»ƒå‚æ•°é‡: {trainable_params:,}")
        
        # å‰å‘ä¼ æ’­ä»¥æ˜¾ç¤ºè¾“å‡ºå½¢çŠ¶
        with torch.no_grad():
            output = model(input_tensor)
        
        print(f"ğŸ“ è¾“å…¥å½¢çŠ¶: {input_tensor.shape}")
        print(f"ğŸ“ è¾“å‡ºå½¢çŠ¶: {output.shape}")
        
        return {
            'total_params': total_params,
            'trainable_params': trainable_params,
            'input_shape': input_tensor.shape,
            'output_shape': output.shape
        }
    except Exception as e:
        print(f"âŒ åˆ›å»ºæ¨¡å‹æ‘˜è¦å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

# å¢å¼ºçš„æ¨¡å‹ä¿å­˜åŠŸèƒ½
def save_model(model, normalizer, save_path, config=None, metadata=None, export_onnx=False, onnx_path=None):
    """
    å¢å¼ºçš„æ¨¡å‹ä¿å­˜åŠŸèƒ½
    
    å‚æ•°:
    - model: è¦ä¿å­˜çš„æ¨¡å‹
    - normalizer: æ•°æ®æ ‡å‡†åŒ–å™¨
    - save_path: ä¿å­˜è·¯å¾„
    - config: è®­ç»ƒé…ç½®
    - metadata: é¢å¤–å…ƒæ•°æ®
    - export_onnx: æ˜¯å¦å¯¼å‡ºä¸ºONNXæ ¼å¼
    - onnx_path: ONNXæ–‡ä»¶ä¿å­˜è·¯å¾„
    """
    try:
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        # æ„å»ºä¿å­˜å­—å…¸
        save_dict = {
            'model_state_dict': model.state_dict(),
            'model_info': getattr(model, 'model_info', {'version': 'unknown'}),
            'normalizer': normalizer,
            'save_time': datetime.now().isoformat(),
            'torch_version': torch.__version__
        }
        
        # æ·»åŠ é…ç½®å’Œå…ƒæ•°æ®
        if config is not None:
            save_dict['config'] = config
        if metadata is not None:
            save_dict['metadata'] = metadata
        
        # ä¿å­˜æ¨¡å‹
        torch.save(save_dict, save_path)
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜è‡³: {save_path}")
        
        # å¯¼å‡ºä¸ºONNXæ ¼å¼
        if export_onnx and HAS_ONNX:
            try:
                if onnx_path is None:
                    onnx_path = save_path.replace('.pth', '.onnx')
                
                # å‡†å¤‡ç¤ºä¾‹è¾“å…¥
                dummy_input = torch.randn(1, model.model_info['input_dim']).to(model.device)
                
                # å¯¼å‡ºä¸ºONNX
                torch.onnx.export(
                    model,
                    dummy_input,
                    onnx_path,
                    export_params=True,
                    opset_version=11,
                    do_constant_folding=True,
                    input_names=['input'],
                    output_names=['output'],
                    dynamic_axes={'input': {0: 'batch_size'},
                                 'output': {0: 'batch_size'}},
                    verbose=False
                )
                
                # éªŒè¯ONNXæ¨¡å‹
                onnx_model = onnx.load(onnx_path)
                onnx.checker.check_model(onnx_model)
                print(f"âœ… æ¨¡å‹å·²å¯¼å‡ºä¸ºONNXæ ¼å¼è‡³: {onnx_path}")
                
            except Exception as e:
                print(f"âš ï¸  ONNXå¯¼å‡ºå¤±è´¥: {str(e)}")
        
        return True
    except Exception as e:
        print(f"âŒ ä¿å­˜æ¨¡å‹å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

# å¢å¼ºçš„æ¨¡å‹åŠ è½½åŠŸèƒ½
def load_model(load_path, device='cpu'):
    """
    å¢å¼ºçš„æ¨¡å‹åŠ è½½åŠŸèƒ½ï¼ŒåŒ…å«ç‰ˆæœ¬æ£€æŸ¥å’Œå…¼å®¹æ€§éªŒè¯
    
    å‚æ•°:
    - load_path: åŠ è½½è·¯å¾„
    - device: è®¾å¤‡
    
    è¿”å›:
    - model: åŠ è½½çš„æ¨¡å‹
    - normalizer: åŠ è½½çš„æ ‡å‡†åŒ–å™¨
    - metadata: å…ƒæ•°æ®å­—å…¸
    """
    try:
        print(f"ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹: {load_path}")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(load_path):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {load_path}")
        
        # åŠ è½½æ¨¡å‹
        checkpoint = torch.load(load_path, map_location=device)
        
        # æå–æ¨¡å‹ä¿¡æ¯
        model_info = checkpoint.get('model_info', {})
        print(f"ğŸ“‹ æ¨¡å‹ä¿¡æ¯: ç‰ˆæœ¬={model_info.get('version', 'unknown')}, "
              f"æ¶æ„={model_info.get('architecture', 'unknown')}")
        
        # å…¼å®¹æ€§æ£€æŸ¥
        check_model_compatibility(model_info)
        
        # é‡å»ºæ¨¡å‹
        model = OptimizedEWPINN(
            input_dim=model_info.get('input_dim', 62),
            output_dim=model_info.get('output_dim', 24),
            hidden_layers=model_info.get('hidden_layers', [128, 64, 32]),
            dropout_rate=model_info.get('dropout_rate', 0.1),
            activation=model_info.get('activation', 'ReLU'),
            batch_norm=model_info.get('batch_norm', True),
            device=device
        )
        
        # åŠ è½½æ¨¡å‹æƒé‡
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()
        
        # æå–æ ‡å‡†åŒ–å™¨
        normalizer = checkpoint.get('normalizer', None)
        if normalizer is None:
            print("âš ï¸  æ¨¡å‹ä¸­æœªæ‰¾åˆ°æ ‡å‡†åŒ–å™¨")
        
        # æå–é¢å¤–ä¿¡æ¯
        metadata = {
            'save_time': checkpoint.get('save_time', 'unknown'),
            'torch_version': checkpoint.get('torch_version', 'unknown'),
            'config': checkpoint.get('config', None),
            'metadata': checkpoint.get('metadata', None)
        }
        
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        return model, normalizer, metadata
        
    except Exception as e:
        print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return None, None, None

def check_model_compatibility(model_info):
    """
    æ£€æŸ¥æ¨¡å‹å…¼å®¹æ€§
    """
    current_version = '1.0.0'
    model_version = model_info.get('version', 'unknown')
    
    # ç®€å•ç‰ˆæœ¬å…¼å®¹æ€§æ£€æŸ¥
    if model_version != current_version:
        warnings.warn(
            f"æ¨¡å‹ç‰ˆæœ¬ä¸åŒ¹é…: åŠ è½½çš„æ¨¡å‹ç‰ˆæœ¬={model_version}, å½“å‰ç‰ˆæœ¬={current_version}"
        )
    
    # æ¶æ„æ£€æŸ¥
    architecture = model_info.get('architecture', 'unknown')
    if architecture != 'EWPINN':
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹æ¶æ„: {architecture}")
    
    print(f"âœ… æ¨¡å‹å…¼å®¹æ€§æ£€æŸ¥é€šè¿‡")

def load_and_test_model(load_path, test_data=None, test_labels=None, device='cpu'):
    """
    åŠ è½½å¹¶æµ‹è¯•ä¿å­˜çš„æ¨¡å‹
    
    å‚æ•°:
    - load_path: æ¨¡å‹åŠ è½½è·¯å¾„
    - test_data: æµ‹è¯•æ•°æ®ï¼ˆå¯é€‰ï¼‰
    - test_labels: æµ‹è¯•æ ‡ç­¾ï¼ˆå¯é€‰ï¼‰
    - device: è®¾å¤‡
    """
    # åŠ è½½æ¨¡å‹
    model, normalizer, metadata = load_model(load_path, device=device)
    
    if model is None:
        return None
    
    # åˆ›å»ºæ¨¡å‹æ‘˜è¦
    create_model_summary(model, input_size=(1, model.model_info['input_dim']), device=device)
    
    # å¦‚æœæä¾›äº†æµ‹è¯•æ•°æ®ï¼Œåˆ™è¿›è¡Œæµ‹è¯•
    if test_data is not None and test_labels is not None:
        print("\nğŸ§ª è¿›è¡Œæ¨¡å‹æµ‹è¯•...")
        
        # ç¡®ä¿æ•°æ®æ ¼å¼æ­£ç¡®
        if isinstance(test_data, np.ndarray):
            test_data = torch.tensor(test_data, dtype=torch.float32).to(device)
        if isinstance(test_labels, np.ndarray):
            test_labels = torch.tensor(test_labels, dtype=torch.float32).to(device)
        
        # å¦‚æœæœ‰æ ‡å‡†åŒ–å™¨ï¼Œä½¿ç”¨å®ƒ
        if normalizer is not None:
            test_data = normalizer.transform_features(test_data)
            test_labels = normalizer.transform_labels(test_labels)
        
        # æµ‹è¯•æ¨¡å‹
        model.eval()
        with torch.no_grad():
            predictions = model(test_data)
            
            # è®¡ç®—æŒ‡æ ‡
            mse_loss = nn.MSELoss()(predictions, test_labels).item()
            mae_loss = nn.L1Loss()(predictions, test_labels).item()
            
            print(f"ğŸ“Š æµ‹è¯•ç»“æœ:")
            print(f"  MSEæŸå¤±: {mse_loss:.6f}")
            print(f"  MAEæŸå¤±: {mae_loss:.6f}")
    
    return model, normalizer, metadata

def parse_arguments():
    """
    è§£æå‘½ä»¤è¡Œå‚æ•°
    """
    import argparse
    parser = argparse.ArgumentParser(description='EWPINNæ¨¡å‹è®­ç»ƒä¸æµ‹è¯•è„šæœ¬')
    
    # æ¨¡å¼é€‰æ‹©
    parser.add_argument('--mode', type=str, default='train', choices=['train', 'test', 'infer'],
                        help='è¿è¡Œæ¨¡å¼: train(è®­ç»ƒ), test(æµ‹è¯•), infer(æ¨ç†)')
    
    # é…ç½®æ–‡ä»¶
    parser.add_argument('--config', type=str, default='model_config.json',
                        help='é…ç½®æ–‡ä»¶è·¯å¾„')
    
    # è®­ç»ƒç›¸å…³å‚æ•°
    parser.add_argument('--resume', action='store_true',
                        help='ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ')
    parser.add_argument('--checkpoint', type=str, default=None,
                        help='æ¢å¤è®­ç»ƒçš„æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--mixed-precision', action='store_true', default=True,
                        help='å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ')
    parser.add_argument('--model-seed', type=int, default=None,
                        help='æ¨¡å‹åˆå§‹åŒ–ç§å­ï¼Œç”¨äºé›†æˆå­¦ä¹ æ—¶åˆ›å»ºä¸åŒåˆå§‹æƒé‡çš„æ¨¡å‹')
    parser.add_argument('--efficient-architecture', action='store_true', default=True,
                        help='ä½¿ç”¨é«˜æ•ˆEWPINNæ¶æ„ï¼ˆåŒ…å«æ®‹å·®è¿æ¥å’Œæ³¨æ„åŠ›æœºåˆ¶ï¼‰')
    parser.add_argument('--model-compression', type=float, default=1.0,
                        help='æ¨¡å‹å‹ç¼©å› å­ï¼Œå°äº1.0å°†å‡å°‘ç½‘ç»œå‚æ•°æ•°é‡ï¼ˆé»˜è®¤ä¸º1.0ï¼Œä¸å‹ç¼©ï¼‰'),
    
    # æµ‹è¯•/æ¨ç†ç›¸å…³å‚æ•°
    parser.add_argument('--model-path', type=str, default='models/best_model.pth',
                        help='æµ‹è¯•/æ¨ç†ä½¿ç”¨çš„æ¨¡å‹è·¯å¾„')
    parser.add_argument('--export-onnx', action='store_true',
                        help='å¯¼å‡ºæ¨¡å‹ä¸ºONNXæ ¼å¼')
    
    # æ•°æ®ç›¸å…³å‚æ•°
    parser.add_argument('--num-samples', type=int, default=200,
                        help='ç”Ÿæˆçš„æ ·æœ¬æ•°é‡')
    parser.add_argument('--data-augmentation', action='store_true', default=True,
                        help='å¯ç”¨æ•°æ®å¢å¼º')
    
    # è¾“å‡ºç›¸å…³å‚æ•°
    parser.add_argument('--output-dir', type=str, default='outputs',
                        help='è¾“å‡ºç›®å½•')
    parser.add_argument('--device', type=str, default=None,
                        help='è¿è¡Œè®¾å¤‡ (cpu, cuda, cuda:0 ç­‰)')
    
    return parser.parse_args()

def run_test_mode(args):
    """
    è¿è¡Œæµ‹è¯•æ¨¡å¼
    """
    print(f"\nğŸ“Š è¿›å…¥æµ‹è¯•æ¨¡å¼")
    
    # ç¡®å®šè®¾å¤‡
    device = args.device
    if device is None:
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ¨¡å‹
    model, normalizer, metadata = load_model(args.model_path, device=device)
    if model is None:
        print(f"âŒ æ— æ³•åŠ è½½æ¨¡å‹: {args.model_path}")
        return False
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    print(f"ğŸ”„ ç”Ÿæˆæµ‹è¯•æ•°æ® ({args.num_samples} æ ·æœ¬)")
    X_test, y_test = generate_realistic_data(
        model, 
        num_samples=args.num_samples,
        config_path=args.config,
        data_augmentation=args.data_augmentation
    )
    
    # æ•°æ®æ ‡å‡†åŒ–
    X_test = torch.tensor(X_test, dtype=torch.float32).to(device)
    y_test = torch.tensor(y_test, dtype=torch.float32).to(device)
    
    if normalizer is not None:
        X_test = normalizer.transform_features(X_test)
        y_test = normalizer.transform_labels(y_test)
    
    # æµ‹è¯•æ¨¡å‹
    print(f"ğŸ§ª æµ‹è¯•æ¨¡å‹æ€§èƒ½...")
    test_results = load_and_test_model(
        args.model_path, 
        test_data=X_test,
        test_labels=y_test,
        device=device
    )
    
    # å¯¼å‡ºONNXï¼ˆå¦‚æœéœ€è¦ï¼‰
    if args.export_onnx and HAS_ONNX:
        onnx_path = args.model_path.replace('.pth', '.onnx')
        success = save_model(
            model, 
            normalizer, 
            args.model_path,
            export_onnx=True,
            onnx_path=onnx_path
        )
        if success:
            print(f"âœ… æ¨¡å‹å·²å¯¼å‡ºä¸ºONNXæ ¼å¼")
    
    return True

def run_infer_mode(args):
    """
    è¿è¡Œæ¨ç†æ¨¡å¼
    """
    print(f"\nğŸ¤– è¿›å…¥æ¨ç†æ¨¡å¼")
    
    # ç¡®å®šè®¾å¤‡
    device = args.device
    if device is None:
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ¨¡å‹
    model, normalizer, metadata = load_model(args.model_path, device=device)
    if model is None:
        print(f"âŒ æ— æ³•åŠ è½½æ¨¡å‹: {args.model_path}")
        return False
    
    print(f"ğŸ” æ¨¡å‹ä¿¡æ¯:")
    for key, value in model.model_info.items():
        print(f"  - {key}: {value}")
    
    # ç¤ºä¾‹æ¨ç†
    print(f"\nğŸ§ª æ‰§è¡Œç¤ºä¾‹æ¨ç†...")
    # åˆ›å»ºéšæœºè¾“å…¥ï¼ˆåœ¨å®é™…åº”ç”¨ä¸­åº”æ›¿æ¢ä¸ºçœŸå®è¾“å…¥ï¼‰
    sample_input = torch.randn(1, model.input_dim).to(device)
    
    if normalizer is not None:
        sample_input = normalizer.transform_features(sample_input)
    
    model.eval()
    with torch.no_grad():
        prediction = model(sample_input)
    
    if normalizer is not None:
        prediction = normalizer.inverse_transform_labels(prediction)
    
    print(f"ğŸ“Š æ¨ç†ç»“æœ:")
    print(f"  è¾“å…¥ç‰¹å¾ç»´åº¦: {sample_input.shape}")
    print(f"  é¢„æµ‹è¾“å‡ºç»´åº¦: {prediction.shape}")
    print(f"  é¢„æµ‹ç¤ºä¾‹å€¼: {prediction[0, :5].cpu().numpy()}")  # æ˜¾ç¤ºå‰5ä¸ªè¾“å‡º
    
    return True

def main():
    """
    ä¸»å‡½æ•° - å®Œæ•´çš„è®­ç»ƒä¸æµ‹è¯•è„šæœ¬å…¥å£
    """
    try:
        # æ‰“å°æ¬¢è¿ä¿¡æ¯
        print("\nğŸš€ EWPINN æ¨¡å‹è®­ç»ƒä¸æµ‹è¯•ç³»ç»Ÿ")
        print("=========================================")
        print(f"ğŸ•’ å¯åŠ¨æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ“± PyTorch ç‰ˆæœ¬: {torch.__version__}")
        print(f"ğŸ“Š CUDA å¯ç”¨: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"   GPU åç§°: {torch.cuda.get_device_name(0)}")
        print(f"ğŸ“¦ ONNX æ”¯æŒ: {HAS_ONNX}")
        print("=========================================")
        
        # è§£æå‘½ä»¤è¡Œå‚æ•°
        args = parse_arguments()
        print(f"\nâš™ï¸  è¿è¡Œé…ç½®:")
        print(f"   æ¨¡å¼: {args.mode}")
        print(f"   é…ç½®æ–‡ä»¶: {args.config}")
        print(f"   è¾“å‡ºç›®å½•: {args.output_dir}")
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(args.output_dir, exist_ok=True)
        
        # æ ¹æ®æ¨¡å¼æ‰§è¡Œä¸åŒåŠŸèƒ½
        if args.mode == 'train':
            print(f"\nğŸ“ˆ è¿›å…¥è®­ç»ƒæ¨¡å¼")
            print(f"   æ¢å¤è®­ç»ƒ: {args.resume}")
            if args.resume and args.checkpoint:
                print(f"   æ£€æŸ¥ç‚¹è·¯å¾„: {args.checkpoint}")
            print(f"   æ··åˆç²¾åº¦è®­ç»ƒ: {args.mixed_precision}")
            print(f"   æ¨¡å‹åˆå§‹åŒ–ç§å­: {args.model_seed if args.model_seed is not None else 'éšæœº'}")
            print(f"   ä½¿ç”¨é«˜æ•ˆæ¶æ„: {args.efficient_architecture}")
            print(f"   æ¨¡å‹å‹ç¼©å› å­: {args.model_compression}")
            
            # æ‰§è¡Œè®­ç»ƒ
            model, normalizer, metadata = progressive_training(
                config_path=args.config,
                resume_training=args.resume,
                resume_checkpoint=args.checkpoint,
                mixed_precision=args.mixed_precision,
                model_init_seed=args.model_seed,
                use_efficient_architecture=args.efficient_architecture,
                model_compression_factor=args.model_compression
            )
            
            print(f"\nâœ… è®­ç»ƒæˆåŠŸå®Œæˆï¼")
            
            # ä¿å­˜æœ€ç»ˆæ¨¡å‹
            final_model_path = os.path.join(args.output_dir, 'final_model.pth')
            save_model(
                model, 
                normalizer, 
                final_model_path,
                config=args.config,
                metadata={
                    'training_completed': datetime.now().isoformat(),
                    'samples_generated': args.num_samples,
                    'mixed_precision': args.mixed_precision
                },
                export_onnx=args.export_onnx
            )
            
        elif args.mode == 'test':
            run_test_mode(args)
            
        elif args.mode == 'infer':
            run_infer_mode(args)
            
        print(f"\nâœ… ä»»åŠ¡å®Œæˆï¼")
        print(f"ğŸ“ æ‰€æœ‰è¾“å‡ºå·²ä¿å­˜è‡³: {args.output_dir}")
        
    except Exception as e:
        print(f"\nâŒ æ‰§è¡Œå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return -1
    
    return 0

if __name__ == '__main__':
    # å…¨å±€å˜é‡æ£€æŸ¥
    HAS_ONNX = False
    try:
        import onnx
        import onnxruntime
        HAS_ONNX = True
        print(f"âœ… ONNXæ”¯æŒå·²å¯ç”¨")
    except ImportError:
        print(f"âš ï¸ ONNXæ”¯æŒæœªå¯ç”¨ï¼Œå°†è·³è¿‡ONNXå¯¼å‡ºåŠŸèƒ½")
    
    # è¿è¡Œä¸»å‡½æ•°
    exit_code = main()
    
    # æ ¹æ®é€€å‡ºç å†³å®šæ˜¯å¦æ˜¾ç¤ºæˆåŠŸä¿¡æ¯
    if exit_code == 0:
        print(f"\nğŸ‰ ç¨‹åºæˆåŠŸæ‰§è¡Œå®Œæˆï¼")
        print(f"\nğŸ“š ä½¿ç”¨è¯´æ˜:")
        print(f"  è®­ç»ƒæ¨¡å‹: python ewp_pinn_optimized_train.py --mode train --config your_config.json")
        print(f"  è®­ç»ƒæŒ‡å®šç§å­çš„æ¨¡å‹: python ewp_pinn_optimized_train.py --mode train --model-seed 42")
        print(f"  ä½¿ç”¨é«˜æ•ˆæ¶æ„: python ewp_pinn_optimized_train.py --mode train --efficient-architecture")
        print(f"  ä½¿ç”¨æ¨¡å‹å‹ç¼©: python ewp_pinn_optimized_train.py --mode train --model-compression 0.8")
        print(f"  æµ‹è¯•æ¨¡å‹: python ewp_pinn_optimized_train.py --mode test --model-path your_model.pth")
        print(f"  æ¨¡å‹æ¨ç†: python ewp_pinn_optimized_train.py --mode infer --model-path your_model.pth")
        print(f"  å¯ç”¨ONNXå¯¼å‡º: python ewp_pinn_optimized_train.py --mode train --export-onnx")
    else:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥ï¼Œé€€å‡ºç : {exit_code}")
