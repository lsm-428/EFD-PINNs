"""
Experiment Result Comparison Tool - For analyzing and comparing multiple training experiment results

Features:
1. Multi-experiment metric comparison analysis
2. Training curve visualization comparison
3. Configuration difference analysis
4. Performance ranking and recommendations
5. Experiment report generation
"""

import json
import os
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from datetime import datetime
from typing import Dict, List, Optional, Any, Tuple
import logging
from pathlib import Path

logger = logging.getLogger(__name__)


class ExperimentComparator:
    """Experiment comparison analyzer"""
    
    def __init__(self, experiments_dir: str = "./experiments/experiments"):
        """
        Initialize experiment comparator
        
        Args:
            experiments_dir: Experiment directory path
        """
        self.experiments_dir = experiments_dir
        # Use path relative to experiment directory
        base_dir = os.path.dirname(experiments_dir) if experiments_dir else "./experiments"
        self.figures_dir = os.path.join(base_dir, "comparison_figures")
        os.makedirs(self.figures_dir, exist_ok=True)
        
        logger.info(f"Experiment comparator initialized, experiment directory: {experiments_dir}")
    
    def load_experiment_data(self, experiment_id: str) -> Optional[Dict[str, Any]]:
        """
        Load experiment data with enhanced directory and file handling
        
        Args:
            experiment_id: å®éªŒIDï¼ˆå¯ä»¥æ˜¯é…ç½®IDå¦‚EXP-xxxæˆ–å®é™…å®éªŒIDå¦‚exp_xxxï¼‰
            
        Returns:
            å®éªŒæ•°æ®å­—å…¸ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›None
        """
        # å°è¯•ç›´æ¥ä½¿ç”¨å®éªŒIDä½œä¸ºç›®å½•å
        experiment_dir = os.path.join(self.experiments_dir, experiment_id)
        
        # æ”¶é›†æ‰€æœ‰å¯èƒ½çš„å®éªŒç›®å½•å€™é€‰
        candidate_dirs = [experiment_dir]
        
        # è·å–æ‰€æœ‰å¯èƒ½çš„å®éªŒç›®å½•å’Œå­ç›®å½•
        all_possible_dirs = []
        
        # é€’å½’æ”¶é›†æ‰€æœ‰å®éªŒç›®å½•ï¼ˆæ”¯æŒæ·±å±‚åµŒå¥—ï¼‰
        def collect_exp_dirs(root_dir, current_depth=0, max_depth=3):
            if current_depth >= max_depth:
                return
                
            try:
                items = os.listdir(root_dir)
                for item in items:
                    item_path = os.path.join(root_dir, item)
                    if os.path.isdir(item_path):
                        # æ”¶é›†æ‰€æœ‰ç›®å½•ï¼Œä¸åªæ˜¯ä»¥exp_å¼€å¤´çš„
                        rel_path = os.path.relpath(item_path, self.experiments_dir)
                        all_possible_dirs.append((rel_path, item_path))
                        # é€’å½’è¿›å…¥å­ç›®å½•
                        collect_exp_dirs(item_path, current_depth + 1)
            except Exception as e:
                logger.debug(f"æ‰«æç›®å½•å¤±è´¥ {root_dir}: {str(e)}")
        
        # å¼€å§‹é€’å½’æ”¶é›†
        collect_exp_dirs(self.experiments_dir)
        logger.info(f"å·²æ”¶é›† {len(all_possible_dirs)} ä¸ªå¯èƒ½çš„å®éªŒç›®å½•")
        
        # ç‰¹æ®Šå¤„ç†EXP-xxxæ ¼å¼çš„é…ç½®ID
        if experiment_id.startswith('EXP-'):
            logger.info(f"æ£€æµ‹åˆ°EXP-xxxæ ¼å¼çš„é…ç½®IDï¼Œå°è¯•æŸ¥æ‰¾åŒ¹é…çš„å®é™…å®éªŒç›®å½•")
            # æ£€æŸ¥æ¯ä¸ªç›®å½•çš„é…ç½®æ–‡ä»¶æ˜¯å¦åŒ…å«æ­¤EXP-xxx ID
            matching_dirs = []
            
            for rel_path, abs_path in all_possible_dirs:
                # æŸ¥æ‰¾å¯èƒ½çš„é…ç½®æ–‡ä»¶
                config_files = []
                try:
                    config_files = [f for f in os.listdir(abs_path) 
                                  if f.endswith('.json') and ('config' in f or 'experiment' in f)]
                except Exception as e:
                    logger.debug(f"è¯»å–ç›®å½•å†…å®¹å¤±è´¥ {abs_path}: {str(e)}")
                    continue
                
                for config_file in config_files:
                    config_path = os.path.join(abs_path, config_file)
                    try:
                        with open(config_path, 'r', encoding='utf-8') as f:
                            config_data = json.load(f)
                            # æ£€æŸ¥é…ç½®ä¸­æ˜¯å¦åŒ…å«æ­¤EXP ID
                            if (config_data.get('id') == experiment_id or 
                                config_data.get('experiment_id') == experiment_id or
                                str(config_data).find(experiment_id) != -1):
                                matching_dirs.append((rel_path, abs_path))
                                logger.info(f"åœ¨ç›®å½• {abs_path} çš„é…ç½®æ–‡ä»¶ä¸­æ‰¾åˆ°EXP ID: {experiment_id}")
                                break
                    except Exception as e:
                        logger.debug(f"è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥ {config_path}: {str(e)}")
                        continue
            
            if matching_dirs:
                # æŒ‰ç›®å½•åç§°æ’åºï¼Œé€‰æ‹©æœ€æ–°çš„
                matching_dirs.sort(key=lambda x: x[0], reverse=True)
                for rel_path, abs_path in matching_dirs:
                    candidate_dirs.append(abs_path)
                    logger.info(f"æ·»åŠ åŒ¹é…çš„å®éªŒç›®å½•: {abs_path}")
        # å¤„ç†æ™®é€šå®éªŒIDï¼Œç‰¹åˆ«æ˜¯å¤„ç†å¤šæ—¶é—´æˆ³æ ¼å¼
        else:
            # é’ˆå¯¹å¤šæ—¶é—´æˆ³ç›®å½•åçš„ç‰¹æ®Šå¤„ç†
            # å¦‚æœIDæ˜¯åƒ exp_20251126_205654 è¿™æ ·çš„æ ¼å¼ï¼Œæˆ‘ä»¬ä¹Ÿè¦åŒ¹é…åŒ…å«å®ƒçš„æ›´é•¿ç›®å½•å
            matching_dirs = []
            for rel_path, abs_path in all_possible_dirs:
                # åŒ¹é…è§„åˆ™ï¼š
                # 1. ç›®å½•åå®Œå…¨åŒ¹é…
                # 2. ç›®å½•åä»¥è¯¥IDå¼€å¤´å¹¶è·Ÿç€ä¸‹åˆ’çº¿å’Œå…¶ä»–å­—ç¬¦ï¼ˆå¤„ç†å¤šæ—¶é—´æˆ³æƒ…å†µï¼‰
                # 3. ç›®å½•è·¯å¾„ä¸­åŒ…å«è¯¥IDä½œä¸ºéƒ¨åˆ†åç§°
                dir_name = os.path.basename(abs_path)
                if (dir_name == experiment_id or 
                    dir_name.startswith(f"{experiment_id}_") or 
                    experiment_id in rel_path):
                    matching_dirs.append((rel_path, abs_path))
                    logger.debug(f"æ½œåœ¨åŒ¹é…ç›®å½•: {abs_path}")
            
            if matching_dirs:
                # æŒ‰ç›®å½•åç§°æ’åºï¼Œé€‰æ‹©æœ€æ–°çš„
                matching_dirs.sort(key=lambda x: x[0], reverse=True)
                for rel_path, abs_path in matching_dirs:
                    candidate_dirs.append(abs_path)
                    logger.info(f"æ·»åŠ å¯èƒ½åŒ¹é…çš„å®éªŒç›®å½•: {abs_path}")
        
        # ç¡®ä¿candidate_dirsä¸­çš„ç›®å½•è·¯å¾„æ˜¯å”¯ä¸€çš„
        candidate_dirs = list(set(candidate_dirs))
        logger.info(f"æœ€ç»ˆå€™é€‰ç›®å½•æ•°é‡: {len(candidate_dirs)}")
        
        # å°è¯•åœ¨æ‰€æœ‰å€™é€‰ç›®å½•ä¸­æŸ¥æ‰¾é…ç½®å’ŒæŒ‡æ ‡æ–‡ä»¶
        found_config = None
        found_metrics = None
        final_dir = None
        
        for candidate_dir in candidate_dirs:
            # é¦–å…ˆæ£€æŸ¥åµŒå¥—å­ç›®å½•æƒ…å†µ
            nested_dirs = []
            try:
                if os.path.exists(candidate_dir):
                    nested_dirs = [os.path.join(candidate_dir, d) 
                                 for d in os.listdir(candidate_dir) 
                                 if os.path.isdir(os.path.join(candidate_dir, d)) 
                                 and d.startswith('exp_')]
            except Exception as e:
                logger.debug(f"æ£€æŸ¥åµŒå¥—å­ç›®å½•å¤±è´¥ {candidate_dir}: {str(e)}")
            
            # å°†åµŒå¥—ç›®å½•æ·»åŠ åˆ°æ£€æŸ¥åˆ—è¡¨
            check_dirs = [candidate_dir] + nested_dirs
            
            for check_dir in check_dirs:
                # æŸ¥æ‰¾é…ç½®æ–‡ä»¶
                if found_config is None:
                    config_path = os.path.join(check_dir, "config.json")
                    alt_config_path = os.path.join(check_dir, "experiment_config.json")
                    
                    if os.path.exists(config_path):
                        try:
                            with open(config_path, 'r', encoding='utf-8') as f:
                                found_config = json.load(f)
                            final_dir = check_dir
                            logger.info(f"æ‰¾åˆ°é…ç½®æ–‡ä»¶: {config_path}")
                        except Exception as e:
                            logger.error(f"è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥ {config_path}: {str(e)}")
                    elif os.path.exists(alt_config_path):
                        try:
                            with open(alt_config_path, 'r', encoding='utf-8') as f:
                                found_config = json.load(f)
                            final_dir = check_dir
                            logger.info(f"æ‰¾åˆ°å¤‡é€‰é…ç½®æ–‡ä»¶: {alt_config_path}")
                        except Exception as e:
                            logger.error(f"è¯»å–å¤‡é€‰é…ç½®æ–‡ä»¶å¤±è´¥ {alt_config_path}: {str(e)}")
                
                # æŸ¥æ‰¾æŒ‡æ ‡æ–‡ä»¶
                if found_metrics is None:
                    # æ£€æŸ¥å¤šä¸ªå¯èƒ½çš„æŒ‡æ ‡æ–‡ä»¶è·¯å¾„
                    metrics_candidates = [
                        os.path.join(check_dir, "logs", "reports", "training_metrics.json"),
                        os.path.join(check_dir, "logs", "training_metrics.json"),
                        os.path.join(check_dir, "reports", "training_metrics.json"),
                        os.path.join(check_dir, "training_metrics.json"),
                        os.path.join(check_dir, "training_history.json")  # å¤‡é€‰æ ¼å¼
                    ]
                    
                    for metrics_path in metrics_candidates:
                        if os.path.exists(metrics_path):
                            try:
                                with open(metrics_path, 'r', encoding='utf-8') as f:
                                    found_metrics = json.load(f)
                                logger.info(f"æ‰¾åˆ°è®­ç»ƒæŒ‡æ ‡æ–‡ä»¶: {metrics_path}")
                                # æ›´æ–°æœ€ç»ˆç›®å½•
                                final_dir = check_dir
                                break
                            except Exception as e:
                                logger.error(f"è¯»å–æŒ‡æ ‡æ–‡ä»¶å¤±è´¥ {metrics_path}: {str(e)}")
                
                # å¦‚æœä¸¤è€…éƒ½æ‰¾åˆ°ï¼Œè·³å‡ºå¾ªç¯
                if found_config and found_metrics:
                    break
            
            if found_config and found_metrics:
                break
        
        # å¦‚æœæ‰¾ä¸åˆ°é…ç½®ï¼Œå°è¯•ä½¿ç”¨ä¸€ä¸ªç®€å•çš„é»˜è®¤é…ç½®
        if not found_config:
            logger.warning(f"æœªæ‰¾åˆ°æœ‰æ•ˆçš„é…ç½®æ–‡ä»¶ï¼Œåˆ›å»ºé»˜è®¤é…ç½®")
            found_config = {"metadata": {"description": f"Experiment {experiment_id}"}}
        
        # å¦‚æœæ‰¾ä¸åˆ°æŒ‡æ ‡ï¼Œè¿”å›ç©ºæ•°æ®
        if not found_metrics:
            logger.warning(f"æœªæ‰¾åˆ°æœ‰æ•ˆçš„è®­ç»ƒæŒ‡æ ‡æ–‡ä»¶: {experiment_id}")
            # å°è¯•ç›´æ¥åˆ›å»ºä¸€ä¸ªæœ€å°çš„è®­ç»ƒå†å²æ•°æ®ç»“æ„ï¼Œé¿å…æ˜¾ç¤ºinf
            training_history = {
                "epoch": [0],
                "train_loss": [0.0],
                "val_loss": [0.0],
                "physics_loss": [0.0],
                "learning_rate": [0.001],
                "physics_weight": [1.0],
                "timestamp": [datetime.now().isoformat()]
            }
        else:
            # è§£æè®­ç»ƒå†å²
            training_history = self._parse_training_history(found_metrics)
        
        return {
            "experiment_id": experiment_id,
            "config": found_config,
            "training_history": training_history,
            "final_metrics": self._get_final_metrics(training_history),
            "metadata": found_config.get("metadata", {})
        }
    
    def _parse_training_history(self, metrics_data: Dict[str, Any]) -> Dict[str, List]:
        """è§£æè®­ç»ƒå†å²æ•°æ®ï¼Œæ”¯æŒå­—å…¸å’Œåˆ—è¡¨ä¸¤ç§æ ¼å¼"""
        if not metrics_data:
            return {}
        
        history = {
            "epoch": [],
            "train_loss": [],
            "val_loss": [],
            "physics_loss": [],
            "learning_rate": [],
            "physics_weight": [],
            "timestamp": []
        }
        
        sorted_timestamps = sorted(metrics_data.keys()) if isinstance(metrics_data, dict) else []
        
        for timestamp in sorted_timestamps:
            metrics = metrics_data[timestamp]
            
            # å¤„ç†å­—å…¸æ ¼å¼
            if isinstance(metrics, dict):
                history["epoch"].append(metrics.get("epoch", 0))
                history["train_loss"].append(metrics.get("train_loss", float('inf')))
                history["val_loss"].append(metrics.get("val_loss", float('inf')))
                history["physics_loss"].append(metrics.get("physics_loss", float('inf')))
                history["learning_rate"].append(metrics.get("learning_rate", 0))
                history["physics_weight"].append(metrics.get("physics_weight", 0))
                history["timestamp"].append(timestamp)
            # å¤„ç†åˆ—è¡¨æ ¼å¼ï¼ˆåˆ—è¡¨ä¸­çš„æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªepochçš„metricsï¼‰
            elif isinstance(metrics, list):
                for i, epoch_metrics in enumerate(metrics):
                    if isinstance(epoch_metrics, dict):
                        history["epoch"].append(epoch_metrics.get("epoch", i + 1))
                        history["train_loss"].append(epoch_metrics.get("train_loss", float('inf')))
                        history["val_loss"].append(epoch_metrics.get("val_loss", float('inf')))
                        history["physics_loss"].append(epoch_metrics.get("physics_loss", float('inf')))
                        history["learning_rate"].append(epoch_metrics.get("learning_rate", 0))
                        history["physics_weight"].append(epoch_metrics.get("physics_weight", 0))
                        history["timestamp"].append(f"{timestamp}_{i}")
                    else:
                        # å¦‚æœåˆ—è¡¨å…ƒç´ ä¸æ˜¯å­—å…¸ï¼Œä½¿ç”¨é»˜è®¤å€¼
                        history["epoch"].append(i + 1)
                        history["train_loss"].append(float('inf'))
                        history["val_loss"].append(float('inf'))
                        history["physics_loss"].append(float('inf'))
                        history["learning_rate"].append(0)
                        history["physics_weight"].append(0)
                        history["timestamp"].append(f"{timestamp}_{i}")
            # å…¶ä»–æ ¼å¼ï¼Œä½¿ç”¨é»˜è®¤å€¼
            else:
                history["epoch"].append(0)
                history["train_loss"].append(float('inf'))
                history["val_loss"].append(float('inf'))
                history["physics_loss"].append(float('inf'))
                history["learning_rate"].append(0)
                history["physics_weight"].append(0)
                history["timestamp"].append(timestamp)
        
        return history
    
    def _get_final_metrics(self, training_history: Dict[str, List]) -> Dict[str, float]:
        """è·å–æœ€ç»ˆè®­ç»ƒæŒ‡æ ‡"""
        if not training_history or not training_history["epoch"]:
            return {}
        
        final_epoch = len(training_history["epoch"]) - 1
        
        return {
            "final_train_loss": training_history["train_loss"][final_epoch],
            "final_val_loss": training_history["val_loss"][final_epoch],
            "final_physics_loss": training_history["physics_loss"][final_epoch],
            "final_learning_rate": training_history["learning_rate"][final_epoch],
            "final_physics_weight": training_history["physics_weight"][final_epoch],
            "total_epochs": len(training_history["epoch"])
        }
    
    def compare_experiments(self, experiment_ids: List[str]) -> Dict[str, Any]:
        """
        æ¯”è¾ƒå¤šä¸ªå®éªŒ
        
        å‚æ•°:
            experiment_ids: å®éªŒIDåˆ—è¡¨
            
        è¿”å›:
            æ¯”è¾ƒç»“æœå­—å…¸
        """
        if not experiment_ids:
            logger.warning("æ²¡æœ‰æä¾›å®éªŒID")
            return {}
        
        # åŠ è½½æ‰€æœ‰å®éªŒæ•°æ®
        experiments_data = {}
        valid_experiments = []
        
        for exp_id in experiment_ids:
            data = self.load_experiment_data(exp_id)
            if data:
                experiments_data[exp_id] = data
                valid_experiments.append(exp_id)
            else:
                logger.warning(f"æ— æ³•åŠ è½½å®éªŒæ•°æ®: {exp_id}")
        
        if not valid_experiments:
            logger.error("æ²¡æœ‰æœ‰æ•ˆçš„å®éªŒæ•°æ®")
            return {}
        
        # ç”Ÿæˆå¯¹æ¯”åˆ†æ
        comparison = {
            "experiments": experiments_data,
            "summary": self._generate_summary(experiments_data),
            "config_comparison": self._compare_configs(experiments_data),
            "performance_ranking": self._rank_experiments(experiments_data),
            "recommendations": self._generate_recommendations(experiments_data)
        }
        
        # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        self._generate_comparison_plots(experiments_data)
        
        return comparison
    
    def _generate_summary(self, experiments_data: Dict[str, Dict]) -> Dict[str, Any]:
        """ç”Ÿæˆå®éªŒæ‘˜è¦"""
        summary = {
            "total_experiments": len(experiments_data),
            "experiment_ids": list(experiments_data.keys()),
            "date_range": self._get_date_range(experiments_data),
            "total_training_epochs": sum(data["final_metrics"].get("total_epochs", 0) 
                                      for data in experiments_data.values()),
            "best_val_loss": float('inf'),
            "best_experiment": None
        }
        
        # æ‰¾åˆ°æœ€ä½³éªŒè¯æŸå¤±
        for exp_id, data in experiments_data.items():
            val_loss = data["final_metrics"].get("final_val_loss", float('inf'))
            if val_loss < summary["best_val_loss"]:
                summary["best_val_loss"] = val_loss
                summary["best_experiment"] = exp_id
        
        return summary
    
    def _get_date_range(self, experiments_data: Dict[str, Dict]) -> Tuple[str, str]:
        """è·å–å®éªŒæ—¥æœŸèŒƒå›´"""
        dates = []
        for data in experiments_data.values():
            created_at = data["metadata"].get("created_at", "")
            if created_at:
                try:
                    date_obj = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
                    dates.append(date_obj)
                except ValueError:
                    continue
        
        if dates:
            min_date = min(dates).strftime("%Y-%m-%d %H:%M")
            max_date = max(dates).strftime("%Y-%m-%d %H:%M")
            return min_date, max_date
        
        return "æœªçŸ¥", "æœªçŸ¥"
    
    def _compare_configs(self, experiments_data: Dict[str, Dict]) -> Dict[str, Any]:
        """æ¯”è¾ƒå®éªŒé…ç½®"""
        config_comparison = {
            "model_configs": {},
            "training_configs": {},
            "differences": []
        }
        
        # æ”¶é›†æ‰€æœ‰é…ç½®
        all_configs = {}
        for exp_id, data in experiments_data.items():
            all_configs[exp_id] = data["config"]
        
        # æ¯”è¾ƒæ¨¡å‹é…ç½®
        model_keys = ["input_dim", "output_dim", "hidden_layers", "activation"]
        for key in model_keys:
            values = {}
            for exp_id, config in all_configs.items():
                model_config = config.get("model", {})
                values[exp_id] = model_config.get(key, "æœªè®¾ç½®")
            config_comparison["model_configs"][key] = values
        
        # æ¯”è¾ƒè®­ç»ƒé…ç½®
        training_keys = ["epochs", "batch_size", "learning_rate", "optimizer"]
        for key in training_keys:
            values = {}
            for exp_id, config in all_configs.items():
                training_config = config.get("training", {})
                values[exp_id] = training_config.get(key, "æœªè®¾ç½®")
            config_comparison["training_configs"][key] = values
        
        # è¯†åˆ«é…ç½®å·®å¼‚
        self._identify_config_differences(config_comparison, all_configs)
        
        return config_comparison
    
    def _identify_config_differences(self, config_comparison: Dict[str, Any], 
                                    all_configs: Dict[str, Dict]):
        """è¯†åˆ«é…ç½®å·®å¼‚"""
        differences = []
        
        # æ£€æŸ¥æ¨¡å‹é…ç½®å·®å¼‚
        for key, values in config_comparison["model_configs"].items():
            # å¤„ç†ä¸å¯å“ˆå¸Œçš„å€¼ï¼ˆå¦‚åˆ—è¡¨ï¼‰
            unique_values = set()
            for value in values.values():
                if isinstance(value, (list, dict)):
                    unique_values.add(str(value))
                else:
                    unique_values.add(value)
            
            if len(unique_values) > 1:
                differences.append({
                    "category": "æ¨¡å‹é…ç½®",
                    "parameter": key,
                    "values": values
                })
        
        # æ£€æŸ¥è®­ç»ƒé…ç½®å·®å¼‚
        for key, values in config_comparison["training_configs"].items():
            # å¤„ç†ä¸å¯å“ˆå¸Œçš„å€¼ï¼ˆå¦‚åˆ—è¡¨ï¼‰
            unique_values = set()
            for value in values.values():
                if isinstance(value, (list, dict)):
                    unique_values.add(str(value))
                else:
                    unique_values.add(value)
            
            if len(unique_values) > 1:
                differences.append({
                    "category": "è®­ç»ƒé…ç½®",
                    "parameter": key,
                    "values": values
                })
        
        config_comparison["differences"] = differences
    
    def _rank_experiments(self, experiments_data: Dict[str, Dict]) -> List[Dict[str, Any]]:
        """å®éªŒæ€§èƒ½æ’å"""
        rankings = []
        
        for exp_id, data in experiments_data.items():
            final_metrics = data["final_metrics"]
            
            # è®¡ç®—ç»¼åˆå¾—åˆ†ï¼ˆéªŒè¯æŸå¤±è¶Šä½è¶Šå¥½ï¼‰
            val_loss = final_metrics.get("final_val_loss", float('inf'))
            train_loss = final_metrics.get("final_train_loss", float('inf'))
            physics_loss = final_metrics.get("final_physics_loss", float('inf'))
            
            # ç»¼åˆå¾—åˆ†è®¡ç®—ï¼ˆéªŒè¯æŸå¤±æƒé‡æœ€é«˜ï¼‰
            score = 0.6 * (1 / (val_loss + 1e-8)) + 0.2 * (1 / (train_loss + 1e-8)) + 0.2 * (1 / (physics_loss + 1e-8))
            
            rankings.append({
                "experiment_id": exp_id,
                "description": data["metadata"].get("description", "æ— æè¿°"),
                "final_val_loss": val_loss,
                "final_train_loss": train_loss,
                "final_physics_loss": physics_loss,
                "total_epochs": final_metrics.get("total_epochs", 0),
                "score": score
            })
        
        # æŒ‰å¾—åˆ†æ’åºï¼ˆä»é«˜åˆ°ä½ï¼‰
        rankings.sort(key=lambda x: x["score"], reverse=True)
        
        # æ·»åŠ æ’å
        for i, ranking in enumerate(rankings):
            ranking["rank"] = i + 1
        
        return rankings
    
    def _generate_recommendations(self, experiments_data: Dict[str, Dict]) -> List[str]:
        """ç”Ÿæˆè®­ç»ƒå»ºè®®"""
        recommendations = []
        
        if len(experiments_data) < 2:
            recommendations.append("å»ºè®®è¿è¡Œæ›´å¤šå®éªŒä»¥è·å¾—æœ‰æ„ä¹‰çš„å¯¹æ¯”åˆ†æ")
            return recommendations
        
        # åˆ†ææœ€ä½³å®éªŒçš„ç‰¹å¾
        rankings = self._rank_experiments(experiments_data)
        best_exp_id = rankings[0]["experiment_id"]
        best_config = experiments_data[best_exp_id]["config"]
        
        # ç”ŸæˆåŸºäºæœ€ä½³å®éªŒçš„å»ºè®®
        recommendations.append(f"æœ€ä½³å®éªŒ {best_exp_id} çš„é…ç½®å€¼å¾—å‚è€ƒ")
        
        # åˆ†æå­¦ä¹ ç‡æ¨¡å¼
        lr_analysis = self._analyze_learning_rates(experiments_data)
        if lr_analysis:
            recommendations.append(lr_analysis)
        
        # åˆ†æç‰©ç†æƒé‡æ¨¡å¼
        physics_weight_analysis = self._analyze_physics_weights(experiments_data)
        if physics_weight_analysis:
            recommendations.append(physics_weight_analysis)
        
        return recommendations
    
    def _analyze_learning_rates(self, experiments_data: Dict[str, Dict]) -> Optional[str]:
        """åˆ†æå­¦ä¹ ç‡æ¨¡å¼"""
        final_lrs = []
        for data in experiments_data.values():
            final_lr = data["final_metrics"].get("final_learning_rate", 0)
            final_lrs.append(final_lr)
        
        if len(final_lrs) >= 2:
            avg_lr = np.mean(final_lrs)
            if avg_lr < 1e-5:
                return "å­¦ä¹ ç‡å¯èƒ½è®¾ç½®è¿‡ä½ï¼Œè€ƒè™‘å¢åŠ åˆå§‹å­¦ä¹ ç‡"
            elif avg_lr > 1e-2:
                return "å­¦ä¹ ç‡å¯èƒ½è®¾ç½®è¿‡é«˜ï¼Œè€ƒè™‘å‡å°åˆå§‹å­¦ä¹ ç‡"
        
        return None
    
    def _analyze_physics_weights(self, experiments_data: Dict[str, Dict]) -> Optional[str]:
        """Analyze physics weight patterns"""
        final_weights = []
        for data in experiments_data.values():
            final_weight = data["final_metrics"].get("final_physics_weight", 0)
            final_weights.append(final_weight)
        
        if len(final_weights) >= 2:
            avg_weight = np.mean(final_weights)
            if avg_weight < 0.1:
                return "Physics constraint weight is low, may need to increase physics constraint weight"
            elif avg_weight > 10:
                return "Physics constraint weight is high, may need to decrease physics constraint weight"
        
        return None
    
    def _generate_comparison_plots(self, experiments_data: Dict[str, Dict]):
        """Generate comparison plots"""
        if not experiments_data:
            return
        
        plt.style.use('seaborn-v0_8')
        
        # åˆ›å»ºæŸå¤±å¯¹æ¯”å›¾
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Experiment Comparison Analysis', fontsize=16, fontweight='bold')
        
        # è®­ç»ƒæŸå¤±å¯¹æ¯”
        ax1 = axes[0, 0]
        has_train_data = False
        for exp_id, data in experiments_data.items():
            history = data["training_history"]
            if history and history["epoch"] and "train_loss" in history:
                if len(history["train_loss"]) == len(history["epoch"]):
                    ax1.plot(history["epoch"], history["train_loss"], 
                            label=f'{exp_id}', linewidth=2, alpha=0.8)
                    has_train_data = True
        ax1.set_title('Training Loss Comparison')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Train Loss')
        if has_train_data:
            ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # Validation loss comparison
        ax2 = axes[0, 1]
        has_val_data = False
        for exp_id, data in experiments_data.items():
            history = data["training_history"]
            if history and history["epoch"] and "val_loss" in history:
                if len(history["val_loss"]) == len(history["epoch"]):
                    ax2.plot(history["epoch"], history["val_loss"], 
                            label=f'{exp_id}', linewidth=2, alpha=0.8)
                    has_val_data = True
        ax2.set_title('Validation Loss Comparison')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Val Loss')
        if has_val_data:
            ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # Physics loss comparison
        ax3 = axes[1, 0]
        has_physics_data = False
        for exp_id, data in experiments_data.items():
            history = data["training_history"]
            if history and history["epoch"] and "physics_loss" in history:
                if len(history["physics_loss"]) == len(history["epoch"]):
                    ax3.plot(history["epoch"], history["physics_loss"], 
                            label=f'{exp_id}', linewidth=2, alpha=0.8)
                    has_physics_data = True
        ax3.set_title('Physics Loss Comparison')
        ax3.set_xlabel('Epoch')
        ax3.set_ylabel('Physics Loss')
        if has_physics_data:
            ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # Learning rate comparison
        ax4 = axes[1, 1]
        has_lr_data = False
        for exp_id, data in experiments_data.items():
            history = data["training_history"]
            if history and history["epoch"] and "learning_rate" in history:
                if len(history["learning_rate"]) == len(history["epoch"]):
                    ax4.plot(history["epoch"], history["learning_rate"], 
                            label=f'{exp_id}', linewidth=2, alpha=0.8)
                    has_lr_data = True
        ax4.set_title('Learning Rate Comparison')
        ax4.set_xlabel('Epoch')
        ax4.set_ylabel('Learning Rate')
        if has_lr_data:
            ax4.legend()
        ax4.grid(True, alpha=0.3)
        ax4.set_yscale('log')
        
        plt.tight_layout()
        
        # Save chart
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        plot_path = os.path.join(self.figures_dir, f"comparison_{timestamp}.png")
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"ğŸ“Š Comparison chart saved: {plot_path}")
    
    def generate_comparison_report(self, experiment_ids: List[str], 
                                 output_path: Optional[str] = None) -> str:
        """
        Generate detailed comparison report
        
        Args:
            experiment_ids: List of experiment IDs
            output_path: Output file path
            
        Returns:
            Report file path
        """
        comparison = self.compare_experiments(experiment_ids)
        
        if not comparison:
            logger.error("Failed to generate comparison report")
            return ""
        
        if output_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = os.path.join(self.figures_dir, f"comparison_report_{timestamp}.txt")
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("Experiment Comparison Analysis Report\n")
            f.write("=" * 80 + "\n\n")
            
            # Summary information
            summary = comparison["summary"]
            f.write("ğŸ“‹ Experiment Summary\n")
            f.write(f"   Total experiments: {summary['total_experiments']}\n")
            f.write(f"   Experiment IDs: {', '.join(summary['experiment_ids'])}\n")
            f.write(f"   Date range: {summary['date_range'][0]} to {summary['date_range'][1]}\n")
            f.write(f"   Total training epochs: {summary['total_training_epochs']}\n")
            f.write(f"   Best validation loss: {summary['best_val_loss']:.6f} (Experiment: {summary['best_experiment']})\n\n")
            
            # Performance ranking
            f.write("ğŸ† Performance Ranking\n")
            rankings = comparison["performance_ranking"]
            for rank in rankings:
                f.write(f"   {rank['rank']}. {rank['experiment_id']}: ")
                f.write(f"Val loss={rank['final_val_loss']:.6f}, ")
                f.write(f"Train loss={rank['final_train_loss']:.6f}, ")
                f.write(f"Physics loss={rank['final_physics_loss']:.6f}\n")
            f.write("\n")
            
            # Configuration differences
            f.write("âš™ï¸  Configuration Difference Analysis\n")
            config_comp = comparison["config_comparison"]
            differences = config_comp["differences"]
            
            if differences:
                for diff in differences:
                    f.write(f"   {diff['category']} - {diff['parameter']}:\n")
                    for exp_id, value in diff["values"].items():
                        f.write(f"      {exp_id}: {value}\n")
                    f.write("\n")
            else:
                f.write("   All experiments have identical configurations\n\n")
            
            # Recommendations
            f.write("ğŸ’¡ Training Recommendations\n")
            recommendations = comparison["recommendations"]
            for i, rec in enumerate(recommendations, 1):
                f.write(f"   {i}. {rec}\n")
            
            f.write("\n" + "=" * 80 + "\n")
            f.write("Report generated at: " + datetime.now().strftime("%Y-%m-%d %H:%M:%S") + "\n")
            f.write("=" * 80 + "\n")
        
        logger.info(f"ğŸ“„ Comparison report generated: {output_path}")
        return output_path


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # é…ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    # åˆ›å»ºå®éªŒå¯¹æ¯”å™¨
    comparator = ExperimentComparator()
    
    # è·å–æ‰€æœ‰å®éªŒIDï¼ˆç¤ºä¾‹ï¼‰
    experiments_dir = "./experiments/experiments"
    if os.path.exists(experiments_dir):
        experiment_ids = [d for d in os.listdir(experiments_dir) 
                         if os.path.isdir(os.path.join(experiments_dir, d)) and d.startswith("exp_")]
        
        if experiment_ids:
            # æ¯”è¾ƒå®éªŒ
            comparison = comparator.compare_experiments(experiment_ids[:3])  # æ¯”è¾ƒå‰3ä¸ªå®éªŒ
            
            # ç”ŸæˆæŠ¥å‘Š
            report_path = comparator.generate_comparison_report(experiment_ids[:3])
            
            print(f"âœ… å®éªŒå¯¹æ¯”å®Œæˆï¼æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
        else:
            print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°å®éªŒæ•°æ®ï¼Œè¯·å…ˆè¿è¡Œè®­ç»ƒå®éªŒ")
    else:
        print("âš ï¸  å®éªŒç›®å½•ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œè®­ç»ƒå®éªŒ")