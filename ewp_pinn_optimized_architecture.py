import torch
import torch.nn as nn
import torch.nn.functional as F
import os
import json
from datetime import datetime
import numpy as np

class EfficientResidualLayer(nn.Module):
    """é«˜æ•ˆæ®‹å·®å±‚ - åŒ…å«æ‰¹å½’ä¸€åŒ–å’Œæ¿€æ´»å‡½æ•°çš„æ®‹å·®å—"""
    def __init__(self, in_features, out_features, activation_fn=F.relu, dropout_rate=0.1):
        super(EfficientResidualLayer, self).__init__()
        
        self.linear1 = nn.Linear(in_features, out_features)
        self.bn1 = nn.BatchNorm1d(out_features)
        self.linear2 = nn.Linear(out_features, out_features)
        self.bn2 = nn.BatchNorm1d(out_features)
        self.dropout = nn.Dropout(dropout_rate)
        self.activation_fn = activation_fn
        
        # å¿«æ·è¿æ¥ - å¦‚æœç»´åº¦ä¸åŒ¹é…éœ€è¦çº¿æ€§å˜æ¢
        self.shortcut = nn.Linear(in_features, out_features) if in_features != out_features else nn.Identity()
        
        # åˆå§‹åŒ–æƒé‡
        self._initialize_weights()
    
    def _initialize_weights(self):
        # Heåˆå§‹åŒ–
        nn.init.kaiming_normal_(self.linear1.weight, mode='fan_in', nonlinearity='relu')
        nn.init.kaiming_normal_(self.linear2.weight, mode='fan_in', nonlinearity='relu')
        if hasattr(self.shortcut, 'weight'):
            nn.init.kaiming_normal_(self.shortcut.weight, mode='fan_in', nonlinearity='relu')
        
        # åç½®åˆå§‹åŒ–ä¸ºå°å€¼
        if self.linear1.bias is not None:
            nn.init.constant_(self.linear1.bias, 0.01)
        if self.linear2.bias is not None:
            nn.init.constant_(self.linear2.bias, 0.01)
        if hasattr(self.shortcut, 'bias') and self.shortcut.bias is not None:
            nn.init.constant_(self.shortcut.bias, 0.01)
    
    def forward(self, x):
        residual = self.shortcut(x)
        
        out = self.linear1(x)
        out = self.bn1(out)
        out = self.activation_fn(out)
        out = self.dropout(out)
        
        out = self.linear2(out)
        out = self.bn2(out)
        
        # æ®‹å·®è¿æ¥
        out += residual
        out = self.activation_fn(out)
        
        return out

class AttentionMechanism(nn.Module):
    """ç®€å•çš„é€šé“æ³¨æ„åŠ›æœºåˆ¶ï¼Œç”¨äºå¢å¼ºé‡è¦ç‰¹å¾"""
    def __init__(self, feature_dim):
        super(AttentionMechanism, self).__init__()
        self.fc1 = nn.Linear(feature_dim, feature_dim // 4)
        self.fc2 = nn.Linear(feature_dim // 4, feature_dim)
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x):
        # å…¨å±€å¹³å‡æ± åŒ– - ä¿®å¤ç»´åº¦å¤„ç†
        if x.dim() == 1:
            # ä¸€ç»´è¾“å…¥ [features]
            attention = x.mean().unsqueeze(0).unsqueeze(0)  # è½¬ä¸º [1, 1]
        elif x.dim() == 2:
            # äºŒç»´è¾“å…¥ [batch, features]
            attention = x.mean(dim=0, keepdim=True)  # [1, features]
        else:
            # å¤šç»´è¾“å…¥ï¼Œé»˜è®¤åœ¨æœ€åä¸€ä¸ªç»´åº¦å¹³å‡
            attention = x.mean(dim=-1, keepdim=True)
        
        # æ³¨æ„åŠ›é—¨æ§ - ç¡®ä¿ç»´åº¦åŒ¹é…
        attention = self.fc1(attention)
        attention = F.relu(attention)
        attention = self.fc2(attention)
        attention = self.sigmoid(attention)
        
        # åº”ç”¨æ³¨æ„åŠ›æƒé‡ - ç¡®ä¿å¹¿æ’­æ­£ç¡®
        if x.dim() == 1 and attention.dim() == 2:
            attention = attention.squeeze(0)  # è°ƒæ•´ä¸ºä¸€ç»´
        
        return x * attention

class EfficientEWPINN(nn.Module):
    """é«˜æ•ˆEWPINNæ¨¡å‹ - å¢å¼ºå‹ç¥ç»ç½‘ç»œæ¶æ„ï¼Œæ”¯æŒæ®‹å·®è¿æ¥å’Œæ³¨æ„åŠ›æœºåˆ¶
    ç‰¹æ€§ï¼šæ®‹å·®ç½‘ç»œã€æ³¨æ„åŠ›æœºåˆ¶ã€åŠ¨æ€ç½‘ç»œç»“æ„ã€æ¢¯åº¦ç´¯ç§¯æ”¯æŒã€é‡åŒ–å‹å¥½è®¾è®¡"""
    
    def __init__(self, input_dim=62, output_dim=24, hidden_layers=None, dropout_rate=0.1, 
                 activation='ReLU', batch_norm=True, use_residual=True, use_attention=True,
                 config_path=None, device='cpu', compression_factor=1.0, gradient_checkpointing=False):
        super(EfficientEWPINN, self).__init__()
        
        self.device = device
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.dropout_rate = dropout_rate
        self.batch_norm = batch_norm
        self.use_residual = use_residual
        self.use_attention = use_attention
        self.compression_factor = compression_factor  # ç½‘ç»œå‹ç¼©å› å­
        self.gradient_checkpointing = gradient_checkpointing
        
        # æ¨¡å‹é…ç½®ä¿¡æ¯
        self.model_info = {
            'version': '2.0.0',  # å‡çº§ç‰ˆæœ¬å·
            'input_dim': input_dim,
            'output_dim': output_dim,
            'hidden_layers': hidden_layers if hidden_layers else [128, 64, 32],
            'dropout_rate': dropout_rate,
            'activation': activation,
            'batch_norm': batch_norm,
            'use_residual': use_residual,
            'use_attention': use_attention,
            'compression_factor': compression_factor,
            'architecture': 'EfficientEWPINN',
            'created_at': datetime.now().isoformat()
        }
        
        # ä»é…ç½®æ–‡ä»¶åŠ è½½å‚æ•°ï¼ˆå¦‚æœæä¾›ï¼‰
        if config_path and os.path.exists(config_path):
            self._load_config(config_path)
        
        # é»˜è®¤éšè—å±‚é…ç½®
        if hidden_layers is None:
            hidden_layers = [128, 64, 32]
        
        # åº”ç”¨å‹ç¼©å› å­
        if compression_factor != 1.0:
            hidden_layers = [max(8, int(dim * compression_factor)) for dim in hidden_layers]
        
        self.hidden_layers = hidden_layers
        self.model_info['hidden_layers'] = hidden_layers
        self.model_info['activation'] = activation
        
        # é€‰æ‹©æ¿€æ´»å‡½æ•°
        activation_map = {
            'ReLU': F.relu,
            'LeakyReLU': F.leaky_relu,
            'GELU': F.gelu,
            'SiLU': F.silu
        }
        self.activation_fn = activation_map.get(activation, F.relu)
        
        # æ„å»ºç½‘ç»œ
        self.layers = nn.ModuleList()
        prev_dim = input_dim
        
        # æ„å»ºéšè—å±‚
        for i, hidden_dim in enumerate(hidden_layers):
            if use_residual and i > 0:  # ç¬¬ä¸€ä¸ªå±‚é€šå¸¸ä¸ä½¿ç”¨æ®‹å·®
                # ä½¿ç”¨æ®‹å·®å—
                layer = EfficientResidualLayer(prev_dim, hidden_dim, self.activation_fn, dropout_rate)
            else:
                # æ ‡å‡†å±‚
                layer_components = [nn.Linear(prev_dim, hidden_dim)]
                if batch_norm:
                    layer_components.append(nn.BatchNorm1d(hidden_dim))
                layer_components.append(nn.Dropout(dropout_rate) if dropout_rate > 0 else nn.Identity())
                layer = nn.Sequential(*layer_components)
            
            self.layers.append(layer)
            prev_dim = hidden_dim
        
        # æ³¨æ„åŠ›æœºåˆ¶å±‚ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        self.attention = AttentionMechanism(prev_dim) if use_attention else nn.Identity()
        self.attention_mechanisms = self.attention
        
        # è¾“å‡ºå±‚ - ä½¿ç”¨è¾ƒå°çš„æƒé‡åˆå§‹åŒ–ï¼Œæœ‰åŠ©äºç¨³å®šè®­ç»ƒ
        self.output_layer = nn.Linear(prev_dim, output_dim)
        
        # åˆå§‹åŒ–æƒé‡
        self._initialize_weights()
        
        # å¦‚æœå¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œåº”ç”¨åˆ°ç›¸åº”çš„å±‚
        if gradient_checkpointing:
            self._apply_gradient_checkpointing()
        
        # å°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
        self.to(self.device)

        self.residual_layers = [layer for layer in self.layers if isinstance(layer, EfficientResidualLayer)]
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        self._print_model_info(activation)
    
    def _load_config(self, config_path):
        """ä»JSONé…ç½®æ–‡ä»¶åŠ è½½æ¨¡å‹å‚æ•°"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
                
            if 'æ¨¡å‹é…ç½®' in config:
                model_config = config['æ¨¡å‹é…ç½®']
                if 'è¾“å…¥ç»´åº¦' in model_config:
                    self.input_dim = model_config['è¾“å…¥ç»´åº¦']
                if 'è¾“å‡ºç»´åº¦' in model_config:
                    self.output_dim = model_config['è¾“å‡ºç»´åº¦']
                if 'ç½‘ç»œæ¶æ„' in model_config:
                    net_config = model_config['ç½‘ç»œæ¶æ„']
                    if 'éšè—å±‚' in net_config:
                        self.hidden_layers = net_config['éšè—å±‚']
                    if 'Dropout' in net_config:
                        self.dropout_rate = net_config['Dropout']
                    if 'æ®‹å·®è¿æ¥' in net_config:
                        self.use_residual = net_config['æ®‹å·®è¿æ¥']
                    if 'æ³¨æ„åŠ›æœºåˆ¶' in net_config:
                        self.use_attention = net_config['æ³¨æ„åŠ›æœºåˆ¶']
                    if 'å‹ç¼©å› å­' in net_config:
                        self.compression_factor = net_config['å‹ç¼©å› å­']
                print(f"âœ… æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
        except Exception as e:
            print(f"âš ï¸  åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {str(e)}")
            print("   å°†ä½¿ç”¨é»˜è®¤é…ç½®")
    
    def _initialize_weights(self):
        """é«˜çº§æƒé‡åˆå§‹åŒ–æ–¹æ³•"""
        # è¾“å‡ºå±‚ä½¿ç”¨è¾ƒå°çš„æƒé‡åˆå§‹åŒ–
        nn.init.normal_(self.output_layer.weight, mean=0.0, std=0.01)
        if self.output_layer.bias is not None:
            nn.init.constant_(self.output_layer.bias, 0.0)
    
    def _apply_gradient_checkpointing(self):
        """åº”ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥å‡å°‘å†…å­˜ä½¿ç”¨"""
        for layer in self.layers:
            if hasattr(layer, 'gradient_checkpointing_enable'):
                layer.gradient_checkpointing_enable()
    
    def _print_model_info(self, activation):
        """æ‰“å°æ¨¡å‹æ¶æ„ä¿¡æ¯"""
        print(f"ğŸš€ é«˜æ•ˆEWPINNæ¨¡å‹å·²åˆå§‹åŒ– - è®¾å¤‡: {self.device}")
        print(f"   è¾“å…¥ç»´åº¦: {self.input_dim}, è¾“å‡ºç»´åº¦: {self.output_dim}")
        print(f"   æ¿€æ´»å‡½æ•°: {activation}")
        print(f"   æ‰¹é‡æ ‡å‡†åŒ–: {'å¯ç”¨' if self.batch_norm else 'ç¦ç”¨'}")
        print(f"   Dropoutç‡: {self.dropout_rate}")
        print(f"   æ®‹å·®è¿æ¥: {'å¯ç”¨' if self.use_residual else 'ç¦ç”¨'}")
        print(f"   æ³¨æ„åŠ›æœºåˆ¶: {'å¯ç”¨' if self.use_attention else 'ç¦ç”¨'}")
        print(f"   ç½‘ç»œå‹ç¼©å› å­: {self.compression_factor}")
        
        # æ‰“å°ç½‘ç»œç»“æ„
        structure_str = f"{self.input_dim}"
        for dim in self.hidden_layers:
            structure_str += f" -> {dim}"
        structure_str += f" -> {self.output_dim}"
        print(f"   ç½‘ç»œç»“æ„: {structure_str}")
        
        # è®¡ç®—å‚æ•°æ•°é‡
        param_count = sum(p.numel() for p in self.parameters())
        print(f"   å‚æ•°æ•°é‡: {param_count:,}")
        
        # è®¡ç®—ç†è®ºFLOPsï¼ˆç®€åŒ–è®¡ç®—ï¼‰
        flops = 0
        prev_dim = self.input_dim
        for dim in self.hidden_layers:
            flops += 2 * prev_dim * dim  # ä¹˜åŠ æ“ä½œ
            prev_dim = dim
        flops += 2 * prev_dim * self.output_dim
        print(f"   ç†è®ºFLOPs: {flops:,}")
    
    def forward(self, x):
        """é«˜æ•ˆå‰å‘ä¼ æ’­"""
        for layer in self.layers:
            # å¯¹äºæ®‹å·®å±‚ï¼Œç›´æ¥è°ƒç”¨
            if isinstance(layer, EfficientResidualLayer):
                x = layer(x)
            else:
                # å¯¹äºæ ‡å‡†å±‚ï¼Œå…ˆç»è¿‡å±‚ï¼Œå†åº”ç”¨æ¿€æ´»å‡½æ•°
                x = layer(x)
                x = self.activation_fn(x)
        
        # åº”ç”¨æ³¨æ„åŠ›æœºåˆ¶ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        x = self.attention(x)
        
        # è¾“å‡ºå±‚
        x = self.output_layer(x)
        
        return x
    
    def get_model_summary(self):
        """è¿”å›æ¨¡å‹æ‘˜è¦ä¿¡æ¯"""
        summary = {
            'input_dim': self.input_dim,
            'output_dim': self.output_dim,
            'hidden_layers': self.hidden_layers,
            'dropout_rate': self.dropout_rate,
            'batch_norm': self.batch_norm,
            'use_residual': self.use_residual,
            'use_attention': self.use_attention,
            'compression_factor': self.compression_factor,
            'parameter_count': sum(p.numel() for p in self.parameters()),
            'device': str(self.device),
            'architecture': 'EfficientEWPINN'
        }
        return summary
    
    def enable_quantization(self):
        """å‡†å¤‡æ¨¡å‹è¿›è¡Œé‡åŒ–"""
        # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        self.eval()
        
        # ç¡®ä¿æ‰¹å½’ä¸€åŒ–å±‚ä½¿ç”¨ç§»åŠ¨ç»Ÿè®¡
        for m in self.modules():
            if isinstance(m, nn.BatchNorm1d):
                m.track_running_stats = True
        
        print("âœ… æ¨¡å‹å·²å‡†å¤‡å¥½è¿›è¡Œé‡åŒ–")
    
    def prune_model(self, pruning_ratio=0.2):
        """å¯¹æ¨¡å‹è¿›è¡Œå‰ªæä»¥å‡å°‘å‚æ•°æ•°é‡"""
        if pruning_ratio <= 0:
            return
        
        # å¯¹çº¿æ€§å±‚åº”ç”¨L1èŒƒæ•°å‰ªæ
        parameters_to_prune = []
        for module in self.modules():
            if isinstance(module, nn.Linear) and module.out_features > 10:  # é¿å…å¯¹è¾“å‡ºå±‚å’Œå°å±‚å‰ªæ
                parameters_to_prune.append((module, 'weight'))
        
        if parameters_to_prune:
            # åº”ç”¨å…¨å±€å‰ªæ
            from torch.nn.utils import prune
            prune.global_unstructured(
                parameters_to_prune,
                pruning_method=prune.L1Unstructured,
                amount=pruning_ratio,
            )
            
            # ä½¿å‰ªææ°¸ä¹…åŒ–
            for module, name in parameters_to_prune:
                prune.remove(module, name)
            
            print(f"âœ… æ¨¡å‹å‰ªæå®Œæˆï¼Œå‰ªææ¯”ä¾‹: {pruning_ratio*100:.1f}%")
            print(f"   å‰ªæåå‚æ•°æ•°é‡: {sum(p.numel() for p in self.parameters() if p.requires_grad):,}")

def create_optimized_model(config_path=None, device='cpu', **kwargs):
    """å·¥å‚å‡½æ•°ï¼šåˆ›å»ºä¼˜åŒ–çš„EWPINNæ¨¡å‹"""
    return EfficientEWPINN(config_path=config_path, device=device, **kwargs)

def get_model_optimization_suggestions(model):
    """åˆ†ææ¨¡å‹å¹¶æä¾›ä¼˜åŒ–å»ºè®®"""
    suggestions = []
    param_count = sum(p.numel() for p in model.parameters())
    
    # åŸºäºå‚æ•°æ•°é‡çš„å‹ç¼©å»ºè®®
    if param_count > 100000:
        suggestions.append(f"å‚æ•°æ•°é‡è¾ƒå¤§ ({param_count:,})ï¼Œå»ºè®®ä½¿ç”¨å‹ç¼©å› å­ 0.75-0.5 å‡å°‘è®¡ç®—é‡")
    
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†æ®‹å·®è¿æ¥
    if hasattr(model, 'use_residual') and not model.use_residual:
        suggestions.append("å»ºè®®å¯ç”¨æ®‹å·®è¿æ¥ä»¥æé«˜æ·±åº¦ç½‘ç»œçš„è®­ç»ƒç¨³å®šæ€§")
    
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†æ³¨æ„åŠ›æœºåˆ¶
    if hasattr(model, 'use_attention') and not model.use_attention and param_count > 50000:
        suggestions.append("å¯¹äºè¾ƒå¤§çš„æ¨¡å‹ï¼Œå»ºè®®å¯ç”¨æ³¨æ„åŠ›æœºåˆ¶ä»¥æé«˜ç‰¹å¾åˆ©ç”¨ç‡")
    
    return suggestions

def benchmark_model_performance(model, input_tensor, iterations=100, warmup=10):
    """åŸºå‡†æµ‹è¯•æ¨¡å‹æ€§èƒ½"""
    model.eval()
    device = next(model.parameters()).device
    input_tensor = input_tensor.to(device)
    
    # é¢„çƒ­
    with torch.no_grad():
        for _ in range(warmup):
            _ = model(input_tensor)
    
    # æµ‹é‡æ¨ç†æ—¶é—´
    torch.cuda.synchronize() if device.type == 'cuda' else None
    start_time = time.time()
    
    with torch.no_grad():
        for _ in range(iterations):
            _ = model(input_tensor)
    
    torch.cuda.synchronize() if device.type == 'cuda' else None
    end_time = time.time()
    
    avg_time = (end_time - start_time) / iterations * 1000  # æ¯«ç§’
    throughput = iterations / (end_time - start_time)  # æ ·æœ¬/ç§’
    
    return {
        'average_inference_time_ms': avg_time,
        'throughput_samples_per_second': throughput,
        'device': str(device),
        'iterations': iterations
    }

# å¯¼å…¥å¿…è¦çš„åº“
import time