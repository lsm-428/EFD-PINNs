import torch
import numpy as np
import json
import os
from datetime import datetime
import logging
from typing import Dict, List, Any, Optional, Tuple

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger('EWPINN_HyperOptimizer')

class AdaptiveHyperparameterOptimizer:
    """
    è‡ªé€‚åº”è¶…å‚æ•°ä¼˜åŒ–å™¨ - ä¸ºEWPINNæ¨¡å‹æä¾›åŠ¨æ€è¶…å‚æ•°è°ƒæ•´åŠŸèƒ½
    """
    
    def __init__(self, config_path: Optional[str] = None, device: Optional[str] = None):
        """
        åˆå§‹åŒ–è‡ªé€‚åº”è¶…å‚æ•°ä¼˜åŒ–å™¨
        """
        self.device = device if device else ('cuda' if torch.cuda.is_available() else 'cpu')
        self.optimization_history = []
        self.iteration = 0
        
        # é»˜è®¤é…ç½®
        self.config = {
            'learning_rate': {
                'initial': 1e-3,
                'min': 1e-6,
                'max': 1e-2,
                'patience': 5,
                'factor': 0.5,
                'cooldown': 3
            },
            'batch_size': {
                'initial': 32,
                'min': 8,
                'max': 128,
                'scale_factor': 2,
                'patience': 8
            },
            'physics_constraint': {
                'initial_weight': 0.1,
                'max_weight': 0.8,
                'growth_rate': 0.05
            },
            'regularization': {
                'weight_decay': {
                    'initial': 1e-4,
                    'min': 1e-6,
                    'max': 1e-3,
                    'patience': 10
                },
                'dropout': {
                    'initial': 0.1
                }
            },
            'early_stopping': {
                'patience': 15,
                'min_delta': 1e-4
            }
        }
        
        # åŠ è½½é…ç½®æ–‡ä»¶
        if config_path and os.path.exists(config_path):
            self._load_config(config_path)
        
        # åˆå§‹åŒ–å½“å‰è¶…å‚æ•°
        self.current_hyperparams = {
            'learning_rate': self.config['learning_rate']['initial'],
            'batch_size': self.config['batch_size']['initial'],
            'physics_weight': self.config['physics_constraint']['initial_weight'],
            'weight_decay': self.config['regularization']['weight_decay']['initial'],
            'dropout_rate': self.config['regularization']['dropout']['initial']
        }
        
        # åˆå§‹åŒ–çŠ¶æ€è·Ÿè¸ªå™¨
        self.state = {
            'lr_patience': 0,
            'batch_size_patience': 0,
            'reg_patience': 0,
            'cooldown': 0,
            'best_val_loss': float('inf'),
            'best_epoch': 0,
            'loss_trend': [],
            'lr_history': [],
            'physics_weight_history': []
        }
        
        logger.info(f"âœ… è‡ªé€‚åº”è¶…å‚æ•°ä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _load_config(self, config_path: str):
        """
        ä»é…ç½®æ–‡ä»¶åŠ è½½ä¼˜åŒ–å™¨è®¾ç½®
        """
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                user_config = json.load(f)
                if 'è‡ªé€‚åº”è¶…å‚æ•°' in user_config:
                    adaptive_config = user_config['è‡ªé€‚åº”è¶…å‚æ•°']
                    for key, value in adaptive_config.items():
                        if key in self.config and isinstance(value, dict):
                            self.config[key].update(value)
            logger.info(f"âœ… æˆåŠŸåŠ è½½è¶…å‚æ•°ä¼˜åŒ–å™¨é…ç½®")
        except Exception as e:
            logger.error(f"âŒ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {str(e)}")
    
    def get_hyperparams(self) -> Dict[str, float]:
        """
        è·å–å½“å‰è¶…å‚æ•°
        """
        return self.current_hyperparams.copy()
    
    def update_optimizer_lr(self, optimizer: torch.optim.Optimizer):
        """
        æ›´æ–°ä¼˜åŒ–å™¨çš„å­¦ä¹ ç‡
        """
        for param_group in optimizer.param_groups:
            param_group['lr'] = self.current_hyperparams['learning_rate']
    
    def adaptive_update(self, metrics: Dict[str, float], epoch: int = 0) -> bool:
        """
        åŸºäºæ€§èƒ½æŒ‡æ ‡è‡ªé€‚åº”æ›´æ–°è¶…å‚æ•°
        """
        self.iteration += 1
        should_stop = False
        
        # è®°å½•æŒ‡æ ‡
        current_val_loss = metrics.get('val_loss', float('inf'))
        self.state['loss_trend'].append(current_val_loss)
        self.state['lr_history'].append(self.current_hyperparams['learning_rate'])
        self.state['physics_weight_history'].append(self.current_hyperparams['physics_weight'])
        
        # æ›´æ–°æœ€ä½³æŒ‡æ ‡
        if current_val_loss < self.state['best_val_loss'] - self.config['early_stopping']['min_delta']:
            self.state['best_val_loss'] = current_val_loss
            self.state['best_epoch'] = epoch
            self.state['lr_patience'] = 0
            self.state['batch_size_patience'] = 0
            self.state['reg_patience'] = 0
        
        # æ£€æŸ¥å†·å´æœŸ
        if self.state['cooldown'] > 0:
            self.state['cooldown'] -= 1
            return should_stop
        
        # è‡ªé€‚åº”è°ƒæ•´è¶…å‚æ•°
        if epoch >= 10:  # é¢„çƒ­åå¼€å§‹è°ƒæ•´
            # å­¦ä¹ ç‡è°ƒæ•´
            self._adjust_learning_rate(current_val_loss)
            
            # ç‰©ç†çº¦æŸæƒé‡è°ƒæ•´
            self._adjust_physics_weight(metrics)
            
            # æ‰¹æ¬¡å¤§å°è°ƒæ•´
            self._adjust_batch_size(metrics, epoch)
            
            # æ­£åˆ™åŒ–è°ƒæ•´
            self._adjust_regularization(metrics)
        
        # è®°å½•å†å²
        self.optimization_history.append({
            'epoch': epoch,
            'metrics': metrics.copy(),
            'hyperparams': self.current_hyperparams.copy()
        })
        
        # æ£€æŸ¥æ—©åœ
        if len(self.state['loss_trend']) > self.config['early_stopping']['patience']:
            recent_losses = self.state['loss_trend'][-self.config['early_stopping']['patience']:]
            if min(recent_losses) >= self.state['best_val_loss'] - self.config['early_stopping']['min_delta']:
                should_stop = True
        
        return should_stop
    
    def _adjust_learning_rate(self, val_loss: float):
        """
        è°ƒæ•´å­¦ä¹ ç‡
        """
        # å¦‚æœéªŒè¯æŸå¤±æ²¡æœ‰æ”¹å–„ï¼Œé™ä½å­¦ä¹ ç‡
        if len(self.state['loss_trend']) >= 2:
            if self.state['loss_trend'][-1] >= self.state['loss_trend'][-2] - self.config['early_stopping']['min_delta']:
                self.state['lr_patience'] += 1
                
                if self.state['lr_patience'] >= self.config['learning_rate']['patience']:
                    new_lr = self.current_hyperparams['learning_rate'] * self.config['learning_rate']['factor']
                    new_lr = max(new_lr, self.config['learning_rate']['min'])
                    
                    if new_lr < self.current_hyperparams['learning_rate']:
                        self.current_hyperparams['learning_rate'] = new_lr
                        self.state['lr_patience'] = 0
                        self.state['cooldown'] = self.config['learning_rate']['cooldown']
                        logger.info(f"ğŸ“‰ å­¦ä¹ ç‡è¡°å‡è‡³: {new_lr}")
    
    def _adjust_physics_weight(self, metrics: Dict[str, float]):
        """
        è°ƒæ•´ç‰©ç†çº¦æŸæƒé‡
        """
        data_loss = metrics.get('data_loss', 0.0)
        physics_loss = metrics.get('physics_loss', 0.0)
        
        if data_loss > 0 and physics_loss > 0:
            loss_ratio = data_loss / physics_loss
            
            # åŠ¨æ€è°ƒæ•´ç‰©ç†æƒé‡
            if loss_ratio > 2.0 and self.current_hyperparams['physics_weight'] < self.config['physics_constraint']['max_weight']:
                new_weight = min(
                    self.current_hyperparams['physics_weight'] + self.config['physics_constraint']['growth_rate'],
                    self.config['physics_constraint']['max_weight']
                )
                self.current_hyperparams['physics_weight'] = new_weight
            elif loss_ratio < 0.5:
                new_weight = max(
                    self.current_hyperparams['physics_weight'] - self.config['physics_constraint']['growth_rate'],
                    0.0
                )
                self.current_hyperparams['physics_weight'] = new_weight
    
    def _adjust_batch_size(self, metrics: Dict[str, float], epoch: int):
        """
        è°ƒæ•´æ‰¹æ¬¡å¤§å°
        """
        if len(self.state['loss_trend']) >= 10:
            recent_losses = self.state['loss_trend'][-10:]
            loss_decrease_rate = (recent_losses[0] - recent_losses[-1]) / recent_losses[0]
            
            if loss_decrease_rate < 0.05:  # æŸå¤±ä¸‹é™ç¼“æ…¢
                self.state['batch_size_patience'] += 1
                
                if self.state['batch_size_patience'] >= self.config['batch_size']['patience']:
                    new_batch_size = max(
                        self.current_hyperparams['batch_size'] // self.config['batch_size']['scale_factor'],
                        self.config['batch_size']['min']
                    )
                    
                    if new_batch_size < self.current_hyperparams['batch_size']:
                        self.current_hyperparams['batch_size'] = new_batch_size
                        self.state['batch_size_patience'] = 0
    
    def _adjust_regularization(self, metrics: Dict[str, float]):
        """
        è°ƒæ•´æ­£åˆ™åŒ–å¼ºåº¦
        """
        train_loss = metrics.get('train_loss', 0.0)
        val_loss = metrics.get('val_loss', 0.0)
        
        if train_loss > 0 and val_loss > 0:
            overfitting_ratio = val_loss / train_loss
            
            if overfitting_ratio > 1.5:  # å¯èƒ½è¿‡æ‹Ÿåˆ
                if self.state['reg_patience'] >= self.config['regularization']['weight_decay']['patience']:
                    new_weight_decay = min(
                        self.current_hyperparams['weight_decay'] * 1.5,
                        self.config['regularization']['weight_decay']['max']
                    )
                    self.current_hyperparams['weight_decay'] = new_weight_decay
            elif overfitting_ratio < 1.1:
                new_weight_decay = max(
                    self.current_hyperparams['weight_decay'] * 0.8,
                    self.config['regularization']['weight_decay']['min']
                )
                self.current_hyperparams['weight_decay'] = new_weight_decay
                self.state['reg_patience'] = 0
    
    def save_history(self, save_path: str):
        """
        ä¿å­˜ä¼˜åŒ–å†å²
        """
        try:
            history_data = {
                'optimization_history': self.optimization_history,
                'final_hyperparams': self.current_hyperparams,
                'timestamp': datetime.now().isoformat()
            }
            
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            
            with open(save_path, 'w', encoding='utf-8') as f:
                json.dump(history_data, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ä¼˜åŒ–å†å²å¤±è´¥: {str(e)}")
    
    def export_recommended_config(self, save_path: str):
        """
        å¯¼å‡ºæ¨èçš„é…ç½®æ–‡ä»¶
        """
        try:
            # è·å–æœ€ä½³è¶…å‚æ•°
            if not self.optimization_history:
                best_hyperparams = self.current_hyperparams
            else:
                best_entry = min(self.optimization_history, key=lambda x: x['metrics'].get('val_loss', float('inf')))
                best_hyperparams = best_entry['hyperparams']
            
            recommended_config = {
                'æ¨¡å‹é…ç½®': {
                    'è¾“å…¥ç»´åº¦': 62,
                    'è¾“å‡ºç»´åº¦': 24,
                    'ç½‘ç»œæ¶æ„': {
                        'éšè—å±‚': [128, 64, 32],
                        'Dropout': best_hyperparams['dropout_rate']
                    }
                },
                'è®­ç»ƒé…ç½®': {
                    'æ‰¹æ¬¡å¤§å°': best_hyperparams['batch_size'],
                    'å­¦ä¹ ç‡': best_hyperparams['learning_rate'],
                    'æ­£åˆ™åŒ–': {
                        'æƒé‡è¡°å‡': best_hyperparams['weight_decay']
                    }
                },
                'ç‰©ç†çº¦æŸ': {
                    'æƒé‡': best_hyperparams['physics_weight']
                },
                'å¯¼å‡ºæ—¶é—´': datetime.now().isoformat()
            }
            
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            
            with open(save_path, 'w', encoding='utf-8') as f:
                json.dump(recommended_config, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.error(f"âŒ å¯¼å‡ºæ¨èé…ç½®å¤±è´¥: {str(e)}")

# é›†æˆé€‚é…å™¨
def integrate_adaptive_optimizer(config_path: str = None):
    """
    é›†æˆè‡ªé€‚åº”è¶…å‚æ•°ä¼˜åŒ–å™¨
    """
    optimizer = AdaptiveHyperparameterOptimizer(config_path=config_path)
    logger.info("âœ… è‡ªé€‚åº”è¶…å‚æ•°ä¼˜åŒ–å™¨å·²é›†æˆ")
    return optimizer