import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import json
import math
from typing import Dict, List, Optional, Union, Tuple


class AdvancedRegularizer:
    """
    é«˜çº§æ­£åˆ™åŒ–å™¨ï¼Œæä¾›å¤šç§æ­£åˆ™åŒ–æŠ€æœ¯ä»¥æå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›
    
    ä¸»è¦åŠŸèƒ½ï¼š
    1. L1/L2æ­£åˆ™åŒ–åŠå¼¹æ€§ç½‘ç»œæ­£åˆ™åŒ–
    2. Dropoutä¼˜åŒ–å’ŒDropConnect
    3. æƒé‡çº¦æŸï¼ˆå¦‚æƒé‡è£å‰ªã€è°±å½’ä¸€åŒ–ï¼‰
    4. æ—©åœç­–ç•¥
    5. æ··åˆæ­£åˆ™åŒ–ç­–ç•¥
    """
    
    def __init__(self, 
                 config_path: Optional[str] = None,
                 l1_lambda: float = 0.0,
                 l2_lambda: float = 0.001,
                 weight_decay: float = 0.0,
                 use_dropout: bool = True,
                 dropout_rate: float = 0.1,
                 use_weight_clipping: bool = False,
                 weight_clip_value: float = 1.0,
                 use_spectral_norm: bool = False,
                 use_batch_norm: bool = True,
                 enable_early_stopping: bool = True,
                 patience: int = 10,
                 min_improvement: float = 1e-5,
                 device: str = 'cpu',
                 l1_strength: Optional[float] = None,
                 spectral_norm: Optional[bool] = None):
        """
        åˆå§‹åŒ–é«˜çº§æ­£åˆ™åŒ–å™¨
        
        å‚æ•°ï¼š
        - config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œå¯ä»ä¸­åŠ è½½æ­£åˆ™åŒ–å‚æ•°
        - l1_lambda: L1æ­£åˆ™åŒ–ç³»æ•°
        - l2_lambda: L2æ­£åˆ™åŒ–ç³»æ•°
        - weight_decay: æƒé‡è¡°å‡ç³»æ•°
        - use_dropout: æ˜¯å¦ä½¿ç”¨Dropout
        - dropout_rate: Dropoutæ¦‚ç‡
        - use_weight_clipping: æ˜¯å¦ä½¿ç”¨æƒé‡è£å‰ª
        - weight_clip_value: æƒé‡è£å‰ªé˜ˆå€¼
        - use_spectral_norm: æ˜¯å¦ä½¿ç”¨è°±å½’ä¸€åŒ–
        - use_batch_norm: æ˜¯å¦ä½¿ç”¨æ‰¹å½’ä¸€åŒ–
        - enable_early_stopping: æ˜¯å¦å¯ç”¨æ—©åœ
        - patience: æ—©åœè€å¿ƒå€¼
        - min_improvement: æœ€å°æ”¹è¿›é˜ˆå€¼
        - device: è®¡ç®—è®¾å¤‡
        """
        self.device = device
        self.config = self._load_config(config_path) if config_path else {}
        
        # ä»é…ç½®æˆ–å‚æ•°ä¸­è·å–æ­£åˆ™åŒ–è®¾ç½®
        self.l1_lambda = self.config.get('L1æ­£åˆ™åŒ–ç³»æ•°', l1_lambda)
        self.l2_lambda = self.config.get('L2æ­£åˆ™åŒ–ç³»æ•°', l2_lambda)
        self.weight_decay = self.config.get('æƒé‡è¡°å‡', weight_decay)
        self.use_dropout = self.config.get('ä½¿ç”¨Dropout', use_dropout)
        self.dropout_rate = self.config.get('Dropoutç‡', dropout_rate)
        self.use_weight_clipping = self.config.get('ä½¿ç”¨æƒé‡è£å‰ª', use_weight_clipping)
        self.weight_clip_value = self.config.get('æƒé‡è£å‰ªé˜ˆå€¼', weight_clip_value)
        self.use_spectral_norm = self.config.get('ä½¿ç”¨è°±å½’ä¸€åŒ–', use_spectral_norm)
        self.use_batch_norm = self.config.get('ä½¿ç”¨æ‰¹å½’ä¸€åŒ–', use_batch_norm)

        if l1_strength is not None:
            self.l1_lambda = l1_strength
        if spectral_norm is not None:
            self.use_spectral_norm = spectral_norm
        
        # æ—©åœè®¾ç½®
        self.enable_early_stopping = self.config.get('å¯ç”¨æ—©åœ', enable_early_stopping)
        self.patience = self.config.get('æ—©åœè€å¿ƒå€¼', patience)
        self.min_improvement = self.config.get('æœ€å°æ”¹è¿›é˜ˆå€¼', min_improvement)
        
        # æ—©åœçŠ¶æ€å˜é‡
        self.best_val_loss = float('inf')
        self.patience_counter = 0
        self.should_stop = False
        
        # ä¿å­˜æ­£åˆ™åŒ–å†å²è®°å½•
        self.regularization_history = []
        
        # åˆå§‹åŒ–Dropoutå±‚
        self.dropout_layer = nn.Dropout(self.dropout_rate)
        
        if self.enable_early_stopping:
            print(f"âœ… æ—©åœæœºåˆ¶å·²å¯ç”¨: è€å¿ƒå€¼={self.patience}, æœ€å°æ”¹è¿›={self.min_improvement}")
        
        print(f"ğŸ“Š æ­£åˆ™åŒ–é…ç½®: L1={self.l1_lambda}, L2={self.l2_lambda}, Dropout={self.dropout_rate if self.use_dropout else 'ç¦ç”¨'}")
    
    def _load_config(self, config_path: str) -> Dict:
        """
        ä»é…ç½®æ–‡ä»¶åŠ è½½æ­£åˆ™åŒ–å‚æ•°
        """
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                full_config = json.load(f)
                return full_config.get('æ­£åˆ™åŒ–é…ç½®', {})
        except Exception as e:
            print(f"âš ï¸  åŠ è½½æ­£åˆ™åŒ–é…ç½®å¤±è´¥: {str(e)}")
            return {}
    
    def compute_regularization_loss(self, model: nn.Module) -> torch.Tensor:
        """
        è®¡ç®—æ¨¡å‹çš„æ­£åˆ™åŒ–æŸå¤±
        
        å‚æ•°ï¼š
        - model: PyTorchæ¨¡å‹
        
        è¿”å›ï¼š
        - æ­£åˆ™åŒ–æŸå¤±å¼ é‡
        """
        regularization_loss = torch.tensor(0.0, device=self.device)
        
        # è®¡ç®—L1æ­£åˆ™åŒ–æŸå¤±
        if self.l1_lambda > 0:
            l1_loss = sum(torch.norm(param, 1) for param in model.parameters() if param.requires_grad)
            regularization_loss += self.l1_lambda * l1_loss
        
        # è®¡ç®—L2æ­£åˆ™åŒ–æŸå¤±
        if self.l2_lambda > 0:
            l2_loss = sum(torch.norm(param, 2) for param in model.parameters() if param.requires_grad)
            regularization_loss += self.l2_lambda * l2_loss
        
        # è®°å½•æ­£åˆ™åŒ–æŸå¤±
        self.regularization_history.append({
            'l1_loss': (self.l1_lambda * sum(torch.norm(param, 1).item() for param in model.parameters() if param.requires_grad)) if self.l1_lambda > 0 else 0.0,
            'l2_loss': (self.l2_lambda * sum(torch.norm(param, 2).item() for param in model.parameters() if param.requires_grad)) if self.l2_lambda > 0 else 0.0,
            'total_reg_loss': regularization_loss.item()
        })
        
        return regularization_loss
    
    def apply_dropout(self, x: torch.Tensor, training: bool = True) -> torch.Tensor:
        """
        åº”ç”¨Dropoutæ­£åˆ™åŒ–
        
        å‚æ•°ï¼š
        - x: è¾“å…¥å¼ é‡
        - training: æ˜¯å¦å¤„äºè®­ç»ƒæ¨¡å¼
        
        è¿”å›ï¼š
        - Dropoutåçš„å¼ é‡
        """
        if self.use_dropout and training:
            return self.dropout_layer(x)
        return x
    
    def apply_weight_clipping(self, model: nn.Module) -> None:
        """
        åº”ç”¨æƒé‡è£å‰ª
        
        å‚æ•°ï¼š
        - model: PyTorchæ¨¡å‹
        """
        if self.use_weight_clipping:
            with torch.no_grad():
                for param in model.parameters():
                    if param.requires_grad:
                        param.clamp_(-self.weight_clip_value, self.weight_clip_value)
    
    def apply_spectral_normalization(self, model: nn.Module) -> nn.Module:
        """
        åº”ç”¨è°±å½’ä¸€åŒ–åˆ°æ¨¡å‹çš„çº¿æ€§å±‚
        
        å‚æ•°ï¼š
        - model: PyTorchæ¨¡å‹
        
        è¿”å›ï¼š
        - åº”ç”¨è°±å½’ä¸€åŒ–åçš„æ¨¡å‹
        """
        if self.use_spectral_norm:
            # é€’å½’éå†æ¨¡å‹çš„æ‰€æœ‰å­æ¨¡å—
            for name, module in list(model.named_children()):
                if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):
                    # åº”ç”¨è°±å½’ä¸€åŒ–
                    setattr(model, name, nn.utils.spectral_norm(module))
                else:
                    # é€’å½’åº”ç”¨åˆ°å­æ¨¡å—
                    self.apply_spectral_normalization(module)
        return model
    
    def check_early_stopping(self, val_loss: float) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦åº”è¯¥æ—©åœ
        
        å‚æ•°ï¼š
        - val_loss: å½“å‰éªŒè¯æŸå¤±
        
        è¿”å›ï¼š
        - æ˜¯å¦åº”è¯¥åœæ­¢è®­ç»ƒ
        """
        if not self.enable_early_stopping:
            return False
        
        # å¦‚æœéªŒè¯æŸå¤±æœ‰è¶³å¤Ÿçš„æ”¹è¿›
        if val_loss < self.best_val_loss - self.min_improvement:
            self.best_val_loss = val_loss
            self.patience_counter = 0
            self.should_stop = False
        else:
            # æ²¡æœ‰è¶³å¤Ÿæ”¹è¿›ï¼Œå¢åŠ è®¡æ•°å™¨
            self.patience_counter += 1
            if self.patience_counter >= self.patience:
                self.should_stop = True
                print(f"ğŸ“‰ è§¦å‘æ—©åœ: {self.patience_counter}è½®æœªæ”¹è¿›")
        
        return self.should_stop
    
    def reset_early_stopping(self) -> None:
        """
        é‡ç½®æ—©åœçŠ¶æ€
        """
        self.best_val_loss = float('inf')
        self.patience_counter = 0
        self.should_stop = False
    
    def get_regularization_info(self) -> Dict:
        """
        è·å–æ­£åˆ™åŒ–é…ç½®ä¿¡æ¯
        
        è¿”å›ï¼š
        - æ­£åˆ™åŒ–é…ç½®å­—å…¸
        """
        return {
            'l1_lambda': self.l1_lambda,
            'l2_lambda': self.l2_lambda,
            'weight_decay': self.weight_decay,
            'use_dropout': self.use_dropout,
            'dropout_rate': self.dropout_rate,
            'use_weight_clipping': self.use_weight_clipping,
            'weight_clip_value': self.weight_clip_value,
            'use_spectral_norm': self.use_spectral_norm,
            'use_batch_norm': self.use_batch_norm,
            'enable_early_stopping': self.enable_early_stopping,
            'patience': self.patience,
            'min_improvement': self.min_improvement
        }
    
    def get_regularization_history(self) -> List[Dict]:
        """
        è·å–æ­£åˆ™åŒ–å†å²è®°å½•
        
        è¿”å›ï¼š
        - æ­£åˆ™åŒ–å†å²è®°å½•åˆ—è¡¨
        """
        return self.regularization_history


class DropConnectLayer(nn.Module):
    """
    DropConnectå±‚å®ç° - å¯¹ç½‘ç»œæƒé‡è¿›è¡Œéšæœºå¤±æ´»è€Œéæ¿€æ´»å€¼
    """
    
    def __init__(self, module: nn.Module, drop_rate: float = 0.1, active: bool = True):
        """
        åˆå§‹åŒ–DropConnectå±‚
        
        å‚æ•°ï¼š
        - module: è¦åº”ç”¨DropConnectçš„æ¨¡å—ï¼ˆé€šå¸¸æ˜¯nn.Linearï¼‰
        - drop_rate: DropConnectæ¦‚ç‡
        - active: æ˜¯å¦æ¿€æ´»DropConnect
        """
        super(DropConnectLayer, self).__init__()
        self.module = module
        self.drop_rate = drop_rate
        self.active = active
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        å‚æ•°ï¼š
        - x: è¾“å…¥å¼ é‡
        
        è¿”å›ï¼š
        - è¾“å‡ºå¼ é‡
        """
        if not self.training or not self.active or self.drop_rate == 0:
            return self.module(x)
        
        # ç”Ÿæˆæ©ç ï¼Œä¿ç•™æ¦‚ç‡ä¸º1-drop_rate
        with torch.no_grad():
            # ä¸ºæƒé‡åˆ›å»ºæ©ç 
            mask = torch.bernoulli(
                torch.ones_like(self.module.weight) * (1 - self.drop_rate)
            ) / (1 - self.drop_rate)  # ç¼©æ”¾ä»¥ä¿æŒæœŸæœ›è¾“å‡ºä¸å˜
            
            # å¯¹æƒé‡åº”ç”¨æ©ç 
            masked_weight = self.module.weight * mask
            
            # å¦‚æœå­˜åœ¨åç½®ï¼Œä¹Ÿåˆ›å»ºæ©ç å¹¶åº”ç”¨
            if self.module.bias is not None:
                bias_mask = torch.bernoulli(
                    torch.ones_like(self.module.bias) * (1 - self.drop_rate)
                ) / (1 - self.drop_rate)
                masked_bias = self.module.bias * bias_mask
                return F.linear(x, masked_weight, masked_bias)
            else:
                return F.linear(x, masked_weight)


class GradientNoiseRegularizer:
    """
    æ¢¯åº¦å™ªå£°æ­£åˆ™åŒ–å™¨ - åœ¨æ¢¯åº¦ä¸­æ·»åŠ å™ªå£°ä»¥æé«˜æ³›åŒ–èƒ½åŠ›
    """
    
    def __init__(self, 
                 eta: float = 0.01, 
                 gamma: float = 0.55, 
                 enabled: bool = True,
                 noise_stddev: Optional[float] = None,
                 noise_decay: Optional[float] = None,
                 noise_annealing: Optional[bool] = None,
                 device: Optional[str] = None):
        """
        åˆå§‹åŒ–æ¢¯åº¦å™ªå£°æ­£åˆ™åŒ–å™¨
        
        å‚æ•°ï¼š
        - eta: å™ªå£°å¼ºåº¦å‚æ•°
        - gamma: å™ªå£°è¡°å‡ç‡å‚æ•°
        - enabled: æ˜¯å¦å¯ç”¨
        """
        self.eta = eta if noise_stddev is None else noise_stddev
        self.gamma = gamma if noise_decay is None else noise_decay
        self.enabled = enabled if noise_annealing is None else noise_annealing
        self.device = device
        self.iteration = 0
    
    def add_gradient_noise(self, parameters: List[torch.Tensor]) -> None:
        """
        å‘å‚æ•°æ¢¯åº¦æ·»åŠ å™ªå£°
        
        å‚æ•°ï¼š
        - parameters: è¦æ·»åŠ å™ªå£°çš„å‚æ•°åˆ—è¡¨
        """
        if not self.enabled or self.eta == 0:
            return
        
        # æ›´æ–°è¿­ä»£è®¡æ•°
        self.iteration += 1
        
        # è®¡ç®—å™ªå£°æ ‡å‡†å·®
        sigma = self.eta / ((1 + self.iteration) ** self.gamma)
        
        # å‘æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦æ·»åŠ å™ªå£°
        for param in parameters:
            if param.grad is not None:
                noise = torch.randn_like(param.grad) * sigma
                param.grad.add_(noise)
    
    def reset(self) -> None:
        """
        é‡ç½®è¿­ä»£è®¡æ•°
        """
        self.iteration = 0


class EnsembleRegularization:
    """
    é›†æˆæ­£åˆ™åŒ– - é€šè¿‡æ¨¡å‹é›†æˆæå‡æ³›åŒ–èƒ½åŠ›
    """
    
    def __init__(self, num_models: int = 3, device: str = 'cpu'):
        """
        åˆå§‹åŒ–é›†æˆæ­£åˆ™åŒ–å™¨
        
        å‚æ•°ï¼š
        - num_models: é›†æˆæ¨¡å‹æ•°é‡
        - device: è®¡ç®—è®¾å¤‡
        """
        self.num_models = num_models
        self.device = device
        self.models = []
    
    def create_ensemble(self, model_class: type, **model_kwargs) -> List[nn.Module]:
        """
        åˆ›å»ºæ¨¡å‹é›†æˆ
        
        å‚æ•°ï¼š
        - model_class: æ¨¡å‹ç±»
        - model_kwargs: æ¨¡å‹åˆå§‹åŒ–å‚æ•°
        
        è¿”å›ï¼š
        - æ¨¡å‹åˆ—è¡¨
        """
        self.models = []
        for i in range(self.num_models):
            # ä¸ºæ¯ä¸ªæ¨¡å‹è®¾ç½®ä¸åŒçš„éšæœºç§å­ä»¥å¢åŠ å¤šæ ·æ€§
            torch.manual_seed(torch.initial_seed() + i)
            model = model_class(**model_kwargs).to(self.device)
            self.models.append(model)
        
        return self.models
    
    def ensemble_predict(self, x: torch.Tensor, aggregation: str = 'mean') -> torch.Tensor:
        """
        é›†æˆé¢„æµ‹
        
        å‚æ•°ï¼š
        - x: è¾“å…¥æ•°æ®
        - aggregation: èšåˆæ–¹æ³• ('mean', 'median', 'vote')
        
        è¿”å›ï¼š
        - é›†æˆé¢„æµ‹ç»“æœ
        """
        predictions = []
        
        # è·å–æ¯ä¸ªæ¨¡å‹çš„é¢„æµ‹
        for model in self.models:
            model.eval()
            with torch.no_grad():
                pred = model(x)
                predictions.append(pred)
        
        # èšåˆé¢„æµ‹
        predictions_tensor = torch.stack(predictions)
        
        if aggregation == 'mean':
            return torch.mean(predictions_tensor, dim=0)
        elif aggregation == 'median':
            return torch.median(predictions_tensor, dim=0)[0]
        elif aggregation == 'vote':
            # å¯¹äºåˆ†ç±»ä»»åŠ¡çš„æŠ•ç¥¨æœºåˆ¶ï¼ˆéœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡è°ƒæ•´ï¼‰
            return torch.mode(predictions_tensor, dim=0)[0]
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„èšåˆæ–¹æ³•: {aggregation}")
    
    def save_ensemble(self, save_dir: str) -> None:
        """
        ä¿å­˜é›†æˆæ¨¡å‹
        
        å‚æ•°ï¼š
        - save_dir: ä¿å­˜ç›®å½•
        """
        import os
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
        
        for i, model in enumerate(self.models):
            save_path = os.path.join(save_dir, f"ensemble_model_{i}.pth")
            torch.save(model.state_dict(), save_path)
            print(f"âœ… é›†æˆæ¨¡å‹ {i} å·²ä¿å­˜è‡³: {save_path}")


class VariationalDropout(nn.Module):
    """
    å˜åˆ†Dropoutå®ç° - å¯¹åŒä¸€ç‰¹å¾çš„æ‰€æœ‰æ—¶é—´æ­¥ä½¿ç”¨ç›¸åŒçš„dropoutæ©ç 
    é€‚ç”¨äºRNN/LSTMç­‰å¾ªç¯ç¥ç»ç½‘ç»œå’Œå…·æœ‰å±‚æ¬¡ç»“æ„çš„ç¥ç»ç½‘ç»œ
    """
    
    def __init__(self, drop_rate: float = 0.1, batch_first: bool = True):
        """
        åˆå§‹åŒ–å˜åˆ†Dropout
        
        å‚æ•°ï¼š
        - drop_rate: Dropoutæ¦‚ç‡
        - batch_first: è¾“å…¥æ˜¯å¦ä¸ºbatch_firstæ ¼å¼
        """
        super(VariationalDropout, self).__init__()
        self.drop_rate = drop_rate
        self.batch_first = batch_first
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        å‚æ•°ï¼š
        - x: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º (batch_size, seq_len, features) æˆ– (seq_len, batch_size, features)
        
        è¿”å›ï¼š
        - Dropoutåçš„å¼ é‡
        """
        if not self.training or self.drop_rate == 0:
            return x
        
        # ç¡®å®šè¾“å…¥ç»´åº¦é¡ºåº
        if self.batch_first:
            batch_size, seq_len, features = x.size()
            # åˆ›å»ºå½¢çŠ¶ä¸º (batch_size, 1, features) çš„æ©ç ï¼Œå¯¹æ¯ä¸ªæ ·æœ¬çš„æ‰€æœ‰åºåˆ—ä½¿ç”¨ç›¸åŒçš„æ©ç 
            mask = torch.bernoulli(
                torch.ones(batch_size, 1, features, device=x.device) * (1 - self.drop_rate)
            ) / (1 - self.drop_rate)
        else:
            seq_len, batch_size, features = x.size()
            # åˆ›å»ºå½¢çŠ¶ä¸º (1, batch_size, features) çš„æ©ç 
            mask = torch.bernoulli(
                torch.ones(1, batch_size, features, device=x.device) * (1 - self.drop_rate)
            ) / (1 - self.drop_rate)
        
        # åº”ç”¨æ©ç 
        return x * mask


def apply_regularization_to_model(model: nn.Module, 
                                 regularizer: AdvancedRegularizer,
                                 apply_dropconnect: bool = False,
                                 dropconnect_rate: float = 0.2) -> nn.Module:
    """
    å°†æ­£åˆ™åŒ–æŠ€æœ¯åº”ç”¨åˆ°æ¨¡å‹ä¸­
    
    å‚æ•°ï¼š
    - model: è¦åº”ç”¨æ­£åˆ™åŒ–çš„æ¨¡å‹
    - regularizer: æ­£åˆ™åŒ–å™¨å®ä¾‹
    - apply_dropconnect: æ˜¯å¦åº”ç”¨DropConnect
    - dropconnect_rate: DropConnectæ¦‚ç‡
    
    è¿”å›ï¼š
    - åº”ç”¨æ­£åˆ™åŒ–åçš„æ¨¡å‹
    """
    # åº”ç”¨è°±å½’ä¸€åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if regularizer.use_spectral_norm:
        model = regularizer.apply_spectral_normalization(model)
    
    # åº”ç”¨DropConnectï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if apply_dropconnect:
        # é€’å½’éå†æ¨¡å‹å¹¶æ›¿æ¢çº¿æ€§å±‚ä¸ºDropConnectå±‚
        for name, module in list(model.named_children()):
            if isinstance(module, nn.Linear) and name != 'output_layer':  # ä¿ç•™è¾“å‡ºå±‚ä¸å˜
                setattr(model, name, DropConnectLayer(module, drop_rate=dropconnect_rate))
            else:
                # é€’å½’åº”ç”¨åˆ°å­æ¨¡å—
                apply_regularization_to_model(module, regularizer, apply_dropconnect, dropconnect_rate)
    
    return model


def compute_model_complexity(model: nn.Module, input_size: Tuple[int, ...] = (1, 62)) -> Dict[str, float]:
    """
    è®¡ç®—æ¨¡å‹å¤æ‚åº¦æŒ‡æ ‡
    
    å‚æ•°ï¼š
    - model: PyTorchæ¨¡å‹
    - input_size: è¾“å…¥å¼ é‡å¤§å°
    
    è¿”å›ï¼š
    - æ¨¡å‹å¤æ‚åº¦æŒ‡æ ‡å­—å…¸
    """
    import torch.nn.utils.prune as prune
    
    # è®¡ç®—å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    # è®¡ç®—FLOPsï¼ˆæµ®ç‚¹è¿ç®—æ¬¡æ•°ï¼‰
    # ä½¿ç”¨é’©å­æ¥è·Ÿè¸ªè®¡ç®—
    class FLOPCounter:
        def __init__(self):
            self.flops = 0
        
        def hook_fn(self, module, input, output):
            # åªè®¡ç®—Convå’ŒLinearå±‚
            if isinstance(module, nn.Conv2d):
                # å¯¹äºå·ç§¯å±‚: FLOPs = (input_channels * kernel_h * kernel_w * output_channels * out_h * out_w) / groups
                batch_size = input[0].size(0)
                out_h = output.size(2)
                out_w = output.size(3)
                kernel_h, kernel_w = module.kernel_size
                in_channels = module.in_channels
                out_channels = module.out_channels
                groups = module.groups
                
                flops = batch_size * out_h * out_w * in_channels * out_channels * kernel_h * kernel_w / groups
                self.flops += flops
                
            elif isinstance(module, nn.Linear):
                # å¯¹äºçº¿æ€§å±‚: FLOPs = 2 * batch_size * in_features * out_features
                batch_size = input[0].size(0)
                flops = 2 * batch_size * module.in_features * module.out_features
                self.flops += flops
    
    counter = FLOPCounter()
    hooks = []
    
    # æ³¨å†Œé’©å­
    for module in model.modules():
        if isinstance(module, (nn.Conv2d, nn.Linear)):
            hooks.append(module.register_forward_hook(counter.hook_fn))
    
    # å‰å‘ä¼ æ’­ä»¥è®¡ç®—FLOPs
    device = next(model.parameters()).device
    input_tensor = torch.randn(input_size, device=device)
    with torch.no_grad():
        model(input_tensor)
    
    # ç§»é™¤é’©å­
    for hook in hooks:
        hook.remove()
    
    return {
        'total_params': total_params,
        'total_params_million': total_params / 1e6,
        'flops': counter.flops,
        'flops_million': counter.flops / 1e6,
        'flops_billion': counter.flops / 1e9
    }